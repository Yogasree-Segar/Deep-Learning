{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X21.V1.791</td>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>...</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X15.V1.924</td>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X8.V1.1</td>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X16.V1.60</td>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>...</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X20.V1.54</td>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   X1   X2   X3   X4   X5   X6   X7   X8   X9  ...  X170  X171  \\\n",
       "0  X21.V1.791  135  190  229  223  192  125   55   -9  -33  ...   -17   -15   \n",
       "1  X15.V1.924  386  382  356  331  320  315  307  272  244  ...   164   150   \n",
       "2     X8.V1.1  -32  -39  -47  -37  -32  -36  -57  -73  -85  ...    57    64   \n",
       "3   X16.V1.60 -105 -101  -96  -92  -89  -95 -102 -100  -87  ...   -82   -81   \n",
       "4   X20.V1.54   -9  -65  -98 -102  -78  -48  -16    0  -21  ...     4     2   \n",
       "\n",
       "   X172  X173  X174  X175  X176  X177  X178  y  \n",
       "0   -31   -77  -103  -127  -116   -83   -51  4  \n",
       "1   146   152   157   156   154   143   129  1  \n",
       "2    48    19   -12   -30   -35   -35   -36  5  \n",
       "3   -80   -77   -85   -77   -72   -69   -65  5  \n",
       "4   -12   -32   -41   -65   -83   -89   -73  5  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "rng = RandomState()\n",
    "\n",
    "train = dataset.sample(frac=0.7, random_state=rng)\n",
    "test = dataset.loc[~dataset.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train.iloc[:,-1]\n",
    "test_label = test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['y'], axis=1)\n",
    "test = test.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 3 ... 4 1 4]\n",
      "[4 0 2 ... 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_label)\n",
    "encoded_train = encoder.transform(train_label)\n",
    "print(encoded_train)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_label)\n",
    "encoded_test = encoder.transform(test_label)\n",
    "print(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(encoded_train)\n",
    "Y_test = to_categorical(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0   X1   X2   X3   X4  X5  X6   X7   X8   X9  ...  X169  X170  \\\n",
      "5475  X10.V1.154   19   18   17   19  17  20   20   17   12  ...  -135  -135   \n",
      "5285   X1.V1.521   72  103  102   74  29 -29  -73 -118 -155  ...   -80   -34   \n",
      "6671  X18.V1.331 -175 -148 -104  -52 -17  11   38   69   95  ...     3   -38   \n",
      "2995  X10.V1.974  327  354  301  188  45 -51 -128 -263 -392  ...   -17  -148   \n",
      "2730  X21.V1.824 -261 -252 -209 -152 -98 -52  -17    3   14  ...    22    33   \n",
      "...          ...  ...  ...  ...  ...  ..  ..  ...  ...  ...  ...   ...   ...   \n",
      "4251  X15.V1.792 -138 -133 -110  -81 -62 -56  -57  -54  -54  ...   -73   -91   \n",
      "1801  X23.V1.363   16   28   37   35  13 -20  -54  -62  -40  ...    19    26   \n",
      "3749   X10.V1.62   28   21    5    0  -8 -10   -7    0   -7  ...   -46   -25   \n",
      "1441   X8.V1.343   -3   -4   -4  -10  -9 -11  -11  -13  -11  ...   -11   -18   \n",
      "9952   X18.V1.87   51   46   37   12 -15 -27  -38  -40  -44  ...    -1     7   \n",
      "\n",
      "      X171  X172  X173  X174  X175  X176  X177  X178  \n",
      "5475  -138  -143  -150  -152  -157  -156  -162  -168  \n",
      "5285    -1    37    48    41    12   -14   -37   -48  \n",
      "6671   -69   -75   -70   -51   -17    20    47    60  \n",
      "2995  -268  -375  -406  -346  -227  -120   -32    48  \n",
      "2730    20     9    -3   -10   -16   -36   -65   -86  \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "4251  -107  -116   -99   -75   -53   -34   -10    18  \n",
      "1801     7    21    22    -4     0     4     6     2  \n",
      "3749    -4    14    20    30    15    -4   -20   -18  \n",
      "1441   -22   -29   -28   -32   -33   -34   -39   -44  \n",
      "9952    10     7    -4   -20   -14    -6    -6   -25  \n",
      "\n",
      "[8050 rows x 179 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['Unnamed: 0'], axis=1)\n",
    "test = test.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train[:1000]\n",
    "partial_x_train = train[1000:]\n",
    "y_val = Y_train[:1000]\n",
    "partial_y_train = Y_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7050, 178)\n"
     ]
    }
   ],
   "source": [
    "print(partial_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(300,activation = 'relu',input_shape = (178,)))\n",
    "    #model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(100,activation='relu'))\n",
    "    #model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(50,activation='relu'))\n",
    "    #model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(20,activation='relu'))\n",
    "    #model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(5,activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc',tf.keras.metrics.SpecificityAtSensitivity(0.7),tf.keras.metrics.SensitivityAtSpecificity(0.7)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7050 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7050/7050 [==============================] - 0s 38us/step - loss: 2.0604 - acc: 0.6801 - specificity_at_sensitivity: 0.3757 - sensitivity_at_specificity: 0.3211 - val_loss: 0.6422 - val_acc: 0.7662 - val_specificity_at_sensitivity: 0.3715 - val_sensitivity_at_specificity: 0.3714\n",
      "Epoch 2/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.6229 - acc: 0.7834 - specificity_at_sensitivity: 0.3749 - sensitivity_at_specificity: 0.3579 - val_loss: 0.6075 - val_acc: 0.7880 - val_specificity_at_sensitivity: 0.3795 - val_sensitivity_at_specificity: 0.3136\n",
      "Epoch 3/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.5969 - acc: 0.7856 - specificity_at_sensitivity: 0.3823 - sensitivity_at_specificity: 0.3640 - val_loss: 0.5970 - val_acc: 0.7772 - val_specificity_at_sensitivity: 0.3836 - val_sensitivity_at_specificity: 0.4764\n",
      "Epoch 4/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.5765 - acc: 0.7824 - specificity_at_sensitivity: 0.3846 - sensitivity_at_specificity: 0.4387 - val_loss: 0.6111 - val_acc: 0.7542 - val_specificity_at_sensitivity: 0.3862 - val_sensitivity_at_specificity: 0.4051\n",
      "Epoch 5/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.5710 - acc: 0.7816 - specificity_at_sensitivity: 0.3864 - sensitivity_at_specificity: 0.3793 - val_loss: 0.5776 - val_acc: 0.7742 - val_specificity_at_sensitivity: 0.3874 - val_sensitivity_at_specificity: 0.3588\n",
      "Epoch 6/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.5715 - acc: 0.7776 - specificity_at_sensitivity: 0.3880 - sensitivity_at_specificity: 0.3512 - val_loss: 0.5953 - val_acc: 0.7596 - val_specificity_at_sensitivity: 0.3883 - val_sensitivity_at_specificity: 0.4472\n",
      "Epoch 7/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.5524 - acc: 0.7835 - specificity_at_sensitivity: 0.3895 - sensitivity_at_specificity: 0.4272 - val_loss: 0.6483 - val_acc: 0.6986 - val_specificity_at_sensitivity: 0.3898 - val_sensitivity_at_specificity: 0.4097\n",
      "Epoch 8/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.5601 - acc: 0.7896 - specificity_at_sensitivity: 0.3908 - sensitivity_at_specificity: 0.3944 - val_loss: 0.5587 - val_acc: 0.7872 - val_specificity_at_sensitivity: 0.3906 - val_sensitivity_at_specificity: 0.3813\n",
      "Epoch 9/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.5354 - acc: 0.7915 - specificity_at_sensitivity: 0.3918 - sensitivity_at_specificity: 0.3704 - val_loss: 0.5660 - val_acc: 0.8038 - val_specificity_at_sensitivity: 0.3930 - val_sensitivity_at_specificity: 0.3602\n",
      "Epoch 10/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.5224 - acc: 0.7992 - specificity_at_sensitivity: 0.3945 - sensitivity_at_specificity: 0.3963 - val_loss: 0.5297 - val_acc: 0.7910 - val_specificity_at_sensitivity: 0.3972 - val_sensitivity_at_specificity: 0.4198\n",
      "Epoch 11/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.5128 - acc: 0.7998 - specificity_at_sensitivity: 0.3972 - sensitivity_at_specificity: 0.4094 - val_loss: 0.5214 - val_acc: 0.8128 - val_specificity_at_sensitivity: 0.3969 - val_sensitivity_at_specificity: 0.4001\n",
      "Epoch 12/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.4979 - acc: 0.8098 - specificity_at_sensitivity: 0.4003 - sensitivity_at_specificity: 0.3919 - val_loss: 0.5310 - val_acc: 0.8010 - val_specificity_at_sensitivity: 0.4033 - val_sensitivity_at_specificity: 0.3842\n",
      "Epoch 13/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.4917 - acc: 0.8125 - specificity_at_sensitivity: 0.4036 - sensitivity_at_specificity: 0.3772 - val_loss: 0.5188 - val_acc: 0.8074 - val_specificity_at_sensitivity: 0.4061 - val_sensitivity_at_specificity: 0.3711\n",
      "Epoch 14/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.4739 - acc: 0.8159 - specificity_at_sensitivity: 0.4097 - sensitivity_at_specificity: 0.4339 - val_loss: 0.5030 - val_acc: 0.8210 - val_specificity_at_sensitivity: 0.4115 - val_sensitivity_at_specificity: 0.4274\n",
      "Epoch 15/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.4568 - acc: 0.8217 - specificity_at_sensitivity: 0.4168 - sensitivity_at_specificity: 0.4214 - val_loss: 0.4943 - val_acc: 0.8060 - val_specificity_at_sensitivity: 0.4175 - val_sensitivity_at_specificity: 0.4162\n",
      "Epoch 16/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.4454 - acc: 0.8231 - specificity_at_sensitivity: 0.4227 - sensitivity_at_specificity: 0.4116 - val_loss: 0.4849 - val_acc: 0.8136 - val_specificity_at_sensitivity: 0.4274 - val_sensitivity_at_specificity: 0.4069\n",
      "Epoch 17/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.4269 - acc: 0.8275 - specificity_at_sensitivity: 0.4326 - sensitivity_at_specificity: 0.4332 - val_loss: 0.5240 - val_acc: 0.8042 - val_specificity_at_sensitivity: 0.4371 - val_sensitivity_at_specificity: 0.4289\n",
      "Epoch 18/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.4136 - acc: 0.8306 - specificity_at_sensitivity: 0.4417 - sensitivity_at_specificity: 0.4250 - val_loss: 0.4869 - val_acc: 0.8220 - val_specificity_at_sensitivity: 0.4465 - val_sensitivity_at_specificity: 0.4207\n",
      "Epoch 19/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.4043 - acc: 0.8365 - specificity_at_sensitivity: 0.4511 - sensitivity_at_specificity: 0.4326 - val_loss: 0.4682 - val_acc: 0.8216 - val_specificity_at_sensitivity: 0.4557 - val_sensitivity_at_specificity: 0.4333\n",
      "Epoch 20/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3859 - acc: 0.8393 - specificity_at_sensitivity: 0.4606 - sensitivity_at_specificity: 0.4305 - val_loss: 0.4559 - val_acc: 0.8202 - val_specificity_at_sensitivity: 0.4650 - val_sensitivity_at_specificity: 0.4275\n",
      "Epoch 21/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3747 - acc: 0.8439 - specificity_at_sensitivity: 0.4696 - sensitivity_at_specificity: 0.4382 - val_loss: 0.4486 - val_acc: 0.8222 - val_specificity_at_sensitivity: 0.4737 - val_sensitivity_at_specificity: 0.4364\n",
      "Epoch 22/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3641 - acc: 0.8489 - specificity_at_sensitivity: 0.4801 - sensitivity_at_specificity: 0.4377 - val_loss: 0.4623 - val_acc: 0.8218 - val_specificity_at_sensitivity: 0.4870 - val_sensitivity_at_specificity: 0.4417\n",
      "Epoch 23/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3520 - acc: 0.8518 - specificity_at_sensitivity: 0.4914 - sensitivity_at_specificity: 0.4401 - val_loss: 0.4636 - val_acc: 0.8184 - val_specificity_at_sensitivity: 0.4952 - val_sensitivity_at_specificity: 0.4415\n",
      "Epoch 24/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3333 - acc: 0.8579 - specificity_at_sensitivity: 0.4995 - sensitivity_at_specificity: 0.4445 - val_loss: 0.4640 - val_acc: 0.8234 - val_specificity_at_sensitivity: 0.5035 - val_sensitivity_at_specificity: 0.4441\n",
      "Epoch 25/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3335 - acc: 0.8587 - specificity_at_sensitivity: 0.5089 - sensitivity_at_specificity: 0.4505 - val_loss: 0.4673 - val_acc: 0.8236 - val_specificity_at_sensitivity: 0.5158 - val_sensitivity_at_specificity: 0.4523\n",
      "Epoch 26/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3198 - acc: 0.8625 - specificity_at_sensitivity: 0.5199 - sensitivity_at_specificity: 0.4579 - val_loss: 0.4718 - val_acc: 0.8246 - val_specificity_at_sensitivity: 0.5238 - val_sensitivity_at_specificity: 0.4632\n",
      "Epoch 27/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3080 - acc: 0.8673 - specificity_at_sensitivity: 0.5277 - sensitivity_at_specificity: 0.4672 - val_loss: 0.4837 - val_acc: 0.8198 - val_specificity_at_sensitivity: 0.5316 - val_sensitivity_at_specificity: 0.4770\n",
      "Epoch 28/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.3066 - acc: 0.8693 - specificity_at_sensitivity: 0.5403 - sensitivity_at_specificity: 0.4787 - val_loss: 0.4619 - val_acc: 0.8202 - val_specificity_at_sensitivity: 0.5446 - val_sensitivity_at_specificity: 0.4800\n",
      "Epoch 29/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2851 - acc: 0.8746 - specificity_at_sensitivity: 0.5490 - sensitivity_at_specificity: 0.4925 - val_loss: 0.4506 - val_acc: 0.8218 - val_specificity_at_sensitivity: 0.5529 - val_sensitivity_at_specificity: 0.4984\n",
      "Epoch 30/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2821 - acc: 0.8757 - specificity_at_sensitivity: 0.5616 - sensitivity_at_specificity: 0.5006 - val_loss: 0.4594 - val_acc: 0.8260 - val_specificity_at_sensitivity: 0.5658 - val_sensitivity_at_specificity: 0.5089\n",
      "Epoch 31/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2784 - acc: 0.8783 - specificity_at_sensitivity: 0.5699 - sensitivity_at_specificity: 0.5178 - val_loss: 0.4771 - val_acc: 0.8226 - val_specificity_at_sensitivity: 0.5737 - val_sensitivity_at_specificity: 0.5194\n",
      "Epoch 32/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2605 - acc: 0.8857 - specificity_at_sensitivity: 0.5818 - sensitivity_at_specificity: 0.5289 - val_loss: 0.4737 - val_acc: 0.8254 - val_specificity_at_sensitivity: 0.5866 - val_sensitivity_at_specificity: 0.5362\n",
      "Epoch 33/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2602 - acc: 0.8836 - specificity_at_sensitivity: 0.5904 - sensitivity_at_specificity: 0.5410 - val_loss: 0.4963 - val_acc: 0.8254 - val_specificity_at_sensitivity: 0.5940 - val_sensitivity_at_specificity: 0.5521\n",
      "Epoch 34/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2507 - acc: 0.8879 - specificity_at_sensitivity: 0.6022 - sensitivity_at_specificity: 0.5548 - val_loss: 0.5242 - val_acc: 0.8226 - val_specificity_at_sensitivity: 0.6064 - val_sensitivity_at_specificity: 0.5567\n",
      "Epoch 35/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2388 - acc: 0.8918 - specificity_at_sensitivity: 0.6100 - sensitivity_at_specificity: 0.5707 - val_loss: 0.5020 - val_acc: 0.8236 - val_specificity_at_sensitivity: 0.6134 - val_sensitivity_at_specificity: 0.5730\n",
      "Epoch 36/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2355 - acc: 0.8914 - specificity_at_sensitivity: 0.6216 - sensitivity_at_specificity: 0.5837 - val_loss: 0.4907 - val_acc: 0.8270 - val_specificity_at_sensitivity: 0.6252 - val_sensitivity_at_specificity: 0.5890\n",
      "Epoch 37/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2297 - acc: 0.8960 - specificity_at_sensitivity: 0.6290 - sensitivity_at_specificity: 0.5963 - val_loss: 0.5501 - val_acc: 0.8166 - val_specificity_at_sensitivity: 0.6317 - val_sensitivity_at_specificity: 0.6043\n",
      "Epoch 38/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2253 - acc: 0.8984 - specificity_at_sensitivity: 0.6400 - sensitivity_at_specificity: 0.6082 - val_loss: 0.4855 - val_acc: 0.8240 - val_specificity_at_sensitivity: 0.6429 - val_sensitivity_at_specificity: 0.6187\n",
      "Epoch 39/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2228 - acc: 0.8980 - specificity_at_sensitivity: 0.6470 - sensitivity_at_specificity: 0.6214 - val_loss: 0.4999 - val_acc: 0.8278 - val_specificity_at_sensitivity: 0.6536 - val_sensitivity_at_specificity: 0.6235\n",
      "Epoch 40/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2101 - acc: 0.9031 - specificity_at_sensitivity: 0.6565 - sensitivity_at_specificity: 0.6348 - val_loss: 0.5499 - val_acc: 0.8198 - val_specificity_at_sensitivity: 0.6592 - val_sensitivity_at_specificity: 0.6374\n",
      "Epoch 41/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2028 - acc: 0.9051 - specificity_at_sensitivity: 0.6637 - sensitivity_at_specificity: 0.6460 - val_loss: 0.5344 - val_acc: 0.8242 - val_specificity_at_sensitivity: 0.6693 - val_sensitivity_at_specificity: 0.6505\n",
      "Epoch 42/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1979 - acc: 0.9071 - specificity_at_sensitivity: 0.6719 - sensitivity_at_specificity: 0.6576 - val_loss: 0.6391 - val_acc: 0.8258 - val_specificity_at_sensitivity: 0.6745 - val_sensitivity_at_specificity: 0.6630\n",
      "Epoch 43/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.2076 - acc: 0.9039 - specificity_at_sensitivity: 0.6784 - sensitivity_at_specificity: 0.6679 - val_loss: 0.4887 - val_acc: 0.8312 - val_specificity_at_sensitivity: 0.6842 - val_sensitivity_at_specificity: 0.6747\n",
      "Epoch 44/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1816 - acc: 0.9124 - specificity_at_sensitivity: 0.6867 - sensitivity_at_specificity: 0.6786 - val_loss: 0.5623 - val_acc: 0.8240 - val_specificity_at_sensitivity: 0.6890 - val_sensitivity_at_specificity: 0.6861\n",
      "Epoch 45/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1949 - acc: 0.9092 - specificity_at_sensitivity: 0.6926 - sensitivity_at_specificity: 0.6886 - val_loss: 0.6431 - val_acc: 0.8104 - val_specificity_at_sensitivity: 0.6980 - val_sensitivity_at_specificity: 0.6969\n",
      "Epoch 46/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1867 - acc: 0.9110 - specificity_at_sensitivity: 0.7002 - sensitivity_at_specificity: 0.6987 - val_loss: 0.5978 - val_acc: 0.8204 - val_specificity_at_sensitivity: 0.7022 - val_sensitivity_at_specificity: 0.7037\n",
      "Epoch 47/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1852 - acc: 0.9114 - specificity_at_sensitivity: 0.7044 - sensitivity_at_specificity: 0.7091 - val_loss: 0.5430 - val_acc: 0.8312 - val_specificity_at_sensitivity: 0.7063 - val_sensitivity_at_specificity: 0.7105\n",
      "Epoch 48/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1676 - acc: 0.9188 - specificity_at_sensitivity: 0.7130 - sensitivity_at_specificity: 0.7186 - val_loss: 0.5895 - val_acc: 0.8234 - val_specificity_at_sensitivity: 0.7149 - val_sensitivity_at_specificity: 0.7204\n",
      "Epoch 49/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1795 - acc: 0.9144 - specificity_at_sensitivity: 0.7168 - sensitivity_at_specificity: 0.7273 - val_loss: 0.6497 - val_acc: 0.8128 - val_specificity_at_sensitivity: 0.7186 - val_sensitivity_at_specificity: 0.7293\n",
      "Epoch 50/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1718 - acc: 0.9180 - specificity_at_sensitivity: 0.7241 - sensitivity_at_specificity: 0.7359 - val_loss: 0.6206 - val_acc: 0.8274 - val_specificity_at_sensitivity: 0.7268 - val_sensitivity_at_specificity: 0.7380\n",
      "Epoch 51/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1623 - acc: 0.9202 - specificity_at_sensitivity: 0.7286 - sensitivity_at_specificity: 0.7442 - val_loss: 0.8299 - val_acc: 0.8106 - val_specificity_at_sensitivity: 0.7302 - val_sensitivity_at_specificity: 0.7463\n",
      "Epoch 52/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1661 - acc: 0.9194 - specificity_at_sensitivity: 0.7346 - sensitivity_at_specificity: 0.7515 - val_loss: 0.5878 - val_acc: 0.8314 - val_specificity_at_sensitivity: 0.7382 - val_sensitivity_at_specificity: 0.7542\n",
      "Epoch 53/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1575 - acc: 0.9221 - specificity_at_sensitivity: 0.7398 - sensitivity_at_specificity: 0.7598 - val_loss: 0.7020 - val_acc: 0.8214 - val_specificity_at_sensitivity: 0.7414 - val_sensitivity_at_specificity: 0.7623\n",
      "Epoch 54/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1704 - acc: 0.9173 - specificity_at_sensitivity: 0.7450 - sensitivity_at_specificity: 0.7672 - val_loss: 0.6020 - val_acc: 0.8226 - val_specificity_at_sensitivity: 0.7490 - val_sensitivity_at_specificity: 0.7699\n",
      "Epoch 55/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1404 - acc: 0.9285 - specificity_at_sensitivity: 0.7505 - sensitivity_at_specificity: 0.7743 - val_loss: 0.6782 - val_acc: 0.8202 - val_specificity_at_sensitivity: 0.7519 - val_sensitivity_at_specificity: 0.7774\n",
      "Epoch 56/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1540 - acc: 0.9261 - specificity_at_sensitivity: 0.7541 - sensitivity_at_specificity: 0.7812 - val_loss: 0.7019 - val_acc: 0.8132 - val_specificity_at_sensitivity: 0.7570 - val_sensitivity_at_specificity: 0.7844\n",
      "Epoch 57/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1571 - acc: 0.9256 - specificity_at_sensitivity: 0.7607 - sensitivity_at_specificity: 0.7876 - val_loss: 0.6708 - val_acc: 0.8146 - val_specificity_at_sensitivity: 0.7620 - val_sensitivity_at_specificity: 0.7911\n",
      "Epoch 58/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1400 - acc: 0.9304 - specificity_at_sensitivity: 0.7633 - sensitivity_at_specificity: 0.7944 - val_loss: 0.6809 - val_acc: 0.8196 - val_specificity_at_sensitivity: 0.7646 - val_sensitivity_at_specificity: 0.7978\n",
      "Epoch 59/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1445 - acc: 0.9306 - specificity_at_sensitivity: 0.7689 - sensitivity_at_specificity: 0.8005 - val_loss: 0.7101 - val_acc: 0.8204 - val_specificity_at_sensitivity: 0.7717 - val_sensitivity_at_specificity: 0.8041\n",
      "Epoch 60/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1397 - acc: 0.9364 - specificity_at_sensitivity: 0.7730 - sensitivity_at_specificity: 0.8066 - val_loss: 0.6390 - val_acc: 0.8196 - val_specificity_at_sensitivity: 0.7742 - val_sensitivity_at_specificity: 0.8104\n",
      "Epoch 61/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1427 - acc: 0.9353 - specificity_at_sensitivity: 0.7755 - sensitivity_at_specificity: 0.8127 - val_loss: 0.6622 - val_acc: 0.8166 - val_specificity_at_sensitivity: 0.7766 - val_sensitivity_at_specificity: 0.8163\n",
      "Epoch 62/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1383 - acc: 0.9351 - specificity_at_sensitivity: 0.7822 - sensitivity_at_specificity: 0.8185 - val_loss: 0.7063 - val_acc: 0.8204 - val_specificity_at_sensitivity: 0.7837 - val_sensitivity_at_specificity: 0.8221\n",
      "Epoch 63/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1221 - acc: 0.9450 - specificity_at_sensitivity: 0.7849 - sensitivity_at_specificity: 0.8240 - val_loss: 0.6941 - val_acc: 0.8268 - val_specificity_at_sensitivity: 0.7860 - val_sensitivity_at_specificity: 0.8279\n",
      "Epoch 64/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1362 - acc: 0.9394 - specificity_at_sensitivity: 0.7879 - sensitivity_at_specificity: 0.8295 - val_loss: 0.9338 - val_acc: 0.8112 - val_specificity_at_sensitivity: 0.7907 - val_sensitivity_at_specificity: 0.8331\n",
      "Epoch 65/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1250 - acc: 0.9439 - specificity_at_sensitivity: 0.7943 - sensitivity_at_specificity: 0.8349 - val_loss: 0.7825 - val_acc: 0.8140 - val_specificity_at_sensitivity: 0.7954 - val_sensitivity_at_specificity: 0.8385\n",
      "Epoch 66/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1369 - acc: 0.9425 - specificity_at_sensitivity: 0.7965 - sensitivity_at_specificity: 0.8397 - val_loss: 0.7871 - val_acc: 0.8190 - val_specificity_at_sensitivity: 0.7975 - val_sensitivity_at_specificity: 0.8434\n",
      "Epoch 67/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.1292 - acc: 0.9418 - specificity_at_sensitivity: 0.7986 - sensitivity_at_specificity: 0.8444 - val_loss: 0.7365 - val_acc: 0.8286 - val_specificity_at_sensitivity: 0.7995 - val_sensitivity_at_specificity: 0.8482\n",
      "Epoch 68/150\n",
      "7050/7050 [==============================] - 0s 15us/step - loss: 0.1134 - acc: 0.9506 - specificity_at_sensitivity: 0.8038 - sensitivity_at_specificity: 0.8490 - val_loss: 0.9624 - val_acc: 0.8090 - val_specificity_at_sensitivity: 0.8065 - val_sensitivity_at_specificity: 0.8532\n",
      "Epoch 69/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1185 - acc: 0.9465 - specificity_at_sensitivity: 0.8075 - sensitivity_at_specificity: 0.8538 - val_loss: 0.8563 - val_acc: 0.8178 - val_specificity_at_sensitivity: 0.8084 - val_sensitivity_at_specificity: 0.8541\n",
      "Epoch 70/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1227 - acc: 0.9452 - specificity_at_sensitivity: 0.8095 - sensitivity_at_specificity: 0.8583 - val_loss: 0.8745 - val_acc: 0.8192 - val_specificity_at_sensitivity: 0.8103 - val_sensitivity_at_specificity: 0.8586\n",
      "Epoch 71/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1323 - acc: 0.9419 - specificity_at_sensitivity: 0.8113 - sensitivity_at_specificity: 0.8624 - val_loss: 0.9702 - val_acc: 0.8228 - val_specificity_at_sensitivity: 0.8121 - val_sensitivity_at_specificity: 0.8629\n",
      "Epoch 72/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1103 - acc: 0.9518 - specificity_at_sensitivity: 0.8177 - sensitivity_at_specificity: 0.8665 - val_loss: 0.8780 - val_acc: 0.8190 - val_specificity_at_sensitivity: 0.8189 - val_sensitivity_at_specificity: 0.8673\n",
      "Epoch 73/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1098 - acc: 0.9526 - specificity_at_sensitivity: 0.8199 - sensitivity_at_specificity: 0.8705 - val_loss: 0.8594 - val_acc: 0.8152 - val_specificity_at_sensitivity: 0.8207 - val_sensitivity_at_specificity: 0.8714\n",
      "Epoch 74/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1163 - acc: 0.9497 - specificity_at_sensitivity: 0.8216 - sensitivity_at_specificity: 0.8743 - val_loss: 0.8394 - val_acc: 0.8258 - val_specificity_at_sensitivity: 0.8224 - val_sensitivity_at_specificity: 0.8754\n",
      "Epoch 75/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1134 - acc: 0.9484 - specificity_at_sensitivity: 0.8241 - sensitivity_at_specificity: 0.8778 - val_loss: 0.7921 - val_acc: 0.8218 - val_specificity_at_sensitivity: 0.8267 - val_sensitivity_at_specificity: 0.8794\n",
      "Epoch 76/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0991 - acc: 0.9563 - specificity_at_sensitivity: 0.8301 - sensitivity_at_specificity: 0.8815 - val_loss: 0.8540 - val_acc: 0.8240 - val_specificity_at_sensitivity: 0.8309 - val_sensitivity_at_specificity: 0.8833\n",
      "Epoch 77/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1098 - acc: 0.9515 - specificity_at_sensitivity: 0.8317 - sensitivity_at_specificity: 0.8849 - val_loss: 0.7682 - val_acc: 0.8230 - val_specificity_at_sensitivity: 0.8325 - val_sensitivity_at_specificity: 0.8871\n",
      "Epoch 78/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0976 - acc: 0.9571 - specificity_at_sensitivity: 0.8333 - sensitivity_at_specificity: 0.8884 - val_loss: 0.8239 - val_acc: 0.8194 - val_specificity_at_sensitivity: 0.8340 - val_sensitivity_at_specificity: 0.8907\n",
      "Epoch 79/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1008 - acc: 0.9559 - specificity_at_sensitivity: 0.8348 - sensitivity_at_specificity: 0.8916 - val_loss: 1.0040 - val_acc: 0.8176 - val_specificity_at_sensitivity: 0.8356 - val_sensitivity_at_specificity: 0.8942\n",
      "Epoch 80/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1030 - acc: 0.9534 - specificity_at_sensitivity: 0.8408 - sensitivity_at_specificity: 0.8946 - val_loss: 0.8765 - val_acc: 0.8312 - val_specificity_at_sensitivity: 0.8423 - val_sensitivity_at_specificity: 0.8948\n",
      "Epoch 81/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1174 - acc: 0.9499 - specificity_at_sensitivity: 0.8430 - sensitivity_at_specificity: 0.8978 - val_loss: 0.7946 - val_acc: 0.8196 - val_specificity_at_sensitivity: 0.8436 - val_sensitivity_at_specificity: 0.8978\n",
      "Epoch 82/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0980 - acc: 0.9596 - specificity_at_sensitivity: 0.8444 - sensitivity_at_specificity: 0.9004 - val_loss: 0.9762 - val_acc: 0.8234 - val_specificity_at_sensitivity: 0.8451 - val_sensitivity_at_specificity: 0.9012\n",
      "Epoch 83/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1121 - acc: 0.9533 - specificity_at_sensitivity: 0.8458 - sensitivity_at_specificity: 0.9031 - val_loss: 0.7045 - val_acc: 0.8200 - val_specificity_at_sensitivity: 0.8464 - val_sensitivity_at_specificity: 0.9043\n",
      "Epoch 84/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0920 - acc: 0.9603 - specificity_at_sensitivity: 0.8484 - sensitivity_at_specificity: 0.9056 - val_loss: 0.9188 - val_acc: 0.8202 - val_specificity_at_sensitivity: 0.8534 - val_sensitivity_at_specificity: 0.9075\n",
      "Epoch 85/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0903 - acc: 0.9607 - specificity_at_sensitivity: 0.8541 - sensitivity_at_specificity: 0.9082 - val_loss: 0.9789 - val_acc: 0.8266 - val_specificity_at_sensitivity: 0.8547 - val_sensitivity_at_specificity: 0.9105\n",
      "Epoch 86/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1011 - acc: 0.9560 - specificity_at_sensitivity: 0.8554 - sensitivity_at_specificity: 0.9108 - val_loss: 0.9148 - val_acc: 0.8284 - val_specificity_at_sensitivity: 0.8560 - val_sensitivity_at_specificity: 0.9108\n",
      "Epoch 87/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0824 - acc: 0.9648 - specificity_at_sensitivity: 0.8567 - sensitivity_at_specificity: 0.9135 - val_loss: 1.0590 - val_acc: 0.8282 - val_specificity_at_sensitivity: 0.8573 - val_sensitivity_at_specificity: 0.9137\n",
      "Epoch 88/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0985 - acc: 0.9590 - specificity_at_sensitivity: 0.8579 - sensitivity_at_specificity: 0.9157 - val_loss: 0.9034 - val_acc: 0.8264 - val_specificity_at_sensitivity: 0.8585 - val_sensitivity_at_specificity: 0.9165\n",
      "Epoch 89/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0931 - acc: 0.9599 - specificity_at_sensitivity: 0.8595 - sensitivity_at_specificity: 0.9178 - val_loss: 0.8799 - val_acc: 0.8252 - val_specificity_at_sensitivity: 0.8596 - val_sensitivity_at_specificity: 0.9192\n",
      "Epoch 90/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0910 - acc: 0.9630 - specificity_at_sensitivity: 0.8663 - sensitivity_at_specificity: 0.9196 - val_loss: 1.1504 - val_acc: 0.8254 - val_specificity_at_sensitivity: 0.8668 - val_sensitivity_at_specificity: 0.9218\n",
      "Epoch 91/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0937 - acc: 0.9606 - specificity_at_sensitivity: 0.8674 - sensitivity_at_specificity: 0.9221 - val_loss: 1.0968 - val_acc: 0.8212 - val_specificity_at_sensitivity: 0.8679 - val_sensitivity_at_specificity: 0.9220\n",
      "Epoch 92/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0877 - acc: 0.9632 - specificity_at_sensitivity: 0.8685 - sensitivity_at_specificity: 0.9239 - val_loss: 1.1001 - val_acc: 0.8280 - val_specificity_at_sensitivity: 0.8690 - val_sensitivity_at_specificity: 0.9246\n",
      "Epoch 93/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0806 - acc: 0.9664 - specificity_at_sensitivity: 0.8695 - sensitivity_at_specificity: 0.9256 - val_loss: 0.9142 - val_acc: 0.8288 - val_specificity_at_sensitivity: 0.8700 - val_sensitivity_at_specificity: 0.9270\n",
      "Epoch 94/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0813 - acc: 0.9668 - specificity_at_sensitivity: 0.8716 - sensitivity_at_specificity: 0.9272 - val_loss: 0.9957 - val_acc: 0.8250 - val_specificity_at_sensitivity: 0.8760 - val_sensitivity_at_specificity: 0.9272\n",
      "Epoch 95/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0818 - acc: 0.9660 - specificity_at_sensitivity: 0.8765 - sensitivity_at_specificity: 0.9296 - val_loss: 1.3225 - val_acc: 0.8198 - val_specificity_at_sensitivity: 0.8769 - val_sensitivity_at_specificity: 0.9295\n",
      "Epoch 96/150\n",
      "7050/7050 [==============================] - ETA: 0s - loss: 0.0916 - acc: 0.9647 - specificity_at_sensitivity: 0.8773 - sensitivity_at_specificity: 0.930 - 0s 14us/step - loss: 0.0882 - acc: 0.9657 - specificity_at_sensitivity: 0.8774 - sensitivity_at_specificity: 0.9309 - val_loss: 0.9272 - val_acc: 0.8324 - val_specificity_at_sensitivity: 0.8779 - val_sensitivity_at_specificity: 0.9318\n",
      "Epoch 97/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0830 - acc: 0.9650 - specificity_at_sensitivity: 0.8784 - sensitivity_at_specificity: 0.9323 - val_loss: 1.1480 - val_acc: 0.8284 - val_specificity_at_sensitivity: 0.8788 - val_sensitivity_at_specificity: 0.9341\n",
      "Epoch 98/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0842 - acc: 0.9664 - specificity_at_sensitivity: 0.8793 - sensitivity_at_specificity: 0.9343 - val_loss: 1.0624 - val_acc: 0.8252 - val_specificity_at_sensitivity: 0.8797 - val_sensitivity_at_specificity: 0.9342\n",
      "Epoch 99/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0723 - acc: 0.9710 - specificity_at_sensitivity: 0.8833 - sensitivity_at_specificity: 0.9358 - val_loss: 0.9885 - val_acc: 0.8334 - val_specificity_at_sensitivity: 0.8854 - val_sensitivity_at_specificity: 0.9365\n",
      "Epoch 100/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.1114 - acc: 0.9618 - specificity_at_sensitivity: 0.8858 - sensitivity_at_specificity: 0.9366 - val_loss: 1.0266 - val_acc: 0.8308 - val_specificity_at_sensitivity: 0.8862 - val_sensitivity_at_specificity: 0.9385\n",
      "Epoch 101/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0647 - acc: 0.9742 - specificity_at_sensitivity: 0.8866 - sensitivity_at_specificity: 0.9387 - val_loss: 1.1363 - val_acc: 0.8260 - val_specificity_at_sensitivity: 0.8870 - val_sensitivity_at_specificity: 0.9387\n",
      "Epoch 102/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0852 - acc: 0.9658 - specificity_at_sensitivity: 0.8875 - sensitivity_at_specificity: 0.9398 - val_loss: 0.9101 - val_acc: 0.8172 - val_specificity_at_sensitivity: 0.8878 - val_sensitivity_at_specificity: 0.9407\n",
      "Epoch 103/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0786 - acc: 0.9700 - specificity_at_sensitivity: 0.8900 - sensitivity_at_specificity: 0.9409 - val_loss: 1.1814 - val_acc: 0.8258 - val_specificity_at_sensitivity: 0.8934 - val_sensitivity_at_specificity: 0.9408\n",
      "Epoch 104/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0663 - acc: 0.9735 - specificity_at_sensitivity: 0.8938 - sensitivity_at_specificity: 0.9425 - val_loss: 0.8812 - val_acc: 0.8266 - val_specificity_at_sensitivity: 0.8942 - val_sensitivity_at_specificity: 0.9429\n",
      "Epoch 105/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0815 - acc: 0.9676 - specificity_at_sensitivity: 0.8946 - sensitivity_at_specificity: 0.9430 - val_loss: 1.1911 - val_acc: 0.8266 - val_specificity_at_sensitivity: 0.8949 - val_sensitivity_at_specificity: 0.9448\n",
      "Epoch 106/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0717 - acc: 0.9722 - specificity_at_sensitivity: 0.8953 - sensitivity_at_specificity: 0.9450 - val_loss: 1.0611 - val_acc: 0.8248 - val_specificity_at_sensitivity: 0.8956 - val_sensitivity_at_specificity: 0.9449\n",
      "Epoch 107/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0799 - acc: 0.9699 - specificity_at_sensitivity: 0.8976 - sensitivity_at_specificity: 0.9456 - val_loss: 0.9870 - val_acc: 0.8372 - val_specificity_at_sensitivity: 0.9010 - val_sensitivity_at_specificity: 0.9469\n",
      "Epoch 108/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0730 - acc: 0.9720 - specificity_at_sensitivity: 0.9013 - sensitivity_at_specificity: 0.9471 - val_loss: 1.0683 - val_acc: 0.8312 - val_specificity_at_sensitivity: 0.9016 - val_sensitivity_at_specificity: 0.9470\n",
      "Epoch 109/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0620 - acc: 0.9763 - specificity_at_sensitivity: 0.9020 - sensitivity_at_specificity: 0.9480 - val_loss: 1.0867 - val_acc: 0.8268 - val_specificity_at_sensitivity: 0.9023 - val_sensitivity_at_specificity: 0.9489\n",
      "Epoch 110/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0871 - acc: 0.9651 - specificity_at_sensitivity: 0.9026 - sensitivity_at_specificity: 0.9490 - val_loss: 1.0751 - val_acc: 0.8276 - val_specificity_at_sensitivity: 0.9029 - val_sensitivity_at_specificity: 0.9489\n",
      "Epoch 111/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0692 - acc: 0.9765 - specificity_at_sensitivity: 0.9055 - sensitivity_at_specificity: 0.9501 - val_loss: 1.1197 - val_acc: 0.8302 - val_specificity_at_sensitivity: 0.9087 - val_sensitivity_at_specificity: 0.9508\n",
      "Epoch 112/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0695 - acc: 0.9755 - specificity_at_sensitivity: 0.9090 - sensitivity_at_specificity: 0.9509 - val_loss: 0.9564 - val_acc: 0.8268 - val_specificity_at_sensitivity: 0.9093 - val_sensitivity_at_specificity: 0.9509\n",
      "Epoch 113/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0614 - acc: 0.9766 - specificity_at_sensitivity: 0.9096 - sensitivity_at_specificity: 0.9519 - val_loss: 1.0715 - val_acc: 0.8326 - val_specificity_at_sensitivity: 0.9099 - val_sensitivity_at_specificity: 0.9527\n",
      "Epoch 114/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0655 - acc: 0.9750 - specificity_at_sensitivity: 0.9102 - sensitivity_at_specificity: 0.9529 - val_loss: 1.3213 - val_acc: 0.8298 - val_specificity_at_sensitivity: 0.9104 - val_sensitivity_at_specificity: 0.9528\n",
      "Epoch 115/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0711 - acc: 0.9724 - specificity_at_sensitivity: 0.9123 - sensitivity_at_specificity: 0.9535 - val_loss: 1.1621 - val_acc: 0.8350 - val_specificity_at_sensitivity: 0.9164 - val_sensitivity_at_specificity: 0.9544\n",
      "Epoch 116/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0665 - acc: 0.9746 - specificity_at_sensitivity: 0.9167 - sensitivity_at_specificity: 0.9546 - val_loss: 1.2894 - val_acc: 0.8332 - val_specificity_at_sensitivity: 0.9169 - val_sensitivity_at_specificity: 0.9545\n",
      "Epoch 117/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0555 - acc: 0.9796 - specificity_at_sensitivity: 0.9172 - sensitivity_at_specificity: 0.9548 - val_loss: 1.2921 - val_acc: 0.8372 - val_specificity_at_sensitivity: 0.9174 - val_sensitivity_at_specificity: 0.9562\n",
      "Epoch 118/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0640 - acc: 0.9764 - specificity_at_sensitivity: 0.9177 - sensitivity_at_specificity: 0.9563 - val_loss: 1.2947 - val_acc: 0.8306 - val_specificity_at_sensitivity: 0.9179 - val_sensitivity_at_specificity: 0.9562\n",
      "Epoch 119/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0663 - acc: 0.9758 - specificity_at_sensitivity: 0.9181 - sensitivity_at_specificity: 0.9563 - val_loss: 1.4080 - val_acc: 0.8332 - val_specificity_at_sensitivity: 0.9183 - val_sensitivity_at_specificity: 0.9562\n",
      "Epoch 120/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0583 - acc: 0.9778 - specificity_at_sensitivity: 0.9235 - sensitivity_at_specificity: 0.9576 - val_loss: 1.0951 - val_acc: 0.8340 - val_specificity_at_sensitivity: 0.9245 - val_sensitivity_at_specificity: 0.9578\n",
      "Epoch 121/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0567 - acc: 0.9803 - specificity_at_sensitivity: 0.9248 - sensitivity_at_specificity: 0.9579 - val_loss: 1.4391 - val_acc: 0.8296 - val_specificity_at_sensitivity: 0.9249 - val_sensitivity_at_specificity: 0.9578\n",
      "Epoch 122/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0567 - acc: 0.9793 - specificity_at_sensitivity: 0.9252 - sensitivity_at_specificity: 0.9585 - val_loss: 1.4334 - val_acc: 0.8208 - val_specificity_at_sensitivity: 0.9253 - val_sensitivity_at_specificity: 0.9595\n",
      "Epoch 123/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0590 - acc: 0.9793 - specificity_at_sensitivity: 0.9256 - sensitivity_at_specificity: 0.9595 - val_loss: 1.3917 - val_acc: 0.8334 - val_specificity_at_sensitivity: 0.9258 - val_sensitivity_at_specificity: 0.9595\n",
      "Epoch 124/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0672 - acc: 0.9760 - specificity_at_sensitivity: 0.9269 - sensitivity_at_specificity: 0.9595 - val_loss: 1.4529 - val_acc: 0.8316 - val_specificity_at_sensitivity: 0.9292 - val_sensitivity_at_specificity: 0.9594\n",
      "Epoch 125/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0554 - acc: 0.9808 - specificity_at_sensitivity: 0.9325 - sensitivity_at_specificity: 0.9605 - val_loss: 1.2417 - val_acc: 0.8278 - val_specificity_at_sensitivity: 0.9326 - val_sensitivity_at_specificity: 0.9611\n",
      "Epoch 126/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0470 - acc: 0.9832 - specificity_at_sensitivity: 0.9328 - sensitivity_at_specificity: 0.9611 - val_loss: 1.3103 - val_acc: 0.8318 - val_specificity_at_sensitivity: 0.9330 - val_sensitivity_at_specificity: 0.9611\n",
      "Epoch 127/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0872 - acc: 0.9734 - specificity_at_sensitivity: 0.9332 - sensitivity_at_specificity: 0.9611 - val_loss: 1.2226 - val_acc: 0.8278 - val_specificity_at_sensitivity: 0.9333 - val_sensitivity_at_specificity: 0.9610\n",
      "Epoch 128/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0439 - acc: 0.9852 - specificity_at_sensitivity: 0.9335 - sensitivity_at_specificity: 0.9623 - val_loss: 1.2662 - val_acc: 0.8270 - val_specificity_at_sensitivity: 0.9336 - val_sensitivity_at_specificity: 0.9626\n",
      "Epoch 129/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0443 - acc: 0.9847 - specificity_at_sensitivity: 0.9338 - sensitivity_at_specificity: 0.9626 - val_loss: 1.4278 - val_acc: 0.8282 - val_specificity_at_sensitivity: 0.9340 - val_sensitivity_at_specificity: 0.9626\n",
      "Epoch 130/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0623 - acc: 0.9770 - specificity_at_sensitivity: 0.9347 - sensitivity_at_specificity: 0.9626 - val_loss: 1.3235 - val_acc: 0.8230 - val_specificity_at_sensitivity: 0.9343 - val_sensitivity_at_specificity: 0.9625\n",
      "Epoch 131/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0462 - acc: 0.9838 - specificity_at_sensitivity: 0.9424 - sensitivity_at_specificity: 0.9639 - val_loss: 1.3179 - val_acc: 0.8300 - val_specificity_at_sensitivity: 0.9425 - val_sensitivity_at_specificity: 0.9641\n",
      "Epoch 132/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0592 - acc: 0.9793 - specificity_at_sensitivity: 0.9426 - sensitivity_at_specificity: 0.9642 - val_loss: 1.3293 - val_acc: 0.8252 - val_specificity_at_sensitivity: 0.9427 - val_sensitivity_at_specificity: 0.9641\n",
      "Epoch 133/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0341 - acc: 0.9885 - specificity_at_sensitivity: 0.9429 - sensitivity_at_specificity: 0.9642 - val_loss: 1.6496 - val_acc: 0.8252 - val_specificity_at_sensitivity: 0.9430 - val_sensitivity_at_specificity: 0.9641\n",
      "Epoch 134/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0727 - acc: 0.9768 - specificity_at_sensitivity: 0.9432 - sensitivity_at_specificity: 0.9646 - val_loss: 1.5075 - val_acc: 0.8228 - val_specificity_at_sensitivity: 0.9432 - val_sensitivity_at_specificity: 0.9655\n",
      "Epoch 135/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0427 - acc: 0.9875 - specificity_at_sensitivity: 0.9434 - sensitivity_at_specificity: 0.9655 - val_loss: 1.4091 - val_acc: 0.8272 - val_specificity_at_sensitivity: 0.9435 - val_sensitivity_at_specificity: 0.9655\n",
      "Epoch 136/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0498 - acc: 0.9830 - specificity_at_sensitivity: 0.9437 - sensitivity_at_specificity: 0.9655 - val_loss: 1.4240 - val_acc: 0.8334 - val_specificity_at_sensitivity: 0.9438 - val_sensitivity_at_specificity: 0.9655\n",
      "Epoch 137/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0656 - acc: 0.9790 - specificity_at_sensitivity: 0.9481 - sensitivity_at_specificity: 0.9655 - val_loss: 1.5552 - val_acc: 0.8236 - val_specificity_at_sensitivity: 0.9512 - val_sensitivity_at_specificity: 0.9654\n",
      "Epoch 138/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0317 - acc: 0.9893 - specificity_at_sensitivity: 0.9514 - sensitivity_at_specificity: 0.9665 - val_loss: 1.5146 - val_acc: 0.8340 - val_specificity_at_sensitivity: 0.9515 - val_sensitivity_at_specificity: 0.9669\n",
      "Epoch 139/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0654 - acc: 0.9786 - specificity_at_sensitivity: 0.9516 - sensitivity_at_specificity: 0.9670 - val_loss: 1.4711 - val_acc: 0.8322 - val_specificity_at_sensitivity: 0.9516 - val_sensitivity_at_specificity: 0.9669\n",
      "Epoch 140/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0484 - acc: 0.9853 - specificity_at_sensitivity: 0.9518 - sensitivity_at_specificity: 0.9669 - val_loss: 1.4065 - val_acc: 0.8314 - val_specificity_at_sensitivity: 0.9518 - val_sensitivity_at_specificity: 0.9668\n",
      "Epoch 141/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0336 - acc: 0.9885 - specificity_at_sensitivity: 0.9520 - sensitivity_at_specificity: 0.9669 - val_loss: 1.4961 - val_acc: 0.8308 - val_specificity_at_sensitivity: 0.9520 - val_sensitivity_at_specificity: 0.9668\n",
      "Epoch 142/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0535 - acc: 0.9826 - specificity_at_sensitivity: 0.9522 - sensitivity_at_specificity: 0.9679 - val_loss: 1.3056 - val_acc: 0.8352 - val_specificity_at_sensitivity: 0.9522 - val_sensitivity_at_specificity: 0.9683\n",
      "Epoch 143/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0351 - acc: 0.9888 - specificity_at_sensitivity: 0.9567 - sensitivity_at_specificity: 0.9684 - val_loss: 1.6901 - val_acc: 0.8276 - val_specificity_at_sensitivity: 0.9592 - val_sensitivity_at_specificity: 0.9683\n",
      "Epoch 144/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0526 - acc: 0.9821 - specificity_at_sensitivity: 0.9592 - sensitivity_at_specificity: 0.9683 - val_loss: 1.3294 - val_acc: 0.8352 - val_specificity_at_sensitivity: 0.9593 - val_sensitivity_at_specificity: 0.9682\n",
      "Epoch 145/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0423 - acc: 0.9852 - specificity_at_sensitivity: 0.9594 - sensitivity_at_specificity: 0.9683 - val_loss: 1.5185 - val_acc: 0.8310 - val_specificity_at_sensitivity: 0.9594 - val_sensitivity_at_specificity: 0.9682\n",
      "Epoch 146/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0628 - acc: 0.9799 - specificity_at_sensitivity: 0.9595 - sensitivity_at_specificity: 0.9683 - val_loss: 1.6256 - val_acc: 0.8286 - val_specificity_at_sensitivity: 0.9595 - val_sensitivity_at_specificity: 0.9698\n",
      "Epoch 147/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0317 - acc: 0.9902 - specificity_at_sensitivity: 0.9596 - sensitivity_at_specificity: 0.9698 - val_loss: 1.5349 - val_acc: 0.8322 - val_specificity_at_sensitivity: 0.9597 - val_sensitivity_at_specificity: 0.9697\n",
      "Epoch 148/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0613 - acc: 0.9813 - specificity_at_sensitivity: 0.9610 - sensitivity_at_specificity: 0.9698 - val_loss: 1.4731 - val_acc: 0.8210 - val_specificity_at_sensitivity: 0.9655 - val_sensitivity_at_specificity: 0.9697\n",
      "Epoch 149/150\n",
      "7050/7050 [==============================] - 0s 14us/step - loss: 0.0590 - acc: 0.9809 - specificity_at_sensitivity: 0.9656 - sensitivity_at_specificity: 0.9697 - val_loss: 1.5750 - val_acc: 0.8334 - val_specificity_at_sensitivity: 0.9656 - val_sensitivity_at_specificity: 0.9696\n",
      "Epoch 150/150\n",
      "7050/7050 [==============================] - 0s 13us/step - loss: 0.0358 - acc: 0.9872 - specificity_at_sensitivity: 0.9657 - sensitivity_at_specificity: 0.9697 - val_loss: 1.6010 - val_acc: 0.8348 - val_specificity_at_sensitivity: 0.9657 - val_sensitivity_at_specificity: 0.9696\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "history = model.fit(partial_x_train,partial_y_train,epochs = 150,batch_size = 512, validation_data = (x_val,y_val))\n",
    "model.save(\"MLP1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(loss) + 1)\n",
    "plt.plot(epochs,loss,label='Training loss')\n",
    "plt.plot(epochs,val_loss,label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3450/3450 [==============================] - 0s 23us/step\n",
      "[1.7719581879740176, 0.8323478102684021, 0.9655757546424866, 0.9691815972328186]\n"
     ]
    }
   ],
   "source": [
    "result =model.evaluate(test,Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
