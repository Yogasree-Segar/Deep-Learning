{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X21.V1.791</td>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>...</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X15.V1.924</td>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X8.V1.1</td>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X16.V1.60</td>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>...</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X20.V1.54</td>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   X1   X2   X3   X4   X5   X6   X7   X8   X9  ...  X170  X171  \\\n",
       "0  X21.V1.791  135  190  229  223  192  125   55   -9  -33  ...   -17   -15   \n",
       "1  X15.V1.924  386  382  356  331  320  315  307  272  244  ...   164   150   \n",
       "2     X8.V1.1  -32  -39  -47  -37  -32  -36  -57  -73  -85  ...    57    64   \n",
       "3   X16.V1.60 -105 -101  -96  -92  -89  -95 -102 -100  -87  ...   -82   -81   \n",
       "4   X20.V1.54   -9  -65  -98 -102  -78  -48  -16    0  -21  ...     4     2   \n",
       "\n",
       "   X172  X173  X174  X175  X176  X177  X178  y  \n",
       "0   -31   -77  -103  -127  -116   -83   -51  4  \n",
       "1   146   152   157   156   154   143   129  1  \n",
       "2    48    19   -12   -30   -35   -35   -36  5  \n",
       "3   -80   -77   -85   -77   -72   -69   -65  5  \n",
       "4   -12   -32   -41   -65   -83   -89   -73  5  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "rng = RandomState()\n",
    "\n",
    "train = dataset.sample(frac=0.7, random_state=rng)\n",
    "test = dataset.loc[~dataset.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train.iloc[:,-1]\n",
    "test_label = test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['y'], axis=1)\n",
    "test = test.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 3 ... 0 0 1]\n",
      "[3 3 1 ... 4 2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_label)\n",
    "encoded_train = encoder.transform(train_label)\n",
    "print(encoded_train)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_label)\n",
    "encoded_test = encoder.transform(test_label)\n",
    "print(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(encoded_train)\n",
    "Y_test = to_categorical(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0   X1   X2   X3   X4   X5   X6   X7   X8   X9  ...  X169  \\\n",
      "2100    X10.V1.73   25    6  -11  -25  -47  -73  -92  -92  -78  ...    39   \n",
      "3392   X21.V1.741   24   24    1  -24  -40  -53  -58  -37   -2  ...    87   \n",
      "5276    X1.V1.101 -159 -176 -174 -145 -101  -45    0   29   15  ...    -6   \n",
      "3366   X12.V1.114    8   12   14   12    5    1    2    8   11  ...  -138   \n",
      "2221    X3.V1.303   33  -71 -142 -174 -159 -116  -92  -84  -61  ...   -93   \n",
      "...           ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
      "1077    X3.V1.612 -102 -102  -96  -89  -81  -73  -47  -35  -23  ...     8   \n",
      "10711  X22.V1.964  -85    1   80  142  209  254  318  385  434  ...   118   \n",
      "4730    X2.V1.854  -97 -104 -101  -91  -60  -39 -131 -231 -247  ...   -21   \n",
      "8736    X1.V1.934 -155 -283 -456 -541 -474 -389 -288 -233 -150  ...   170   \n",
      "3121    X3.V1.283 -168 -157 -146 -140 -136 -139 -148 -157 -166  ...   -19   \n",
      "\n",
      "       X170  X171  X172  X173  X174  X175  X176  X177  X178  \n",
      "2100     48    63    58    46    20     5    -4     0    -8  \n",
      "3392     75    55    43    34    42    60    83    88    84  \n",
      "5276    -58   -93  -108  -101   -86   -66   -34    34   139  \n",
      "3366   -139  -136  -133  -127  -127  -130  -134  -131  -126  \n",
      "2221    -80   -48   -59   -56   -46   -29    18    94   123  \n",
      "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "1077     20    28    30    36    39    51    63    78    88  \n",
      "10711   171   218   258   293   318   337   351   372   390  \n",
      "4730     18    59    91   103   105   119   138   163   170  \n",
      "8736    316   361   374   294   225   212   231   255   256  \n",
      "3121    -23   -24   -29   -26   -27   -22    -8    -4     0  \n",
      "\n",
      "[8050 rows x 179 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['Unnamed: 0'], axis=1)\n",
    "test = test.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train[:1000]\n",
    "partial_x_train = train[1000:]\n",
    "y_val = Y_train[:1000]\n",
    "partial_y_train = Y_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7050, 178)\n"
     ]
    }
   ],
   "source": [
    "print(partial_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(512,activation = 'relu',input_shape = (178,)))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(512,activation='relu'))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(512,activation='relu'))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(512,activation='relu'))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(5,activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc',tf.keras.metrics.SpecificityAtSensitivity(0.7),tf.keras.metrics.SensitivityAtSpecificity(0.7)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7050 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7050/7050 [==============================] - 1s 89us/step - loss: 11.8491 - acc: 0.7012 - specificity_at_sensitivity: 0.2740 - sensitivity_at_specificity: 0.3100 - val_loss: 0.7484 - val_acc: 0.6946 - val_specificity_at_sensitivity: 0.1415 - val_sensitivity_at_specificity: 0.3393\n",
      "Epoch 2/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.9224 - acc: 0.7307 - specificity_at_sensitivity: 0.5026 - sensitivity_at_specificity: 0.3697 - val_loss: 0.5975 - val_acc: 0.7508 - val_specificity_at_sensitivity: 0.4377 - val_sensitivity_at_specificity: 0.3908\n",
      "Epoch 3/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.6750 - acc: 0.7509 - specificity_at_sensitivity: 0.4221 - sensitivity_at_specificity: 0.4108 - val_loss: 0.5122 - val_acc: 0.7862 - val_specificity_at_sensitivity: 0.4288 - val_sensitivity_at_specificity: 0.4235\n",
      "Epoch 4/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.5859 - acc: 0.7693 - specificity_at_sensitivity: 0.4409 - sensitivity_at_specificity: 0.4368 - val_loss: 0.4933 - val_acc: 0.7578 - val_specificity_at_sensitivity: 0.4496 - val_sensitivity_at_specificity: 0.4470\n",
      "Epoch 5/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.5179 - acc: 0.7808 - specificity_at_sensitivity: 0.4605 - sensitivity_at_specificity: 0.4571 - val_loss: 0.4538 - val_acc: 0.8006 - val_specificity_at_sensitivity: 0.4703 - val_sensitivity_at_specificity: 0.4689\n",
      "Epoch 6/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.4850 - acc: 0.7956 - specificity_at_sensitivity: 0.4802 - sensitivity_at_specificity: 0.4779 - val_loss: 0.4360 - val_acc: 0.8262 - val_specificity_at_sensitivity: 0.4875 - val_sensitivity_at_specificity: 0.4908\n",
      "Epoch 7/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.4498 - acc: 0.8083 - specificity_at_sensitivity: 0.5000 - sensitivity_at_specificity: 0.5006 - val_loss: 0.4147 - val_acc: 0.8250 - val_specificity_at_sensitivity: 0.5111 - val_sensitivity_at_specificity: 0.5108\n",
      "Epoch 8/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.4256 - acc: 0.8157 - specificity_at_sensitivity: 0.5203 - sensitivity_at_specificity: 0.5182 - val_loss: 0.4096 - val_acc: 0.8308 - val_specificity_at_sensitivity: 0.5306 - val_sensitivity_at_specificity: 0.5296\n",
      "Epoch 9/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.4253 - acc: 0.8196 - specificity_at_sensitivity: 0.5402 - sensitivity_at_specificity: 0.5401 - val_loss: 0.4148 - val_acc: 0.8282 - val_specificity_at_sensitivity: 0.5464 - val_sensitivity_at_specificity: 0.5477\n",
      "Epoch 10/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.3966 - acc: 0.8279 - specificity_at_sensitivity: 0.5570 - sensitivity_at_specificity: 0.5571 - val_loss: 0.3764 - val_acc: 0.8368 - val_specificity_at_sensitivity: 0.5641 - val_sensitivity_at_specificity: 0.5715\n",
      "Epoch 11/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.3797 - acc: 0.8318 - specificity_at_sensitivity: 0.5758 - sensitivity_at_specificity: 0.5788 - val_loss: 0.3818 - val_acc: 0.8292 - val_specificity_at_sensitivity: 0.5859 - val_sensitivity_at_specificity: 0.5858\n",
      "Epoch 12/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.3739 - acc: 0.8351 - specificity_at_sensitivity: 0.5938 - sensitivity_at_specificity: 0.5964 - val_loss: 0.4841 - val_acc: 0.8126 - val_specificity_at_sensitivity: 0.6027 - val_sensitivity_at_specificity: 0.6054\n",
      "Epoch 13/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.3854 - acc: 0.8340 - specificity_at_sensitivity: 0.6078 - sensitivity_at_specificity: 0.6107 - val_loss: 0.3669 - val_acc: 0.8358 - val_specificity_at_sensitivity: 0.6181 - val_sensitivity_at_specificity: 0.6172\n",
      "Epoch 14/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.3543 - acc: 0.8386 - specificity_at_sensitivity: 0.6247 - sensitivity_at_specificity: 0.6277 - val_loss: 0.3752 - val_acc: 0.8302 - val_specificity_at_sensitivity: 0.6361 - val_sensitivity_at_specificity: 0.6348\n",
      "Epoch 15/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.3378 - acc: 0.8452 - specificity_at_sensitivity: 0.6423 - sensitivity_at_specificity: 0.6427 - val_loss: 0.3691 - val_acc: 0.8336 - val_specificity_at_sensitivity: 0.6480 - val_sensitivity_at_specificity: 0.6523\n",
      "Epoch 16/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.3509 - acc: 0.8415 - specificity_at_sensitivity: 0.6570 - sensitivity_at_specificity: 0.6568 - val_loss: 0.3444 - val_acc: 0.8428 - val_specificity_at_sensitivity: 0.6652 - val_sensitivity_at_specificity: 0.6614\n",
      "Epoch 17/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.3295 - acc: 0.8486 - specificity_at_sensitivity: 0.6710 - sensitivity_at_specificity: 0.6720 - val_loss: 0.3541 - val_acc: 0.8326 - val_specificity_at_sensitivity: 0.6764 - val_sensitivity_at_specificity: 0.6769\n",
      "Epoch 18/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.3221 - acc: 0.8531 - specificity_at_sensitivity: 0.6835 - sensitivity_at_specificity: 0.6840 - val_loss: 0.3402 - val_acc: 0.8424 - val_specificity_at_sensitivity: 0.6899 - val_sensitivity_at_specificity: 0.6910\n",
      "Epoch 19/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.3141 - acc: 0.8550 - specificity_at_sensitivity: 0.6961 - sensitivity_at_specificity: 0.6954 - val_loss: 0.3465 - val_acc: 0.8400 - val_specificity_at_sensitivity: 0.7032 - val_sensitivity_at_specificity: 0.7046\n",
      "Epoch 20/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.3068 - acc: 0.8581 - specificity_at_sensitivity: 0.7075 - sensitivity_at_specificity: 0.7082 - val_loss: 0.3482 - val_acc: 0.8368 - val_specificity_at_sensitivity: 0.7133 - val_sensitivity_at_specificity: 0.7112\n",
      "Epoch 21/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2977 - acc: 0.8601 - specificity_at_sensitivity: 0.7180 - sensitivity_at_specificity: 0.7185 - val_loss: 0.3733 - val_acc: 0.8340 - val_specificity_at_sensitivity: 0.7231 - val_sensitivity_at_specificity: 0.7230\n",
      "Epoch 22/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.2909 - acc: 0.8667 - specificity_at_sensitivity: 0.7272 - sensitivity_at_specificity: 0.7281 - val_loss: 0.3483 - val_acc: 0.8390 - val_specificity_at_sensitivity: 0.7327 - val_sensitivity_at_specificity: 0.7346\n",
      "Epoch 23/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2936 - acc: 0.8670 - specificity_at_sensitivity: 0.7364 - sensitivity_at_specificity: 0.7373 - val_loss: 0.3389 - val_acc: 0.8388 - val_specificity_at_sensitivity: 0.7418 - val_sensitivity_at_specificity: 0.7452\n",
      "Epoch 24/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.2715 - acc: 0.8744 - specificity_at_sensitivity: 0.7449 - sensitivity_at_specificity: 0.7480 - val_loss: 0.3290 - val_acc: 0.8452 - val_specificity_at_sensitivity: 0.7509 - val_sensitivity_at_specificity: 0.7506\n",
      "Epoch 25/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.2774 - acc: 0.8708 - specificity_at_sensitivity: 0.7528 - sensitivity_at_specificity: 0.7575 - val_loss: 0.3354 - val_acc: 0.8422 - val_specificity_at_sensitivity: 0.7595 - val_sensitivity_at_specificity: 0.7609\n",
      "Epoch 26/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.2654 - acc: 0.8766 - specificity_at_sensitivity: 0.7613 - sensitivity_at_specificity: 0.7651 - val_loss: 0.3226 - val_acc: 0.8444 - val_specificity_at_sensitivity: 0.7629 - val_sensitivity_at_specificity: 0.7708\n",
      "Epoch 27/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.2623 - acc: 0.8783 - specificity_at_sensitivity: 0.7685 - sensitivity_at_specificity: 0.7733 - val_loss: 0.3274 - val_acc: 0.8496 - val_specificity_at_sensitivity: 0.7711 - val_sensitivity_at_specificity: 0.7803\n",
      "Epoch 28/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2613 - acc: 0.8815 - specificity_at_sensitivity: 0.7749 - sensitivity_at_specificity: 0.7820 - val_loss: 0.3525 - val_acc: 0.8368 - val_specificity_at_sensitivity: 0.7790 - val_sensitivity_at_specificity: 0.7836\n",
      "Epoch 29/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.2482 - acc: 0.8890 - specificity_at_sensitivity: 0.7806 - sensitivity_at_specificity: 0.7895 - val_loss: 0.4490 - val_acc: 0.8160 - val_specificity_at_sensitivity: 0.7817 - val_sensitivity_at_specificity: 0.7917\n",
      "Epoch 30/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.2473 - acc: 0.8865 - specificity_at_sensitivity: 0.7872 - sensitivity_at_specificity: 0.7949 - val_loss: 0.3580 - val_acc: 0.8450 - val_specificity_at_sensitivity: 0.7891 - val_sensitivity_at_specificity: 0.7995\n",
      "Epoch 31/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.2380 - acc: 0.8933 - specificity_at_sensitivity: 0.7926 - sensitivity_at_specificity: 0.8011 - val_loss: 0.3878 - val_acc: 0.8340 - val_specificity_at_sensitivity: 0.7965 - val_sensitivity_at_specificity: 0.8047\n",
      "Epoch 32/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2415 - acc: 0.8944 - specificity_at_sensitivity: 0.7979 - sensitivity_at_specificity: 0.8085 - val_loss: 0.3291 - val_acc: 0.8580 - val_specificity_at_sensitivity: 0.7990 - val_sensitivity_at_specificity: 0.8098\n",
      "Epoch 33/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2297 - acc: 0.8966 - specificity_at_sensitivity: 0.8045 - sensitivity_at_specificity: 0.8143 - val_loss: 0.3472 - val_acc: 0.8448 - val_specificity_at_sensitivity: 0.8059 - val_sensitivity_at_specificity: 0.8172\n",
      "Epoch 34/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2228 - acc: 0.8989 - specificity_at_sensitivity: 0.8088 - sensitivity_at_specificity: 0.8199 - val_loss: 0.3554 - val_acc: 0.8426 - val_specificity_at_sensitivity: 0.8127 - val_sensitivity_at_specificity: 0.8242\n",
      "Epoch 35/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2186 - acc: 0.9003 - specificity_at_sensitivity: 0.8138 - sensitivity_at_specificity: 0.8255 - val_loss: 0.3351 - val_acc: 0.8546 - val_specificity_at_sensitivity: 0.8149 - val_sensitivity_at_specificity: 0.8268\n",
      "Epoch 36/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2107 - acc: 0.9057 - specificity_at_sensitivity: 0.8197 - sensitivity_at_specificity: 0.8320 - val_loss: 0.3595 - val_acc: 0.8538 - val_specificity_at_sensitivity: 0.8213 - val_sensitivity_at_specificity: 0.8333\n",
      "Epoch 37/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2184 - acc: 0.9068 - specificity_at_sensitivity: 0.8230 - sensitivity_at_specificity: 0.8368 - val_loss: 0.3598 - val_acc: 0.8520 - val_specificity_at_sensitivity: 0.8276 - val_sensitivity_at_specificity: 0.8396\n",
      "Epoch 38/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1997 - acc: 0.9110 - specificity_at_sensitivity: 0.8286 - sensitivity_at_specificity: 0.8416 - val_loss: 0.3812 - val_acc: 0.8498 - val_specificity_at_sensitivity: 0.8295 - val_sensitivity_at_specificity: 0.8455\n",
      "Epoch 39/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2017 - acc: 0.9093 - specificity_at_sensitivity: 0.8320 - sensitivity_at_specificity: 0.8465 - val_loss: 0.3476 - val_acc: 0.8604 - val_specificity_at_sensitivity: 0.8354 - val_sensitivity_at_specificity: 0.8474\n",
      "Epoch 40/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2032 - acc: 0.9123 - specificity_at_sensitivity: 0.8364 - sensitivity_at_specificity: 0.8517 - val_loss: 0.3828 - val_acc: 0.8366 - val_specificity_at_sensitivity: 0.8372 - val_sensitivity_at_specificity: 0.8530\n",
      "Epoch 41/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.2006 - acc: 0.9141 - specificity_at_sensitivity: 0.8406 - sensitivity_at_specificity: 0.8555 - val_loss: 0.4131 - val_acc: 0.8478 - val_specificity_at_sensitivity: 0.8427 - val_sensitivity_at_specificity: 0.8588\n",
      "Epoch 42/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1969 - acc: 0.9163 - specificity_at_sensitivity: 0.8436 - sensitivity_at_specificity: 0.8597 - val_loss: 0.3980 - val_acc: 0.8442 - val_specificity_at_sensitivity: 0.8443 - val_sensitivity_at_specificity: 0.8603\n",
      "Epoch 43/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1855 - acc: 0.9201 - specificity_at_sensitivity: 0.8484 - sensitivity_at_specificity: 0.8646 - val_loss: 0.3912 - val_acc: 0.8518 - val_specificity_at_sensitivity: 0.8497 - val_sensitivity_at_specificity: 0.8655\n",
      "Epoch 44/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1934 - acc: 0.9179 - specificity_at_sensitivity: 0.8508 - sensitivity_at_specificity: 0.8679 - val_loss: 0.3718 - val_acc: 0.8528 - val_specificity_at_sensitivity: 0.8548 - val_sensitivity_at_specificity: 0.8705\n",
      "Epoch 45/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1777 - acc: 0.9241 - specificity_at_sensitivity: 0.8556 - sensitivity_at_specificity: 0.8713 - val_loss: 0.3823 - val_acc: 0.8586 - val_specificity_at_sensitivity: 0.8563 - val_sensitivity_at_specificity: 0.8721\n",
      "Epoch 46/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1782 - acc: 0.9238 - specificity_at_sensitivity: 0.8581 - sensitivity_at_specificity: 0.8760 - val_loss: 0.4155 - val_acc: 0.8514 - val_specificity_at_sensitivity: 0.8612 - val_sensitivity_at_specificity: 0.8768\n",
      "Epoch 47/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.1693 - acc: 0.9291 - specificity_at_sensitivity: 0.8620 - sensitivity_at_specificity: 0.8789 - val_loss: 0.3799 - val_acc: 0.8604 - val_specificity_at_sensitivity: 0.8627 - val_sensitivity_at_specificity: 0.8814\n",
      "Epoch 48/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1648 - acc: 0.9271 - specificity_at_sensitivity: 0.8649 - sensitivity_at_specificity: 0.8821 - val_loss: 0.3786 - val_acc: 0.8584 - val_specificity_at_sensitivity: 0.8675 - val_sensitivity_at_specificity: 0.8826\n",
      "Epoch 49/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1699 - acc: 0.9288 - specificity_at_sensitivity: 0.8682 - sensitivity_at_specificity: 0.8865 - val_loss: 0.4127 - val_acc: 0.8440 - val_specificity_at_sensitivity: 0.8687 - val_sensitivity_at_specificity: 0.8871\n",
      "Epoch 50/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1619 - acc: 0.9299 - specificity_at_sensitivity: 0.8712 - sensitivity_at_specificity: 0.8887 - val_loss: 0.3797 - val_acc: 0.8558 - val_specificity_at_sensitivity: 0.8732 - val_sensitivity_at_specificity: 0.8914\n",
      "Epoch 51/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.1693 - acc: 0.9313 - specificity_at_sensitivity: 0.8739 - sensitivity_at_specificity: 0.8920 - val_loss: 0.3783 - val_acc: 0.8486 - val_specificity_at_sensitivity: 0.8744 - val_sensitivity_at_specificity: 0.8923\n",
      "Epoch 52/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.1665 - acc: 0.9334 - specificity_at_sensitivity: 0.8768 - sensitivity_at_specificity: 0.8952 - val_loss: 0.3647 - val_acc: 0.8570 - val_specificity_at_sensitivity: 0.8787 - val_sensitivity_at_specificity: 0.8965\n",
      "Epoch 53/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.1541 - acc: 0.9368 - specificity_at_sensitivity: 0.8793 - sensitivity_at_specificity: 0.8971 - val_loss: 0.3907 - val_acc: 0.8534 - val_specificity_at_sensitivity: 0.8798 - val_sensitivity_at_specificity: 0.8990\n",
      "Epoch 54/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.1537 - acc: 0.9380 - specificity_at_sensitivity: 0.8824 - sensitivity_at_specificity: 0.9010 - val_loss: 0.4001 - val_acc: 0.8614 - val_specificity_at_sensitivity: 0.8840 - val_sensitivity_at_specificity: 0.9014\n",
      "Epoch 55/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.1554 - acc: 0.9368 - specificity_at_sensitivity: 0.8846 - sensitivity_at_specificity: 0.9028 - val_loss: 0.4055 - val_acc: 0.8484 - val_specificity_at_sensitivity: 0.8850 - val_sensitivity_at_specificity: 0.9053\n",
      "Epoch 56/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1502 - acc: 0.9409 - specificity_at_sensitivity: 0.8875 - sensitivity_at_specificity: 0.9057 - val_loss: 0.3878 - val_acc: 0.8608 - val_specificity_at_sensitivity: 0.8890 - val_sensitivity_at_specificity: 0.9060\n",
      "Epoch 57/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1412 - acc: 0.9420 - specificity_at_sensitivity: 0.8896 - sensitivity_at_specificity: 0.9083 - val_loss: 0.4645 - val_acc: 0.8600 - val_specificity_at_sensitivity: 0.8900 - val_sensitivity_at_specificity: 0.9096\n",
      "Epoch 58/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1527 - acc: 0.9391 - specificity_at_sensitivity: 0.8924 - sensitivity_at_specificity: 0.9100 - val_loss: 0.4325 - val_acc: 0.8448 - val_specificity_at_sensitivity: 0.8938 - val_sensitivity_at_specificity: 0.9103\n",
      "Epoch 59/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1362 - acc: 0.9455 - specificity_at_sensitivity: 0.8943 - sensitivity_at_specificity: 0.9132 - val_loss: 0.4731 - val_acc: 0.8544 - val_specificity_at_sensitivity: 0.8947 - val_sensitivity_at_specificity: 0.9139\n",
      "Epoch 60/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.1494 - acc: 0.9405 - specificity_at_sensitivity: 0.8965 - sensitivity_at_specificity: 0.9143 - val_loss: 0.3928 - val_acc: 0.8624 - val_specificity_at_sensitivity: 0.8983 - val_sensitivity_at_specificity: 0.9145\n",
      "Epoch 61/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.1336 - acc: 0.9472 - specificity_at_sensitivity: 0.8988 - sensitivity_at_specificity: 0.9177 - val_loss: 0.4177 - val_acc: 0.8576 - val_specificity_at_sensitivity: 0.8991 - val_sensitivity_at_specificity: 0.9179\n",
      "Epoch 62/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1385 - acc: 0.9444 - specificity_at_sensitivity: 0.9005 - sensitivity_at_specificity: 0.9183 - val_loss: 0.4260 - val_acc: 0.8530 - val_specificity_at_sensitivity: 0.9025 - val_sensitivity_at_specificity: 0.9198\n",
      "Epoch 63/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1319 - acc: 0.9496 - specificity_at_sensitivity: 0.9030 - sensitivity_at_specificity: 0.9214 - val_loss: 0.4702 - val_acc: 0.8588 - val_specificity_at_sensitivity: 0.9033 - val_sensitivity_at_specificity: 0.9216\n",
      "Epoch 64/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1333 - acc: 0.9482 - specificity_at_sensitivity: 0.9046 - sensitivity_at_specificity: 0.9220 - val_loss: 0.4170 - val_acc: 0.8604 - val_specificity_at_sensitivity: 0.9066 - val_sensitivity_at_specificity: 0.9236\n",
      "Epoch 65/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1299 - acc: 0.9493 - specificity_at_sensitivity: 0.9070 - sensitivity_at_specificity: 0.9253 - val_loss: 0.4450 - val_acc: 0.8664 - val_specificity_at_sensitivity: 0.9073 - val_sensitivity_at_specificity: 0.9255\n",
      "Epoch 66/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1209 - acc: 0.9526 - specificity_at_sensitivity: 0.9084 - sensitivity_at_specificity: 0.9258 - val_loss: 0.4707 - val_acc: 0.8582 - val_specificity_at_sensitivity: 0.9105 - val_sensitivity_at_specificity: 0.9260\n",
      "Epoch 67/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.1220 - acc: 0.9510 - specificity_at_sensitivity: 0.9109 - sensitivity_at_specificity: 0.9290 - val_loss: 0.4264 - val_acc: 0.8642 - val_specificity_at_sensitivity: 0.9111 - val_sensitivity_at_specificity: 0.9291\n",
      "Epoch 68/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.1309 - acc: 0.9492 - specificity_at_sensitivity: 0.9120 - sensitivity_at_specificity: 0.9294 - val_loss: 0.4460 - val_acc: 0.8622 - val_specificity_at_sensitivity: 0.9141 - val_sensitivity_at_specificity: 0.9295\n",
      "Epoch 69/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.1187 - acc: 0.9540 - specificity_at_sensitivity: 0.9145 - sensitivity_at_specificity: 0.9317 - val_loss: 0.4574 - val_acc: 0.8670 - val_specificity_at_sensitivity: 0.9147 - val_sensitivity_at_specificity: 0.9326\n",
      "Epoch 70/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.1240 - acc: 0.9520 - specificity_at_sensitivity: 0.9154 - sensitivity_at_specificity: 0.9328 - val_loss: 0.5164 - val_acc: 0.8614 - val_specificity_at_sensitivity: 0.9176 - val_sensitivity_at_specificity: 0.9330\n",
      "Epoch 71/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.1163 - acc: 0.9545 - specificity_at_sensitivity: 0.9179 - sensitivity_at_specificity: 0.9341 - val_loss: 0.4428 - val_acc: 0.8658 - val_specificity_at_sensitivity: 0.9182 - val_sensitivity_at_specificity: 0.9359\n",
      "Epoch 72/150\n",
      "7050/7050 [==============================] - 0s 60us/step - loss: 0.1153 - acc: 0.9550 - specificity_at_sensitivity: 0.9187 - sensitivity_at_specificity: 0.9361 - val_loss: 0.4678 - val_acc: 0.8578 - val_specificity_at_sensitivity: 0.9209 - val_sensitivity_at_specificity: 0.9362\n",
      "Epoch 73/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1097 - acc: 0.9593 - specificity_at_sensitivity: 0.9212 - sensitivity_at_specificity: 0.9365 - val_loss: 0.4668 - val_acc: 0.8592 - val_specificity_at_sensitivity: 0.9215 - val_sensitivity_at_specificity: 0.9366\n",
      "Epoch 74/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.1150 - acc: 0.9554 - specificity_at_sensitivity: 0.9218 - sensitivity_at_specificity: 0.9392 - val_loss: 0.4224 - val_acc: 0.8604 - val_specificity_at_sensitivity: 0.9220 - val_sensitivity_at_specificity: 0.9395\n",
      "Epoch 75/150\n",
      "7050/7050 [==============================] - 0s 60us/step - loss: 0.1066 - acc: 0.9598 - specificity_at_sensitivity: 0.9242 - sensitivity_at_specificity: 0.9397 - val_loss: 0.4765 - val_acc: 0.8584 - val_specificity_at_sensitivity: 0.9246 - val_sensitivity_at_specificity: 0.9398\n",
      "Epoch 76/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.1127 - acc: 0.9578 - specificity_at_sensitivity: 0.9249 - sensitivity_at_specificity: 0.9407 - val_loss: 0.5293 - val_acc: 0.8594 - val_specificity_at_sensitivity: 0.9251 - val_sensitivity_at_specificity: 0.9425\n",
      "Epoch 77/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.1114 - acc: 0.9567 - specificity_at_sensitivity: 0.9269 - sensitivity_at_specificity: 0.9427 - val_loss: 0.4358 - val_acc: 0.8562 - val_specificity_at_sensitivity: 0.9276 - val_sensitivity_at_specificity: 0.9427\n",
      "Epoch 78/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.1105 - acc: 0.9596 - specificity_at_sensitivity: 0.9278 - sensitivity_at_specificity: 0.9429 - val_loss: 0.4664 - val_acc: 0.8674 - val_specificity_at_sensitivity: 0.9280 - val_sensitivity_at_specificity: 0.9430\n",
      "Epoch 79/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.1150 - acc: 0.9576 - specificity_at_sensitivity: 0.9294 - sensitivity_at_specificity: 0.9446 - val_loss: 0.5186 - val_acc: 0.8634 - val_specificity_at_sensitivity: 0.9304 - val_sensitivity_at_specificity: 0.9457\n",
      "Epoch 80/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0982 - acc: 0.9619 - specificity_at_sensitivity: 0.9307 - sensitivity_at_specificity: 0.9459 - val_loss: 0.5276 - val_acc: 0.8654 - val_specificity_at_sensitivity: 0.9308 - val_sensitivity_at_specificity: 0.9460\n",
      "Epoch 81/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0988 - acc: 0.9641 - specificity_at_sensitivity: 0.9321 - sensitivity_at_specificity: 0.9462 - val_loss: 0.6055 - val_acc: 0.8490 - val_specificity_at_sensitivity: 0.9332 - val_sensitivity_at_specificity: 0.9462\n",
      "Epoch 82/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0982 - acc: 0.9639 - specificity_at_sensitivity: 0.9334 - sensitivity_at_specificity: 0.9479 - val_loss: 0.5249 - val_acc: 0.8520 - val_specificity_at_sensitivity: 0.9336 - val_sensitivity_at_specificity: 0.9488\n",
      "Epoch 83/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.1022 - acc: 0.9620 - specificity_at_sensitivity: 0.9345 - sensitivity_at_specificity: 0.9490 - val_loss: 0.5249 - val_acc: 0.8642 - val_specificity_at_sensitivity: 0.9358 - val_sensitivity_at_specificity: 0.9490\n",
      "Epoch 84/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.1072 - acc: 0.9616 - specificity_at_sensitivity: 0.9360 - sensitivity_at_specificity: 0.9492 - val_loss: 0.4942 - val_acc: 0.8568 - val_specificity_at_sensitivity: 0.9361 - val_sensitivity_at_specificity: 0.9492\n",
      "Epoch 85/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.0938 - acc: 0.9658 - specificity_at_sensitivity: 0.9367 - sensitivity_at_specificity: 0.9507 - val_loss: 0.5189 - val_acc: 0.8592 - val_specificity_at_sensitivity: 0.9383 - val_sensitivity_at_specificity: 0.9518\n",
      "Epoch 86/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.1015 - acc: 0.9625 - specificity_at_sensitivity: 0.9384 - sensitivity_at_specificity: 0.9519 - val_loss: 0.5632 - val_acc: 0.8522 - val_specificity_at_sensitivity: 0.9386 - val_sensitivity_at_specificity: 0.9519\n",
      "Epoch 87/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.0944 - acc: 0.9656 - specificity_at_sensitivity: 0.9389 - sensitivity_at_specificity: 0.9520 - val_loss: 0.5574 - val_acc: 0.8598 - val_specificity_at_sensitivity: 0.9406 - val_sensitivity_at_specificity: 0.9521\n",
      "Epoch 88/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.1017 - acc: 0.9638 - specificity_at_sensitivity: 0.9408 - sensitivity_at_specificity: 0.9526 - val_loss: 0.4588 - val_acc: 0.8688 - val_specificity_at_sensitivity: 0.9409 - val_sensitivity_at_specificity: 0.9545\n",
      "Epoch 89/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.0923 - acc: 0.9665 - specificity_at_sensitivity: 0.9411 - sensitivity_at_specificity: 0.9547 - val_loss: 0.4921 - val_acc: 0.8518 - val_specificity_at_sensitivity: 0.9412 - val_sensitivity_at_specificity: 0.9547\n",
      "Epoch 90/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0975 - acc: 0.9662 - specificity_at_sensitivity: 0.9429 - sensitivity_at_specificity: 0.9548 - val_loss: 0.5373 - val_acc: 0.8620 - val_specificity_at_sensitivity: 0.9431 - val_sensitivity_at_specificity: 0.9548\n",
      "Epoch 91/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.0954 - acc: 0.9660 - specificity_at_sensitivity: 0.9433 - sensitivity_at_specificity: 0.9550 - val_loss: 0.6191 - val_acc: 0.8638 - val_specificity_at_sensitivity: 0.9434 - val_sensitivity_at_specificity: 0.9550\n",
      "Epoch 92/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0903 - acc: 0.9683 - specificity_at_sensitivity: 0.9447 - sensitivity_at_specificity: 0.9559 - val_loss: 0.4922 - val_acc: 0.8668 - val_specificity_at_sensitivity: 0.9452 - val_sensitivity_at_specificity: 0.9574\n",
      "Epoch 93/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0918 - acc: 0.9686 - specificity_at_sensitivity: 0.9453 - sensitivity_at_specificity: 0.9575 - val_loss: 0.5075 - val_acc: 0.8654 - val_specificity_at_sensitivity: 0.9455 - val_sensitivity_at_specificity: 0.9576\n",
      "Epoch 94/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0884 - acc: 0.9684 - specificity_at_sensitivity: 0.9467 - sensitivity_at_specificity: 0.9577 - val_loss: 0.4907 - val_acc: 0.8736 - val_specificity_at_sensitivity: 0.9472 - val_sensitivity_at_specificity: 0.9577\n",
      "Epoch 95/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.0863 - acc: 0.9701 - specificity_at_sensitivity: 0.9473 - sensitivity_at_specificity: 0.9579 - val_loss: 0.5226 - val_acc: 0.8626 - val_specificity_at_sensitivity: 0.9474 - val_sensitivity_at_specificity: 0.9579\n",
      "Epoch 96/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0893 - acc: 0.9692 - specificity_at_sensitivity: 0.9486 - sensitivity_at_specificity: 0.9584 - val_loss: 0.5479 - val_acc: 0.8558 - val_specificity_at_sensitivity: 0.9491 - val_sensitivity_at_specificity: 0.9602\n",
      "Epoch 97/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0905 - acc: 0.9678 - specificity_at_sensitivity: 0.9492 - sensitivity_at_specificity: 0.9603 - val_loss: 0.5181 - val_acc: 0.8646 - val_specificity_at_sensitivity: 0.9493 - val_sensitivity_at_specificity: 0.9603\n",
      "Epoch 98/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0908 - acc: 0.9681 - specificity_at_sensitivity: 0.9502 - sensitivity_at_specificity: 0.9604 - val_loss: 0.4756 - val_acc: 0.8676 - val_specificity_at_sensitivity: 0.9509 - val_sensitivity_at_specificity: 0.9604\n",
      "Epoch 99/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0836 - acc: 0.9724 - specificity_at_sensitivity: 0.9511 - sensitivity_at_specificity: 0.9606 - val_loss: 0.5778 - val_acc: 0.8646 - val_specificity_at_sensitivity: 0.9511 - val_sensitivity_at_specificity: 0.9606\n",
      "Epoch 100/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0942 - acc: 0.9688 - specificity_at_sensitivity: 0.9519 - sensitivity_at_specificity: 0.9606 - val_loss: 0.5120 - val_acc: 0.8630 - val_specificity_at_sensitivity: 0.9526 - val_sensitivity_at_specificity: 0.9606\n",
      "Epoch 101/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0888 - acc: 0.9697 - specificity_at_sensitivity: 0.9528 - sensitivity_at_specificity: 0.9617 - val_loss: 0.5347 - val_acc: 0.8578 - val_specificity_at_sensitivity: 0.9528 - val_sensitivity_at_specificity: 0.9630\n",
      "Epoch 102/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.0863 - acc: 0.9679 - specificity_at_sensitivity: 0.9534 - sensitivity_at_specificity: 0.9631 - val_loss: 0.5979 - val_acc: 0.8628 - val_specificity_at_sensitivity: 0.9542 - val_sensitivity_at_specificity: 0.9631\n",
      "Epoch 103/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.0852 - acc: 0.9704 - specificity_at_sensitivity: 0.9543 - sensitivity_at_specificity: 0.9632 - val_loss: 0.4826 - val_acc: 0.8652 - val_specificity_at_sensitivity: 0.9544 - val_sensitivity_at_specificity: 0.9632\n",
      "Epoch 104/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0790 - acc: 0.9735 - specificity_at_sensitivity: 0.9550 - sensitivity_at_specificity: 0.9633 - val_loss: 0.5814 - val_acc: 0.8634 - val_specificity_at_sensitivity: 0.9558 - val_sensitivity_at_specificity: 0.9633\n",
      "Epoch 105/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0803 - acc: 0.9716 - specificity_at_sensitivity: 0.9559 - sensitivity_at_specificity: 0.9634 - val_loss: 0.6652 - val_acc: 0.8720 - val_specificity_at_sensitivity: 0.9560 - val_sensitivity_at_specificity: 0.9634\n",
      "Epoch 106/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0884 - acc: 0.9700 - specificity_at_sensitivity: 0.9565 - sensitivity_at_specificity: 0.9635 - val_loss: 0.5505 - val_acc: 0.8608 - val_specificity_at_sensitivity: 0.9573 - val_sensitivity_at_specificity: 0.9634\n",
      "Epoch 107/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0790 - acc: 0.9743 - specificity_at_sensitivity: 0.9574 - sensitivity_at_specificity: 0.9657 - val_loss: 0.5823 - val_acc: 0.8678 - val_specificity_at_sensitivity: 0.9574 - val_sensitivity_at_specificity: 0.9659\n",
      "Epoch 108/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0823 - acc: 0.9731 - specificity_at_sensitivity: 0.9579 - sensitivity_at_specificity: 0.9660 - val_loss: 0.5293 - val_acc: 0.8636 - val_specificity_at_sensitivity: 0.9587 - val_sensitivity_at_specificity: 0.9659\n",
      "Epoch 109/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0740 - acc: 0.9739 - specificity_at_sensitivity: 0.9588 - sensitivity_at_specificity: 0.9660 - val_loss: 0.6864 - val_acc: 0.8598 - val_specificity_at_sensitivity: 0.9588 - val_sensitivity_at_specificity: 0.9660\n",
      "Epoch 110/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.0815 - acc: 0.9732 - specificity_at_sensitivity: 0.9594 - sensitivity_at_specificity: 0.9660 - val_loss: 0.6350 - val_acc: 0.8558 - val_specificity_at_sensitivity: 0.9600 - val_sensitivity_at_specificity: 0.9660\n",
      "Epoch 111/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0799 - acc: 0.9742 - specificity_at_sensitivity: 0.9601 - sensitivity_at_specificity: 0.9661 - val_loss: 0.5458 - val_acc: 0.8676 - val_specificity_at_sensitivity: 0.9602 - val_sensitivity_at_specificity: 0.9661\n",
      "Epoch 112/150\n",
      "7050/7050 [==============================] - 0s 53us/step - loss: 0.0657 - acc: 0.9772 - specificity_at_sensitivity: 0.9607 - sensitivity_at_specificity: 0.9662 - val_loss: 0.6341 - val_acc: 0.8650 - val_specificity_at_sensitivity: 0.9613 - val_sensitivity_at_specificity: 0.9662\n",
      "Epoch 113/150\n",
      "7050/7050 [==============================] - 0s 54us/step - loss: 0.0827 - acc: 0.9725 - specificity_at_sensitivity: 0.9614 - sensitivity_at_specificity: 0.9674 - val_loss: 0.5288 - val_acc: 0.8680 - val_specificity_at_sensitivity: 0.9614 - val_sensitivity_at_specificity: 0.9686\n",
      "Epoch 114/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.0709 - acc: 0.9767 - specificity_at_sensitivity: 0.9621 - sensitivity_at_specificity: 0.9687 - val_loss: 0.4854 - val_acc: 0.8702 - val_specificity_at_sensitivity: 0.9626 - val_sensitivity_at_specificity: 0.9687\n",
      "Epoch 115/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0714 - acc: 0.9767 - specificity_at_sensitivity: 0.9627 - sensitivity_at_specificity: 0.9688 - val_loss: 0.5941 - val_acc: 0.8652 - val_specificity_at_sensitivity: 0.9627 - val_sensitivity_at_specificity: 0.9688\n",
      "Epoch 116/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.0693 - acc: 0.9777 - specificity_at_sensitivity: 0.9634 - sensitivity_at_specificity: 0.9688 - val_loss: 0.5540 - val_acc: 0.8562 - val_specificity_at_sensitivity: 0.9637 - val_sensitivity_at_specificity: 0.9688\n",
      "Epoch 117/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0744 - acc: 0.9752 - specificity_at_sensitivity: 0.9638 - sensitivity_at_specificity: 0.9688 - val_loss: 0.5830 - val_acc: 0.8634 - val_specificity_at_sensitivity: 0.9639 - val_sensitivity_at_specificity: 0.9688\n",
      "Epoch 118/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0782 - acc: 0.9750 - specificity_at_sensitivity: 0.9646 - sensitivity_at_specificity: 0.9689 - val_loss: 0.5093 - val_acc: 0.8692 - val_specificity_at_sensitivity: 0.9649 - val_sensitivity_at_specificity: 0.9689\n",
      "Epoch 119/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0678 - acc: 0.9780 - specificity_at_sensitivity: 0.9649 - sensitivity_at_specificity: 0.9690 - val_loss: 0.5269 - val_acc: 0.8684 - val_specificity_at_sensitivity: 0.9650 - val_sensitivity_at_specificity: 0.9689\n",
      "Epoch 120/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.0730 - acc: 0.9757 - specificity_at_sensitivity: 0.9659 - sensitivity_at_specificity: 0.9690 - val_loss: 0.5400 - val_acc: 0.8750 - val_specificity_at_sensitivity: 0.9660 - val_sensitivity_at_specificity: 0.9690\n",
      "Epoch 121/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.0861 - acc: 0.9732 - specificity_at_sensitivity: 0.9660 - sensitivity_at_specificity: 0.9701 - val_loss: 0.5389 - val_acc: 0.8646 - val_specificity_at_sensitivity: 0.9661 - val_sensitivity_at_specificity: 0.9715\n",
      "Epoch 122/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.0635 - acc: 0.9792 - specificity_at_sensitivity: 0.9669 - sensitivity_at_specificity: 0.9716 - val_loss: 0.5573 - val_acc: 0.8674 - val_specificity_at_sensitivity: 0.9670 - val_sensitivity_at_specificity: 0.9716\n",
      "Epoch 123/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0716 - acc: 0.9754 - specificity_at_sensitivity: 0.9671 - sensitivity_at_specificity: 0.9716 - val_loss: 0.4999 - val_acc: 0.8686 - val_specificity_at_sensitivity: 0.9678 - val_sensitivity_at_specificity: 0.9716\n",
      "Epoch 124/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0735 - acc: 0.9765 - specificity_at_sensitivity: 0.9679 - sensitivity_at_specificity: 0.9717 - val_loss: 0.4816 - val_acc: 0.8674 - val_specificity_at_sensitivity: 0.9679 - val_sensitivity_at_specificity: 0.9717\n",
      "Epoch 125/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0764 - acc: 0.9758 - specificity_at_sensitivity: 0.9681 - sensitivity_at_specificity: 0.9717 - val_loss: 0.5099 - val_acc: 0.8652 - val_specificity_at_sensitivity: 0.9688 - val_sensitivity_at_specificity: 0.9717\n",
      "Epoch 126/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0756 - acc: 0.9758 - specificity_at_sensitivity: 0.9688 - sensitivity_at_specificity: 0.9717 - val_loss: 0.5110 - val_acc: 0.8660 - val_specificity_at_sensitivity: 0.9689 - val_sensitivity_at_specificity: 0.9717\n",
      "Epoch 127/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0613 - acc: 0.9799 - specificity_at_sensitivity: 0.9691 - sensitivity_at_specificity: 0.9718 - val_loss: 0.5963 - val_acc: 0.8674 - val_specificity_at_sensitivity: 0.9697 - val_sensitivity_at_specificity: 0.9718\n",
      "Epoch 128/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0737 - acc: 0.9760 - specificity_at_sensitivity: 0.9697 - sensitivity_at_specificity: 0.9718 - val_loss: 0.7378 - val_acc: 0.8486 - val_specificity_at_sensitivity: 0.9697 - val_sensitivity_at_specificity: 0.9718\n",
      "Epoch 129/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0693 - acc: 0.9781 - specificity_at_sensitivity: 0.9700 - sensitivity_at_specificity: 0.9718 - val_loss: 0.5352 - val_acc: 0.8748 - val_specificity_at_sensitivity: 0.9705 - val_sensitivity_at_specificity: 0.9718\n",
      "Epoch 130/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0685 - acc: 0.9777 - specificity_at_sensitivity: 0.9706 - sensitivity_at_specificity: 0.9718 - val_loss: 0.5297 - val_acc: 0.8686 - val_specificity_at_sensitivity: 0.9706 - val_sensitivity_at_specificity: 0.9718\n",
      "Epoch 131/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0633 - acc: 0.9803 - specificity_at_sensitivity: 0.9710 - sensitivity_at_specificity: 0.9719 - val_loss: 0.5186 - val_acc: 0.8690 - val_specificity_at_sensitivity: 0.9713 - val_sensitivity_at_specificity: 0.9719\n",
      "Epoch 132/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0667 - acc: 0.9785 - specificity_at_sensitivity: 0.9714 - sensitivity_at_specificity: 0.9743 - val_loss: 0.6082 - val_acc: 0.8670 - val_specificity_at_sensitivity: 0.9714 - val_sensitivity_at_specificity: 0.9744\n",
      "Epoch 133/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0731 - acc: 0.9776 - specificity_at_sensitivity: 0.9719 - sensitivity_at_specificity: 0.9745 - val_loss: 0.5739 - val_acc: 0.8644 - val_specificity_at_sensitivity: 0.9721 - val_sensitivity_at_specificity: 0.9744\n",
      "Epoch 134/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0626 - acc: 0.9790 - specificity_at_sensitivity: 0.9721 - sensitivity_at_specificity: 0.9745 - val_loss: 0.5756 - val_acc: 0.8662 - val_specificity_at_sensitivity: 0.9722 - val_sensitivity_at_specificity: 0.9745\n",
      "Epoch 135/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.0630 - acc: 0.9803 - specificity_at_sensitivity: 0.9727 - sensitivity_at_specificity: 0.9745 - val_loss: 0.6781 - val_acc: 0.8588 - val_specificity_at_sensitivity: 0.9728 - val_sensitivity_at_specificity: 0.9745\n",
      "Epoch 136/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0687 - acc: 0.9790 - specificity_at_sensitivity: 0.9729 - sensitivity_at_specificity: 0.9745 - val_loss: 0.6138 - val_acc: 0.8692 - val_specificity_at_sensitivity: 0.9732 - val_sensitivity_at_specificity: 0.9745\n",
      "Epoch 137/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0684 - acc: 0.9777 - specificity_at_sensitivity: 0.9735 - sensitivity_at_specificity: 0.9746 - val_loss: 0.6259 - val_acc: 0.8658 - val_specificity_at_sensitivity: 0.9736 - val_sensitivity_at_specificity: 0.9745\n",
      "Epoch 138/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0694 - acc: 0.9767 - specificity_at_sensitivity: 0.9737 - sensitivity_at_specificity: 0.9746 - val_loss: 0.5118 - val_acc: 0.8674 - val_specificity_at_sensitivity: 0.9742 - val_sensitivity_at_specificity: 0.9746\n",
      "Epoch 139/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0651 - acc: 0.9777 - specificity_at_sensitivity: 0.9742 - sensitivity_at_specificity: 0.9746 - val_loss: 0.5731 - val_acc: 0.8612 - val_specificity_at_sensitivity: 0.9742 - val_sensitivity_at_specificity: 0.9746\n",
      "Epoch 140/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0697 - acc: 0.9786 - specificity_at_sensitivity: 0.9745 - sensitivity_at_specificity: 0.9746 - val_loss: 0.6199 - val_acc: 0.8564 - val_specificity_at_sensitivity: 0.9748 - val_sensitivity_at_specificity: 0.9746\n",
      "Epoch 141/150\n",
      "7050/7050 [==============================] - 0s 55us/step - loss: 0.0638 - acc: 0.9795 - specificity_at_sensitivity: 0.9749 - sensitivity_at_specificity: 0.9747 - val_loss: 0.7804 - val_acc: 0.8686 - val_specificity_at_sensitivity: 0.9749 - val_sensitivity_at_specificity: 0.9746\n",
      "Epoch 142/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0728 - acc: 0.9780 - specificity_at_sensitivity: 0.9752 - sensitivity_at_specificity: 0.9747 - val_loss: 0.5249 - val_acc: 0.8662 - val_specificity_at_sensitivity: 0.9755 - val_sensitivity_at_specificity: 0.9747\n",
      "Epoch 143/150\n",
      "7050/7050 [==============================] - 0s 57us/step - loss: 0.0558 - acc: 0.9826 - specificity_at_sensitivity: 0.9755 - sensitivity_at_specificity: 0.9747 - val_loss: 0.6015 - val_acc: 0.8648 - val_specificity_at_sensitivity: 0.9755 - val_sensitivity_at_specificity: 0.9747\n",
      "Epoch 144/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.0682 - acc: 0.9805 - specificity_at_sensitivity: 0.9759 - sensitivity_at_specificity: 0.9747 - val_loss: 0.5347 - val_acc: 0.8676 - val_specificity_at_sensitivity: 0.9761 - val_sensitivity_at_specificity: 0.9747\n",
      "Epoch 145/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0599 - acc: 0.9812 - specificity_at_sensitivity: 0.9761 - sensitivity_at_specificity: 0.9748 - val_loss: 0.5684 - val_acc: 0.8656 - val_specificity_at_sensitivity: 0.9761 - val_sensitivity_at_specificity: 0.9747\n",
      "Epoch 146/150\n",
      "7050/7050 [==============================] - 0s 59us/step - loss: 0.0632 - acc: 0.9796 - specificity_at_sensitivity: 0.9766 - sensitivity_at_specificity: 0.9748 - val_loss: 0.6775 - val_acc: 0.8658 - val_specificity_at_sensitivity: 0.9767 - val_sensitivity_at_specificity: 0.9748\n",
      "Epoch 147/150\n",
      "7050/7050 [==============================] - 0s 61us/step - loss: 0.0618 - acc: 0.9791 - specificity_at_sensitivity: 0.9768 - sensitivity_at_specificity: 0.9776 - val_loss: 0.6328 - val_acc: 0.8626 - val_specificity_at_sensitivity: 0.9767 - val_sensitivity_at_specificity: 0.9776\n",
      "Epoch 148/150\n",
      "7050/7050 [==============================] - 0s 56us/step - loss: 0.0604 - acc: 0.9805 - specificity_at_sensitivity: 0.9773 - sensitivity_at_specificity: 0.9776 - val_loss: 0.6100 - val_acc: 0.8710 - val_specificity_at_sensitivity: 0.9773 - val_sensitivity_at_specificity: 0.9776\n",
      "Epoch 149/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.0566 - acc: 0.9821 - specificity_at_sensitivity: 0.9774 - sensitivity_at_specificity: 0.9776 - val_loss: 0.6358 - val_acc: 0.8746 - val_specificity_at_sensitivity: 0.9778 - val_sensitivity_at_specificity: 0.9776\n",
      "Epoch 150/150\n",
      "7050/7050 [==============================] - 0s 58us/step - loss: 0.0687 - acc: 0.9780 - specificity_at_sensitivity: 0.9778 - sensitivity_at_specificity: 0.9777 - val_loss: 0.5977 - val_acc: 0.8694 - val_specificity_at_sensitivity: 0.9778 - val_sensitivity_at_specificity: 0.9776\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "history = model.fit(partial_x_train,partial_y_train,epochs = 150,batch_size = 512, validation_data = (x_val,y_val))\n",
    "model.save(\"MLP2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU5b348c93luwJgRAEiQq4swkxKtYFUOt1qWtdQKlLtVa7aGt7r9Taulzvvbb1qqU/a2tbaW+hUqu1UtdaS0VbCwIiq4gKSlgDkpCQbZbv74/nZDKTBULIZMLM9/165ZWZM2fO88yZme/znO955jmiqhhjjMkcvlRXwBhjTO+ywG+MMRnGAr8xxmQYC/zGGJNhLPAbY0yGscBvjDEZxgK/2W8i4heROhE5tCfXTSUROUJEenyss4icJSLr4+6vEZHTurJuN8r6pYjc2d3n72G794vIr3t6u6b3BFJdAdP7RKQu7m4e0AREvPtfVtXZ+7I9VY0ABT29biZQ1aN7YjsiciMwTVUnxW37xp7Ytkk/FvgzkKrGAq/Xo7xRVf/a2foiElDVcG/UzRiTfJbqMe14h/K/F5EnRaQWmCYiJ4vIv0SkWkQ2i8gMEQl66wdEREVkmHd/lvf4SyJSKyJvicjwfV3Xe/xcEXlfRGpE5Cci8g8Rua6Teneljl8WkQ9EZKeIzIh7rl9EHhaRHSLyIXDOHvbPXSIyp82yR0XkIe/2jSKy2ns9H3q98c62VSkik7zbeSLyW69uK4HjOyj3I2+7K0XkQm/5GOD/Aad5abTtcfv2nrjn3+y99h0i8icRGdKVfbM3InKxV59qEfmbiBwd99idIrJJRHaJyHtxr3WCiCzxlm8VkR91tTzTA1TV/jL4D1gPnNVm2f1AM3ABrnOQC5wAnIQ7ShwBvA98zVs/ACgwzLs/C9gOVABB4PfArG6sOwioBS7yHrsdCAHXdfJaulLH54B+wDDg05bXDnwNWAmUASXAfPf16LCcEUAdkB+37W1AhXf/Am8dAc4AGoCx3mNnAevjtlUJTPJuPwj8HegPHAasarPuFcAQ7z25yqvDQd5jNwJ/b1PPWcA93u2zvTqOA3KAnwJ/68q+6eD13w/82rt9rFePM7z36E5vvweBUcDHwGBv3eHACO/228BU73YhcFKqvwuZ9Gc9ftOZN1X1z6oaVdUGVX1bVReoalhVPwIeBybu4flPq+oiVQ0Bs3EBZ1/X/RywVFWf8x57GNdIdKiLdfwfVa1R1fW4INtS1hXAw6paqao7gAf2UM5HwApcgwTwWaBaVRd5j/9ZVT9S52/Aa0CHJ3DbuAK4X1V3qurHuF58fLlPqepm7z35Ha7RrujCdgGuBn6pqktVtRGYDkwUkbK4dTrbN3syBZirqn/z3qMHgCJcAxzGNTKjvHThOm/fgWvAjxSRElWtVdUFXXwdpgdY4Ded2RB/R0SOEZEXRGSLiOwC7gMG7uH5W+Ju17PnE7qdrXtwfD1UVXE95A51sY5dKgvXU92T3wFTvdtX4Rqslnp8TkQWiMinIlKN623vaV+1GLKnOojIdSLyrpdSqQaO6eJ2wb2+2PZUdRewExgat86+vGedbTeKe4+Gquoa4Fu492Gblzoc7K16PTASWCMiC0XkvC6+DtMDLPCbzrQdyvhzXC/3CFUtAr6PS2Uk02Zc6gUAERESA1Vb+1PHzcAhcff3Ntz098BZXo/5IlxDgIjkAk8D/4NLwxQDf+liPbZ0VgcRGQE8BtwClHjbfS9uu3sberoJlz5q2V4hLqW0sQv12pft+nDv2UYAVZ2lqqfg0jx+3H5BVdeo6hRcOu9/gWdEJGc/62K6yAK/6apCoAbYLSLHAl/uhTKfB8pF5AIRCQC3AaVJquNTwDdEZKiIlAB37GllVd0KvAnMBNao6lrvoWwgC6gCIiLyOeDMfajDnSJSLO53Dl+Le6wAF9yrcG3gjbgef4utQFnLyewOPAncICJjRSQbF4DfUNVOj6D2oc4Xisgkr+x/x52XWSAix4rIZK+8Bu8vgnsBXxCRgd4RQo332qL7WRfTRRb4TVd9C7gW96X+Oa7Hm1RecL0SeAjYARwOvIP73UFP1/ExXC5+Oe7E49NdeM7vcCdrfxdX52rgm8CzuBOkl+EasK64G3fksR54Cfi/uO0uA2YAC711jgHi8+KvAmuBrSISn7Jpef7LuJTLs97zD8Xl/feLqq7E7fPHcI3SOcCFXr4/G/gh7rzMFtwRxl3eU88DVosbNfYgcKWqNu9vfUzXiEubGtP3iYgfl1q4TFXfSHV9jDlQWY/f9Gkico6I9PPSBd/DjRRZmOJqGXNAs8Bv+rpTgY9w6YJzgItVtbNUjzGmCyzVY4wxGcZ6/MYYk2EOiEnaBg4cqMOGDUt1NYwx5oCyePHi7arabgj0ARH4hw0bxqJFi1JdDWOMOaCISIe/QLdUjzHGZBgL/MYYk2Es8BtjTIZJWo5fRJ7ATau7TVVHe8t+hJurvBn4ELje+4m7MaYPCYVCVFZW0tjYmOqqmC7IycmhrKyMYLCzqZoSJfPk7q9x84n/X9yyV4HvqGpYRH4AfIe9TIZljOl9lZWVFBYWMmzYMNykqKavUlV27NhBZWUlw4cP3/sTSGKqR1Xn4yapil/2F229duu/iJty1xjTdzQ2NlJSUmJB/wAgIpSUlOzT0Vkqc/xfxM1A2CERuUlEFonIoqqqql6sljEGsKB/ANnX9yolgV9EvoubbGt2Z+uo6uOqWqGqFaWle5qCvXOvrd7KT//+QTdraYwx6anXA7+IXIs76Xu1JnmioL+vqeIX8z/a+4rGmD5lx44djBs3jnHjxjF48GCGDh0au9/c3LVp+6+//nrWrFmzx3UeffRRZs/utP+5T0499VSWLl3aI9tKtl795a6InIM7mTtRVeuTXZ7fJ0SiNgmdMQeakpKSWBC95557KCgo4Nvf/nbCOqqKquLzddx/nTlz5l7L+epXv7r/lT0AJa3HLyJPAm8BR4tIpYjcgBvlUwi8KiJLReRnySofwCeCxX1j0scHH3zA6NGjufnmmykvL2fz5s3cdNNNVFRUMGrUKO67777Yui098HA4THFxMdOnT+e4447j5JNPZtu2bQDcddddPPLII7H1p0+fzoknnsjRRx/NP//5TwB2797N5z//eY477jimTp1KRUXFXnv2s2bNYsyYMYwePZo777wTgHA4zBe+8IXY8hkzZgDw8MMPM3LkSI477jimTZvW4/usI0nr8avq1A4W/ypZ5XXEJxC1aaeN2S/3/nklqzbt6tFtjjy4iLsvGNWt565atYqZM2fys5+5fuMDDzzAgAEDCIfDTJ48mcsuu4yRI0cmPKempoaJEyfywAMPcPvtt/PEE08wffr0dttWVRYuXMjcuXO57777ePnll/nJT37C4MGDeeaZZ3j33XcpLy/fY/0qKyu56667WLRoEf369eOss87i+eefp7S0lO3bt7N8+XIAqqvdT5h++MMf8vHHH5OVlRVblmxp/ctdS/UYk34OP/xwTjjhhNj9J598kvLycsrLy1m9ejWrVq1q95zc3FzOPfdcAI4//njWr1/f4bYvvfTSduu8+eabTJkyBYDjjjuOUaP23GAtWLCAM844g4EDBxIMBrnqqquYP38+RxxxBGvWrOG2227jlVdeoV+/fgCMGjWKadOmMXv27C7/AGt/HRCzc3aXzyfW4zdmP3W3Z54s+fn5sdtr167lxz/+MQsXLqS4uJhp06Z1OJ49Kysrdtvv9xMOh9utA5Cdnd1unX0dg9LZ+iUlJSxbtoyXXnqJGTNm8Mwzz/D444/zyiuv8Prrr/Pcc89x//33s2LFCvx+/z6Vua/Su8cv1uM3Jp3t2rWLwsJCioqK2Lx5M6+88kqPl3Hqqafy1FNPAbB8+fIOjyjiTZgwgXnz5rFjxw7C4TBz5sxh4sSJVFVVoapcfvnl3HvvvSxZsoRIJEJlZSVnnHEGP/rRj6iqqqK+PunjXjKhx+9aYPsxijHpp7y8nJEjRzJ69GhGjBjBKaec0uNlfP3rX+eaa65h7NixlJeXM3r06FiapiNlZWXcd999TJo0CVXlggsu4Pzzz2fJkiXccMMNsXj0gx/8gHA4zFVXXUVtbS3RaJQ77riDwsLCHn8NbR0Q19ytqKjQ7lyI5cd/XcvDf32fD//7PPw+C/zGdNXq1as59thjU12NPiEcDhMOh8nJyWHt2rWcffbZrF27lkCgb/WbO3rPRGSxqla0Xbdv1byH+b1EVlQVPxb4jTH7rq6ujjPPPJNwOIyq8vOf/7zPBf19dWDXfi9a0juRqBJM7rkSY0yaKi4uZvHixamuRo9K75O7XnrHRvYYY0yr9A78cT1+Y4wxTloHfl9Ljz+a4ooYY0wfktaB3++dz41YqscYY2LSO/D7LNVjzIFo0qRJ7X6M9cgjj/CVr3xlj88rKCgAYNOmTVx22WWdbntvw8MfeeSRhB9SnXfeeT0yj84999zDgw8+uN/b2V9pHfhbUj0Hwm8VjDGtpk6dypw5cxKWzZkzh6lTO5r7sb2DDz6Yp59+utvltw38L774IsXFxd3eXl+T3oG/5eSuBX5jDiiXXXYZzz//PE1NTQCsX7+eTZs2ceqpp8bG1ZeXlzNmzBiee+65ds9fv349o0ePBqChoYEpU6YwduxYrrzyShoaGmLr3XLLLbEpne+++24AZsyYwaZNm5g8eTKTJ08GYNiwYWzfvh2Ahx56iNGjRzN69OjYlM7r16/n2GOP5Utf+hKjRo3i7LPPTiinI0uXLmXChAmMHTuWSy65hJ07d8bKHzlyJGPHjo1NDvf666/HLkQzfvx4amtru71vIc3H8duoHmN6wEvTYcvynt3m4DFw7gOdPlxSUsKJJ57Iyy+/zEUXXcScOXO48sorERFycnJ49tlnKSoqYvv27UyYMIELL7yw02lZHnvsMfLy8li2bBnLli1LmFb5v/7rvxgwYACRSIQzzzyTZcuWceutt/LQQw8xb948Bg4cmLCtxYsXM3PmTBYsWICqctJJJzFx4kT69+/P2rVrefLJJ/nFL37BFVdcwTPPPLPH+fWvueYafvKTnzBx4kS+//3vc++99/LII4/wwAMPsG7dOrKzs2PppQcffJBHH32UU045hbq6OnJycvZlb7eT3j1+G9VjzAErPt0Tn+ZRVe68807Gjh3LWWedxcaNG9m6dWun25k/f34sAI8dO5axY8fGHnvqqacoLy9n/PjxrFy5cq8TsL355ptccskl5OfnU1BQwKWXXsobb7wBwPDhwxk3bhyw56mfwV0foLq6mokTJwJw7bXXMn/+/Fgdr776ambNmhX7hfApp5zC7bffzowZM6iurt7vXw6nd4/fa9Ys1WPMfthDzzyZLr74Ym6//XaWLFlCQ0NDrKc+e/ZsqqqqWLx4McFgkGHDhnU4FXO8jo4G1q1bx4MPPsjbb79N//79ue666/a6nT2dL2yZ0hnctM57S/V05oUXXmD+/PnMnTuX//zP/2TlypVMnz6d888/nxdffJEJEybw17/+lWOOOaZb24d07/FbqseYA1ZBQQGTJk3ii1/8YsJJ3ZqaGgYNGkQwGGTevHl8/PHHe9zO6aefHrug+ooVK1i2bBngpnTOz8+nX79+bN26lZdeein2nMLCwg7z6Keffjp/+tOfqK+vZ/fu3Tz77LOcdtpp+/za+vXrR//+/WNHC7/97W+ZOHEi0WiUDRs2MHnyZH74wx9SXV1NXV0dH374IWPGjOGOO+6goqKC9957b5/LjJfmPX6bssGYA9nUqVO59NJLE0b4XH311VxwwQVUVFQwbty4vfZ8b7nlFq6//nrGjh3LuHHjOPHEEwF3Na3x48czatSodlM633TTTZx77rkMGTKEefPmxZaXl5dz3XXXxbZx4403Mn78+D2mdTrzm9/8hptvvpn6+npGjBjBzJkziUQiTJs2jZqaGlSVb37zmxQXF/O9732PefPm4ff7GTlyZOxqYt2V1tMyv7R8M7fMXsLL3ziNYwYXJaFmxqQnm5b5wLMv0zKndapHLNVjjDHtpHXg99uoHmOMaSfNA7/7b6N6jNl3B0Ia2Dj7+l6ldeC3UT3GdE9OTg47duyw4H8AUFV27NixTz/qStqoHhF5AvgcsE1VR3vLBgC/B4YB64ErVHVnsupgo3qM6Z6ysjIqKyupqqpKdVVMF+Tk5FBWVtbl9ZM5nPPXwP8D/i9u2XTgNVV9QESme/fvSFYFbMoGY7onGAwyfPjwVFfDJEnSUj2qOh/4tM3ii4DfeLd/A1ycrPIhbsoG6/EbY0xMb+f4D1LVzQDe/0GdrSgiN4nIIhFZ1N3DzZYcv43qMcaYVn325K6qPq6qFapaUVpa2q1t2KgeY4xpr7cD/1YRGQLg/d+WzMJae/wW+I0xpkVvB/65wLXe7WuB9ldQ6EF26UVjjGkvaYFfRJ4E3gKOFpFKEbkBeAD4rIisBT7r3U8auwKXMca0l7ThnKra2cUxz0xWmW357Zq7xhjTTp89udsTWlM9Ka6IMcb0IWkd+L24b6keY4yJk+aB30b1GGNMW2kd+G1UjzHGtJfWgd9G9RhjTHtpHfhbL8Rigd8YY1pkRuC3uG+MMTFpHfgt1WOMMe2leeB3/y3VY4wxrdI68NuoHmOMaS+tA79diMUYY9pL68Bvl140xpj20jvw++zkrjHGtJXWgb9lVI/FfWOMaZXWgd9O7hpjTHtpHfhjs3Na4DfGmJi0DvwigoiN6jHGmHhpHfjBjeyxHr8xxrRK+8Dv84mN6jHGmDhpH/j9IjZlgzHGxEn/wO8Tm53TGGPipH3g94mN6jHGmHjpH/h9YqN6jDEmTtoHfhvVY4wxidI+8FuP3xhjEqUk8IvIN0VkpYisEJEnRSQnWWVZj98YYxL1euAXkaHArUCFqo4G/MCUZJXn9wmRaLK2bowxB55UpXoCQK6IBIA8YFOyCvL5QC3VY4wxMb0e+FV1I/Ag8AmwGahR1b+0XU9EbhKRRSKyqKqqqtvl+cR+uWuMMfFSkerpD1wEDAcOBvJFZFrb9VT1cVWtUNWK0tLSbpdnOX5jjEmUilTPWcA6Va1S1RDwR+AzySrMRvUYY0yiVAT+T4AJIpInIgKcCaxOVmHW4zfGmESpyPEvAJ4GlgDLvTo8nqzyfDaqxxhjEgRSUaiq3g3c3Rtl+W1UjzHGJEj7X+76bVSPMcYkSPvAL5bjN8aYBGkf+P02qscYYxKkf+C3Hr8xxiRI+8Dv80HURvUYY0xM2gd+v11s3RhjEqR94PeJ5fiNMSZe2gd+v0+IWo7fGGNi0j7w2+ycxhiTKDMCv53cNcaYmLQP/H4fluoxxpg4GRD4LdVjjDHx0j7w+8RO7hpjTLy0D/w2ZYMxxiRK/8Bvo3qMMSZB2gd+EbEpG4wxJk7aB36/D5ukzRhj4mRA4LdUjzHGxEv7wG+jeowxJlHaB37r8RtjTKIuBX4ROVxEsr3bk0TkVhEpTm7Veob1+I0xJlFXe/zPABEROQL4FTAc+F3SatWD3Dj+VNfCGGP6jq4G/qiqhoFLgEdU9ZvAkORVq+f4xEb1GGNMvK4G/pCITAWuBZ73lgWTU6We5bMcvzHGJOhq4L8eOBn4L1VdJyLDgVndLVREikXkaRF5T0RWi8jJ3d3W3vgtx2+MMQkCXVlJVVcBtwKISH+gUFUf2I9yfwy8rKqXiUgWkLcf29ojG9VjjDGJujqq5+8iUiQiA4B3gZki8lB3ChSRIuB03EliVLVZVau7s62u8ImgCmrB3xhjgK6nevqp6i7gUmCmqh4PnNXNMkcAVbjG4x0R+aWI5LddSURuEpFFIrKoqqqqm0W5Hj9gI3uMMcbT1cAfEJEhwBW0ntztrgBQDjymquOB3cD0tiup6uOqWqGqFaWlpd0urCXw28geY4xxuhr47wNeAT5U1bdFZASwtptlVgKVqrrAu/80riFICnFx3+bkN8YYT1dP7v4B+EPc/Y+Az3enQFXdIiIbRORoVV0DnAms6s62usIv1uM3xph4XT25WyYiz4rINhHZKiLPiEjZfpT7dWC2iCwDxgH/vR/b2qNYqsd6/MYYA3Sxxw/MxE3RcLl3f5q37LPdKVRVlwIV3XnuvvJ5PX4by2+MMU5Xc/ylqjpTVcPe36+B7p9x7UU2qscYYxJ1NfBvF5FpIuL3/qYBO5JZsZ7is1E9xhiToKuB/4u4oZxbgM3AZbhpHPq8lpO7NqrHGGOcLgV+Vf1EVS9U1VJVHaSqF+N+zNXneR1+6/EbY4xnf67AdXuP1SKJLNVjjDGJ9ifwS4/VIoks1WOMMYn2J/AfEJHUpmwwxphEexzHLyK1dBzgBchNSo16mM+GcxpjTII9Bn5VLeytiiSLpXqMMSbR/qR6Dgh+7xVaqscYY5y0D/xik7QZY0yCtA/8luoxxphE6R/4bVSPMcYkSPvA3zqqxwK/McZABgT+1lRPiitijDF9RNoHfp+N6jHGmARpH/j9diEWY4xJkPaB32eXXjTGmATpH/htHL8xxiRI+8Dvt1E9xhiTIP0DfyzHn+KKGGNMH5H2gT82qsd6/MYYA2RA4I+leizHb4wxQAYE/tjJXevxG2MMkEmB33r8xhgDpDDwi4hfRN4RkeeTWY6N6jHGmESp7PHfBqxOdiH+WI8/2SUZY8yBISWBX0TKgPOBXya7rJZRPdbjN8YYJ1U9/keA/wA67YeLyE0iskhEFlVVVXW7IBvVY4wxiXo98IvI54Btqrp4T+up6uOqWqGqFaWlpd0uz2+jeowxJkEqevynABeKyHpgDnCGiMxKVmFis3MaY0yCXg/8qvodVS1T1WHAFOBvqjotWeXZpReNMSZR2o/jb031pLgixhjTRwRSWbiq/h34ezLLiI3qsR6/McYAmdDjtx9wGWNMgrQP/DZXjzHGJEr7wG/j+I0xJlHaB36fTdlgjDEJMiDwu/+W6jHGGCftA7+I4BNL9RhjTIu0D/zg8vzW4zfGGCcjAr9PxIZzGmOMJyMCv98nluoxxhhPZgR+ERvVY4wxnowI/CL2y11jjGmREYHf7xObndMYYzyZE/itx2+MMUCGBH6fCGqB3xhjgAwJ/JbqMcaYVhkR+H02qscYY2IyIvD7ffYDLmOMaZERgd8nds1dY4xpkRmB30b1GGNMTEYEfr/YlA3GGNMiMwK/5fiNMSYmIwK/jeoxxphWGRH4rcdvjDGtMiLw++wHXMYYE9PrgV9EDhGReSKyWkRWishtyS7TZ7NzGmNMTCAFZYaBb6nqEhEpBBaLyKuquipZBbr5+C3wG2MMpKDHr6qbVXWJd7sWWA0MTWaZluoxxphWKc3xi8gwYDywoIPHbhKRRSKyqKqqar/K8YtgmR5jjHFSFvhFpAB4BviGqu5q+7iqPq6qFapaUVpaul9l2Xz8xhjTKiWBX0SCuKA/W1X/mOzyLNVjjDGtUjGqR4BfAatV9aHeKNNvo3qMMSYmFT3+U4AvAGeIyFLv77xkFuizUT3GGBPT68M5VfVNQHqzTEv1GGNMq4z45a5fbMoGY4xpkRmB3ydYh98YY5yMCPw+n83Hb4wxLTIi8PsFG8dvjDGejAj8dnLXGGNaZUbgt0svGmNMTEYEfr/YlA3GGNMiIwK/z0b1GGNMTEYEfr8PS/UYY4wnMwK/pXqMMSYmIwK/jeoxxphWGRH4/TaqxxhjYjIi8BflBqkPRajcWZ/qqhhjTMplROC/vKKMoM/HY3//MNVVMcaYlEvvwL9tNbz9K4b0y+WKE8p4atEGNlU3pLpWxphU2PEhvXLx7Vfvht9cCNFI8svqpvQO/P/6KbzwLVjzMrdMOgKAn79uvX5jui0a3fPj6+a7Dldfs/AX8JNyeLMbF/2r/gR27+jausv+AP94BNa9Du89v+d1I2HY/sHe92kS9PqFWHrVuT+Eze/CH7/E0Btf47Ljy5i14BNKC7O5ZdIR+H29ej0Yk+miUfD1YF+rdov7Xzi457bZmWjEdaTm/Q+c+g04/d9B2nx/1rwMT17pbg87DSZ/Fw472fWyVzwDRQfDYZ9Jfl3bev8v8NJ/QCAXXv8RjLkCig9pv14kDOJLfI9WPgvP3gzihwm3wIiJULcNhhwHA49MfP6WFfDn2+DQk906b/wvHHth+/0E0FQLT10DH/4N+h0Coy+FshNdvbatdkcnh54Ew06HQFbP7g9A9AAY315RUaGLFi3q3pOrN8DjkyArn/qLn2D6W37mvruJk4YP4L6LRnP04MIerasx7YSb4I2H4B8/huOmwGfvg5yijteNhOGTf7rAkJUPQysguyBxnWgUFv0KXv0+BHPhuhdh0DGdlx+NQqQZgjmJy6veh5V/hPJrXFAGaKiGtx6FVX+Cc38Ah58BuzbDMzfAx/+A/sNh5zqY8BU4aDRUvg1lFa6eT5wNxYe5ILbwl1C7CU77lgtk7z3vgufnHoLjr/P2S7N7DVXvwTHnuzp88hb4s+EzX4fc4s5fT80GCDVA6dEdB9YWK/8Ez30VBoyAS3/hYsGRZ8GVsxLXW/40PH+7C7KHnwGFQ2B3FSydDYecBEVD3b5qEcyDKbNhxGRY86I7ovjo75BXAje/AR/8FeZ+HS5+DD54DbYsg5O/5t7/Te+4hmjLCjjlNtiy3DUA2kFqKLsILpvp6twNIrJYVSvaLU/7wA9QuQh+/wXYvQ2d8FX+0XwE9y8S3m8awKXlZXx25EFMGF5Cv7xg17epuucPnDnwRCOwei4074axV4J/L5+Hhp2w+DewdQVMvKO1B6gKn/zLBYraLe6LvXMdHDIBNixwAW7wWNe7PGgkHDrBpRIq34ZVz8Huba1l9B8OU5+EQcdC/adum0t+C5uXuqCzbRUgcPGj0LgLomHoP8y9lg3/cvX45F/QtMv1UoefDqMugfod8Ifr3fJADoy5DHZvh4/fgqYayB8EjdUw+U74189cQ3Tej+C4qfDyHbDwcVe/QC6EvfNmOf3gy/Nd+U218MK3Ydkc8AXhjLtgvRcQx1zuAuBbj7qAV3wYVH/stuHPcq8hbyCMn+aCb/UnXrplu/vOhZsg0uTWP+QkOPmrcHA55KnuaSIAABM0SURBVA1wQXXLctfQbX7XHWkcPB6mPAlFQ1wv/LX7YNAot35OPwg3unqVnQj9D4MP57n94gu4Ruy8/3WNZtUaqN0M2YUw91bY/j6UHOHeg36HuPqO/wL0G+oatRnjYNdG15ANPAq2LnfbjIYhmA+Xz4Sj/s29juZ6t53qj6H0WFePdW+4BnPSdOhX1q2PdGYHfnBfmhe/7T4Ini25R/Ds7jEsDx/Chwwle9BRnHDEYE4eUcLxh/UnN8uP3ycE/W0Oz9e85N74CbfAabfvX732V/2nLp94zAXgT9PM3ZYVsPDnrtc29HgXyNbNh5EXQcUNiYfmTXXu/Vn9nAsoJ34JcvvDxsWuA1C5yH3hx1zuAtT2991RYd0WF3R3fOC2U3qs+yJXrXZBR3yuNzfqEhhwuOtxL/0dhOpd8PP54cy7oX676wFuWe6+3P3KoGAQfOZWOOps2PA2/O0+17OOhFz5LT29QC4ccaZrdIoPdeW+8C1XxsCjXFBDXdA6+Ssw7mrXW/71+S6Qd6TkSNew5JfChoWu4YmG3GMHjXbBfPGv3WsfMMIFyZO+7ALZ7Mth4yK3n6b8Dg4a5Z6n6oJ47gAYNBLWz3f7YtzVLhUS7/1X3D44aJQ7mnntXlg0E5pr3RHAhTPcft72nmtohoxzr+mFb7myCw5y72PxoW4/qroGueRw1+N/66dQ80nHr90XgFNvh4n/0dqIh5tg3n+7/d5Q7cpsqnMN0cQ7uv4datgJT17lGunTvu0+T22fu2qua6gn3+Xqu/rP8PE/3fsxYqL7XCaZBf4WDTvdCZXKhbD6efSTtxDcPojgY6MOpFZzaSJIDiF8RHk/Zyw7yyZz0IBijmpczvAVM4hkFxFoqmHt0bcgJ1zPiIIwvkCW6w3kD+p6Lrdhp/sw5w3Y99eyaak7kqn5BA47BS593DUEn34IR5zl6tJVdVXuy9D/MPdFjUZcoPlwnmtYNOq+fPml7vCzcLA7zA7muV7ZlmWu9xZugsPPhKHl7stWs9H1puq2ujqWnQCBbNfLLBrigt3md2DrKvclbNzleluhehd8omFY9IT7EocbvcqKFxg/hkM/4wLC5nddGSHvtxoth+qxkRXqnld6tOu1NdYkvn7xw5CxcOo3XVkvTXf7NXeA69WJwKcfuW2C68WNvRxOusV9gf9wnftMic8Fr/IvuACelb/n/d64CzYtcZ+ZgUe1Dx41G+HPt7oe9OFnwNHnekcLkrjOlmVun/gC8Ok6934dciLkD0zcXv2nLgDVbnaph5Y0UkdHsE218O4cGP357n0+OxNqgLWvus/Z0PKO11F1DePe8tuRkGvMtq91783gsW6bwTx39JCE/HhCHfv4Ub8F/s4018OOtS7fuX0NkR0fUVNTTf3u3YR82WgkxNCaxWRrU+wpL0RO5N9DN/O9wG+ZGpjXbpONvjw+ChyO+LMo8dXiy8ojVHAw/mAuWTSTRTPBaDOBmvXIznXuScWHwsCj3Rc1r8T9BXKguc59wAoHu5xnqNEFuE3vuJ5l3kA44Ysw/8HWoAeQU+yCT7jJBYL6HS6FUXyIl6ddD1tXuudEI64H1qLkCPclagmOg8dAVoHrgdbviAvAbQwa6YL6pncSl+eXuvTG5mXAHj5v4ne575x+LrDWbHD1G3c1nH2/+5JvWuJSFoVD4J1Z8Ne7Xd0OHu8CSW6xO7l26GdccHtnlvtylp3gAkJOP7cPP3jV9fhKj3Y92rwS12tvEW5yqYWig1u/3JEwfDTPNQCjLoWC0tb1IyHXEA88svPctDG9zAL//miuRysXUtMQYVNTNtvyjqQxHKUox88hm15ha9V2Vu0Uduyqo7mumiN9Gxkl6whHomwJ5ZFHE0NkBwGJ0KRBmsiiiSBbtD9rfEcSDAYYIx9xsG6hKFpDke4iJ66h6Uhj3sHsGnwSdRPvYUDpwQR3riV75R8IDB7pDo8X/Mw1DFmFUDLCNRBZeS7gf7rONTSDx7pAKz6v4TnKHWave8P1okdMcn9te43hZheUt7/vGoH8Qe5QtmV0Sd02lzKJRlwQHDTKHQHVf+q2Hw27Xt+uTa5hGzzW9bZzihN7UKqusWp7cjPeAdDrMiZVLPCnSGMowva6JnY1hNnVGGJXQ4iahhC7GsPsagh5y8LUNIRojkTJ8gvV9SHWb9lOqKmBBsnBJ0JJ9FOKpJ4GsqjWAqrpOI2THfBRlBskElX8kQayc/IZUJBNcV4W/fOCFGQHKMgOkJ8dIC/LT31zhLqmMKUF2YwozScvK4CisY55y6fD75PYtgcVZuMToaYhRFbAR0l+FmLB15g+p7PAn6ZnA/uOnKCfsv55sI/ncVSVpnCU7IAPVdhZ38zO+hCRqBKKRIlElfrmCNtqG9le10w0qjRHotQ0hKhtDBHw+fD7hF0NIT71nrt++252N4WpawrTFG790UhWwEdzuPs/IskN+umfF0SBgF8oyA5SmB2gMCdA0O+jPhQhGlWKcgNkB/zUNoZpCkfw+4ScgJ9BRdmU5GcT8At+n+AXQcQ1NgG/j6KcAEU5Qaobmtm5O8TAwmyGFudQ2xhmW20T/fOyGFqci98nNIQiZPl9FGQHyAr48AmICD6B/OwAOcHWdI6qUtsUxidCQbZ9FUzmSMmnXUTOAX4M+IFfquoDqahHXyYisSAlAiUF2ZQUZPfY9sORKPWhCDkBP1kBHzt3N7Nux26aQtFYmfF9+EhUaQxHqGkIUVXbRFShX26QxlCEDZ82sKsxhAChSJS6pgh1TSG21jbSHI6SmxXAJ7C5poGmcJTCnCC5QV+s8Xrrox3UNIR67LXtSXbAF2tMG0IRwt6srXlZfnKD7ggIoCg3QF5WoHUfxO2M+P0Sf6QjsWXuv0+Esv65lPXPc6MQI0o4GiUUUcKRKOGoesuU/nlBhvTLIaLKroYwAb9QmBMkHIlS2xgm6PcxID+IiNDQHCGiik/czLM+r7H0+do3nH6f4BP3F1VFcQ1eS2OXHfDxaX2zO3rz+8gO+mP7KOh3nYeAt52AzzWkdU1hahvD5Gf7Kc5zJ09DkSi5QT/9coM0R6Lsagi7cnxC/7wsDi7OoTEU5eMdu4kqDCrMJj+7/dGlKm5Z7LarL0BjKMrWXY00NEc46qBCDhmQS1ShvjlMQyhCU8h1iKKqRL0M4KDCbApz3Od0x+5mfAJZfh9ZAe/P70NEYh2thuYIDaEIeVnutXT3SDYSVXY1hFCgf14wVkYoomQFujbwQ1WpawqT7X1He1KvB34R8QOPAp8FKoG3RWSuqq7q7bpksoDfR1HcMNX++Vn0z0/iCIi9aDmKiap6/91V00LRqJcSC1OcG6R/XhbbapvYWF1PUU6Q0sJsdtaH2LjTjSXPCfoIecEy3BIEom6b9aEI1fUhmsNRRFqOVLKIqLJ1VyONoSj5Wa6xrW0MUx9yjUB8OjQhMarxN1uDVYvmcJTKnQ0sWPcpAAHvCCbo/Q/4XVD1ibB8YzPbapvwiVCYEyAccV/6gM/dD3n3AXwCAZ+PiLevMpW/i9fZyAn6aAx1fkSb5fcRikbbTeMT8Eks4GoH73X75a1CkdbtFWQH6JcbpKquieZwlJygOyJVJdZIqSpBv2tsA37XcO+oa6a+OcKsG07i1CPbnGfbT6no8Z8IfKCqHwGIyBzgIsACfwZzH/qOHxtUmPiL0/75WQm/uD6sBMYdcuCPpIlENZaaAtfwSdz9pnAEVXfUEt8TjUY11gi0BJJIVGPLo15D2rJtEfec2qYwTaEoJQVZFOUECUWjNIWiNIYiNIVdQxyORr30ott+JKoUeGm8+uYI1fXNAAQDPhqa3RFh0O+jMCcQC8yf7m5mU3UDWQEfwwbmE/AJ23Y1xRpWofUoSZCEo0132y3IDvg4qCiHrICP97fUsmFnPdkBd6SWm+WOVAJ+15CKdw2OrbsaqaptojgvyEDviLkpHKU5HKU5EqUpFKEpEiXL7yPXO+rL8Y78dtQ1EYq0NhgdHd21vSPenSy/UJyXhQIbPq2npiHEoMJsCrID1HrpVp8QOxoD11iEIy6VG1WlpCCbQYXZHDogbz8+VR1LReAfCmyIu18JnNR2JRG5CbgJ4NBDD+2dmhmTQm3njvK1uZ8d6Lhl9PkEH9Jpw9mZQW3u5+KHnA5X7XPKD03+j5/SWSpm5+woadbueE1VH1fVClWtKC0t7eApxhhjuiMVgb8SiJ8arwzYlIJ6GGNMRkpF4H8bOFJEhotIFjAFmJuCehhjTEbq9Ry/qoZF5GvAK7jhnE+o6srerocxxmSqlIzjV9UXgRdTUbYxxmS69L70ojHGmHYs8BtjTIaxwG+MMRnmgJidU0SqgI/38WkDge1JqE5Psjr2DKvj/uvr9QOrY3ccpqrtfgh1QAT+7hCRRR1NR9qXWB17htVx//X1+oHVsSdZqscYYzKMBX5jjMkw6Rz4H091BbrA6tgzrI77r6/XD6yOPSZtc/zGGGM6ls49fmOMMR2wwG+MMRkmLQO/iJwjImtE5AMRmd4H6nOIiMwTkdUislJEbvOWDxCRV0Vkrfc/5VeXEBG/iLwjIs9794eLyAKvjr/3ZlRNZf2KReRpEXnP258n97X9KCLf9N7nFSLypIjkpHo/isgTIrJNRFbELetwv4kzw/v+LBOR8hTW8Ufee71MRJ4VkeK4x77j1XGNiPxbquoY99i3RURFZKB3PyX7sSvSLvDHXdP3XGAkMFVERqa2VoSBb6nqscAE4KtenaYDr6nqkcBr3v1Uuw1YHXf/B8DDXh13AjekpFatfgy8rKrHAMfh6tpn9qOIDAVuBSpUdTRuBtoppH4//ho4p82yzvbbucCR3t9NwGMprOOrwGhVHQu8D3wHwPv+TAFGec/5qffdT0UdEZFDcNcR/yRucar2496palr9AScDr8Td/w7wnVTXq00dn8N9SNYAQ7xlQ4A1Ka5XGS4AnAE8j7ta2nYg0NG+TUH9ioB1eIMS4pb3mf1I66VFB+Bmv30e+Le+sB+BYcCKve034OfA1I7W6+06tnnsEmC2dzvhe42b5v3kVNUReBrXEVkPDEz1ftzbX9r1+On4mr5DU1SXdkRkGDAeWAAcpKqbAbz/bS+D2tseAf4DaLnCdAlQraph736q9+UIoAqY6aWjfiki+fSh/aiqG4EHcT2/zUANsJi+tR9bdLbf+up36IvAS97tPlNHEbkQ2Kiq77Z5qM/Usa10DPxduqZvKohIAfAM8A1V3ZXq+sQTkc8B21R1cfziDlZN5b4MAOXAY6o6HthN30iPxXh58ouA4cDBQD7ukL+tPvGZ7ERfe98Rke/iUqazWxZ1sFqv11FE8oDvAt/v6OEOlvWJ9z0dA3+fvKaviARxQX+2qv7RW7xVRIZ4jw8BtqWqfsApwIUish6Yg0v3PAIUi0jLBXtSvS8rgUpVXeDdfxrXEPSl/XgWsE5Vq1Q1BPwR+Ax9az+26Gy/9anvkIhcC3wOuFq9nAl9p46H4xr5d73vThmwREQG03fq2E46Bv4+d01fERHgV8BqVX0o7qG5wLXe7Wtxuf+UUNXvqGqZqg7D7bO/qerVwDzgMm+1VNdxC7BBRI72Fp0JrKIP7UdcimeCiOR573tLHfvMfozT2X6bC1zjjUqZANS0pIR6m4icA9wBXKiq9XEPzQWmiEi2iAzHnUBd2Nv1U9XlqjpIVYd5351KoNz7rPaZ/dhOqk8yJOnky3m4EQAfAt/tA/U5FXeItwxY6v2dh8uhvwas9f4PSHVdvfpOAp73bo/AfaE+AP4AZKe4buOARd6+/BPQv6/tR+Be4D1gBfBbIDvV+xF4EnfOIYQLTjd0tt9wKYpHve/PctwIpVTV8QNcnrzle/OzuPW/69VxDXBuqurY5vH1tJ7cTcl+7MqfTdlgjDEZJh1TPcYYY/bAAr8xxmQYC/zGGJNhLPAbY0yGscBvjDEZxgK/yWgiEhGRpXF/PfZLYBEZ1tEsjsakWmDvqxiT1hpUdVyqK2FMb7IevzEdEJH1IvIDEVno/R3hLT9MRF7z5ld/TUQO9ZYf5M0X/6739xlvU34R+YU3P/9fRCTXW/9WEVnlbWdOil6myVAW+E2my22T6rky7rFdqnoi8P9w8xbh3f4/dfPDzwZmeMtnAK+r6nG4+YNWesuPBB5V1VFANfB5b/l0YLy3nZuT9eKM6Yj9ctdkNBGpU9WCDpavB85Q1Y+8Cfa2qGqJiGzHzake8pZvVtWBIlIFlKlqU9w2hgGvqrvQCSJyBxBU1ftF5GWgDjftxJ9UtS7JL9WYGOvxG9M57eR2Z+t0pCnudoTW82rn4+ZxOR5YHDdzpzFJZ4HfmM5dGff/Le/2P3GzlwJcDbzp3X4NuAVi1y0u6myjIuIDDlHVebgL3xQD7Y46jEkW62WYTJcrIkvj7r+sqi1DOrNFZAGugzTVW3Yr8ISI/DvuamDXe8tvAx4XkRtwPftbcLM4dsQPzBKRfrgZHB9W1eoee0XG7IXl+I3pgJfjr1DV7amuizE9zVI9xhiTYazHb4wxGcZ6/MYYk2Es8BtjTIaxwG+MMRnGAr8xxmQYC/zGGJNh/j+kvZU5hvAIJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(loss) + 1)\n",
    "plt.plot(epochs,loss,label='Training loss')\n",
    "plt.plot(epochs,val_loss,label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3450/3450 [==============================] - 0s 37us/step\n",
      "[0.5509640333099641, 0.8719998598098755, 0.9777584075927734, 0.9774889349937439]\n"
     ]
    }
   ],
   "source": [
    "result =model.evaluate(test,Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
