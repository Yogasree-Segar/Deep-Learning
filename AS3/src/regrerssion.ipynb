{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\divya\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras) (5.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\divya\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.27.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\divya\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (45.2.0.post20200210)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.11.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\divya\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attdata = pd.read_csv('att.csv',delim_whitespace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = pd.read_csv('communities.data',names = attdata['attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 128)\n"
     ]
    }
   ],
   "source": [
    "print(comm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>community</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>...</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>81440</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>95</td>\n",
       "      <td>6096</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state county community        communityname  fold  population  \\\n",
       "0      8      ?         ?         Lakewoodcity     1        0.19   \n",
       "1     53      ?         ?          Tukwilacity     1        0.00   \n",
       "2     24      ?         ?         Aberdeentown     1        0.00   \n",
       "3     34      5     81440  Willingborotownship     1        0.04   \n",
       "4     42     95      6096    Bethlehemtownship     1        0.01   \n",
       "\n",
       "   householdsize  racepctblack  racePctWhite  racePctAsian  ...  LandArea  \\\n",
       "0           0.33          0.02          0.90          0.12  ...      0.12   \n",
       "1           0.16          0.12          0.74          0.45  ...      0.02   \n",
       "2           0.42          0.49          0.56          0.17  ...      0.01   \n",
       "3           0.77          1.00          0.08          0.12  ...      0.02   \n",
       "4           0.55          0.02          0.95          0.09  ...      0.04   \n",
       "\n",
       "   PopDens  PctUsePubTrans  PolicCars  PolicOperBudg  LemasPctPolicOnPatr  \\\n",
       "0     0.26            0.20       0.06           0.04                  0.9   \n",
       "1     0.12            0.45          ?              ?                    ?   \n",
       "2     0.21            0.02          ?              ?                    ?   \n",
       "3     0.39            0.28          ?              ?                    ?   \n",
       "4     0.09            0.02          ?              ?                    ?   \n",
       "\n",
       "   LemasGangUnitDeploy  LemasPctOfficDrugUn  PolicBudgPerPop  \\\n",
       "0                  0.5                 0.32             0.14   \n",
       "1                    ?                 0.00                ?   \n",
       "2                    ?                 0.00                ?   \n",
       "3                    ?                 0.00                ?   \n",
       "4                    ?                 0.00                ?   \n",
       "\n",
       "   ViolentCrimesPerPop  \n",
       "0                 0.20  \n",
       "1                 0.67  \n",
       "2                 0.43  \n",
       "3                 0.12  \n",
       "4                 0.03  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = comm.drop(columns = ['state','county','community','communityname','fold'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "comm = comm.replace('?',np.nan)\n",
    "feat_miss = comm.columns[comm.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 100)\n"
     ]
    }
   ],
   "source": [
    "comm = comm.dropna(axis=1)\n",
    "print(comm.shape)\n",
    "# commdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = commdata.iloc[:,0:100].values\n",
    "# y = commdata.iloc[:,100].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain,Xtest = train_test_split(comm,test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['population', 'householdsize', 'racepctblack', 'racePctWhite',\n",
      "       'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29',\n",
      "       'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome',\n",
      "       'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst',\n",
      "       'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap',\n",
      "       'indianPerCap', 'AsianPerCap', 'HispPerCap', 'NumUnderPov',\n",
      "       'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore',\n",
      "       'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ',\n",
      "       'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr',\n",
      "       'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par',\n",
      "       'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids',\n",
      "       'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent',\n",
      "       'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig',\n",
      "       'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly',\n",
      "       'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup',\n",
      "       'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous',\n",
      "       'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR',\n",
      "       'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded',\n",
      "       'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb',\n",
      "       'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ',\n",
      "       'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc',\n",
      "       'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters',\n",
      "       'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85',\n",
      "       'PctSameCity85', 'PctSameState85', 'LandArea', 'PopDens',\n",
      "       'PctUsePubTrans', 'LemasPctOfficDrugUn', 'ViolentCrimesPerPop'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(Xtest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80      0.30\n",
      "254     0.04\n",
      "1098    0.19\n",
      "1113    0.13\n",
      "1497    0.12\n",
      "        ... \n",
      "970     0.69\n",
      "47      0.12\n",
      "1050    0.13\n",
      "627     0.01\n",
      "1658    0.05\n",
      "Name: ViolentCrimesPerPop, Length: 1395, dtype: float64\n",
      "\n",
      "639     0.18\n",
      "1156    0.29\n",
      "1741    0.06\n",
      "1311    0.06\n",
      "865     0.04\n",
      "        ... \n",
      "960     0.26\n",
      "110     0.56\n",
      "155     0.44\n",
      "515     0.03\n",
      "962     0.01\n",
      "Name: ViolentCrimesPerPop, Length: 599, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Ytrain = Xtrain.pop(\"ViolentCrimesPerPop\")\n",
    "Ytest = Xtest.pop(\"ViolentCrimesPerPop\")\n",
    "print(Ytrain)\n",
    "print()\n",
    "print(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing\n",
    "\n",
    "#Xtrain,Xtest,Ytrain,Ytest\n",
    "\n",
    "x_mean = Xtrain.mean(axis=0)\n",
    "Xtrain -= x_mean\n",
    "x_std = Xtrain.std(axis=0)\n",
    "Xtrain /=x_std\n",
    "Xtest -= x_mean\n",
    "Xtest /= x_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model=models.Sequential()\n",
    "    model.add(layers.Dense(90, activation='relu',input_shape=(Xtrain.shape[1],)))\n",
    "    model.add(layers.Dropout(0.8))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(90,activation='relu'))\n",
    "    model.add(layers.Dropout(0.8))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 1163 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "1163/1163 [==============================] - 0s 304us/step - loss: 1.4228 - mae: 0.9383 - val_loss: 0.1891 - val_mae: 0.3142\n",
      "Epoch 2/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.9619 - mae: 0.7711 - val_loss: 0.0686 - val_mae: 0.1963\n",
      "Epoch 3/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.6644 - mae: 0.6453 - val_loss: 0.0453 - val_mae: 0.1722\n",
      "Epoch 4/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.3738 - mae: 0.4867 - val_loss: 0.0391 - val_mae: 0.1617\n",
      "Epoch 5/100\n",
      "1163/1163 [==============================] - 0s 67us/step - loss: 0.2008 - mae: 0.3557 - val_loss: 0.0345 - val_mae: 0.1452\n",
      "Epoch 6/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.1242 - mae: 0.2745 - val_loss: 0.0349 - val_mae: 0.1426\n",
      "Epoch 7/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0723 - mae: 0.2053 - val_loss: 0.0332 - val_mae: 0.1357\n",
      "Epoch 8/100\n",
      "1163/1163 [==============================] - 0s 72us/step - loss: 0.0556 - mae: 0.1749 - val_loss: 0.0327 - val_mae: 0.1394\n",
      "Epoch 9/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0504 - mae: 0.1666 - val_loss: 0.0309 - val_mae: 0.1325\n",
      "Epoch 10/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0480 - mae: 0.1627 - val_loss: 0.0297 - val_mae: 0.1337\n",
      "Epoch 11/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0443 - mae: 0.1559 - val_loss: 0.0273 - val_mae: 0.1225\n",
      "Epoch 12/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0439 - mae: 0.1549 - val_loss: 0.0269 - val_mae: 0.1222\n",
      "Epoch 13/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0442 - mae: 0.1572 - val_loss: 0.0265 - val_mae: 0.1222\n",
      "Epoch 14/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0402 - mae: 0.1495 - val_loss: 0.0250 - val_mae: 0.1152\n",
      "Epoch 15/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0430 - mae: 0.1543 - val_loss: 0.0256 - val_mae: 0.1173\n",
      "Epoch 16/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0391 - mae: 0.1444 - val_loss: 0.0233 - val_mae: 0.1179\n",
      "Epoch 17/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0404 - mae: 0.1490 - val_loss: 0.0223 - val_mae: 0.1123\n",
      "Epoch 18/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0392 - mae: 0.1462 - val_loss: 0.0222 - val_mae: 0.1125\n",
      "Epoch 19/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0385 - mae: 0.1449 - val_loss: 0.0232 - val_mae: 0.1128\n",
      "Epoch 20/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0377 - mae: 0.1435 - val_loss: 0.0205 - val_mae: 0.1041\n",
      "Epoch 21/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0362 - mae: 0.1410 - val_loss: 0.0202 - val_mae: 0.1050\n",
      "Epoch 22/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0382 - mae: 0.1445 - val_loss: 0.0209 - val_mae: 0.1036\n",
      "Epoch 23/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0371 - mae: 0.1400 - val_loss: 0.0199 - val_mae: 0.1051\n",
      "Epoch 24/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0361 - mae: 0.1417 - val_loss: 0.0198 - val_mae: 0.1042\n",
      "Epoch 25/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0358 - mae: 0.1417 - val_loss: 0.0223 - val_mae: 0.1044\n",
      "Epoch 26/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0355 - mae: 0.1367 - val_loss: 0.0193 - val_mae: 0.1027\n",
      "Epoch 27/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0348 - mae: 0.1372 - val_loss: 0.0201 - val_mae: 0.1044\n",
      "Epoch 28/100\n",
      "1163/1163 [==============================] - 0s 67us/step - loss: 0.0365 - mae: 0.1405 - val_loss: 0.0204 - val_mae: 0.1086\n",
      "Epoch 29/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0353 - mae: 0.1376 - val_loss: 0.0203 - val_mae: 0.1017\n",
      "Epoch 30/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0357 - mae: 0.1364 - val_loss: 0.0199 - val_mae: 0.1039\n",
      "Epoch 31/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0360 - mae: 0.1397 - val_loss: 0.0209 - val_mae: 0.1032\n",
      "Epoch 32/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0352 - mae: 0.1369 - val_loss: 0.0208 - val_mae: 0.1034\n",
      "Epoch 33/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0349 - mae: 0.1367 - val_loss: 0.0202 - val_mae: 0.1012\n",
      "Epoch 34/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0358 - mae: 0.1371 - val_loss: 0.0208 - val_mae: 0.1025\n",
      "Epoch 35/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0329 - mae: 0.1324 - val_loss: 0.0194 - val_mae: 0.0984\n",
      "Epoch 36/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0359 - mae: 0.1379 - val_loss: 0.0201 - val_mae: 0.1029\n",
      "Epoch 37/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0332 - mae: 0.1329 - val_loss: 0.0200 - val_mae: 0.1001\n",
      "Epoch 38/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0325 - mae: 0.1306 - val_loss: 0.0193 - val_mae: 0.0990\n",
      "Epoch 39/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0323 - mae: 0.1303 - val_loss: 0.0207 - val_mae: 0.1003\n",
      "Epoch 40/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0344 - mae: 0.1336 - val_loss: 0.0194 - val_mae: 0.1007\n",
      "Epoch 41/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0336 - mae: 0.1339 - val_loss: 0.0190 - val_mae: 0.1006\n",
      "Epoch 42/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0328 - mae: 0.1322 - val_loss: 0.0196 - val_mae: 0.0984\n",
      "Epoch 43/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0331 - mae: 0.1317 - val_loss: 0.0203 - val_mae: 0.0992\n",
      "Epoch 44/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0341 - mae: 0.1333 - val_loss: 0.0191 - val_mae: 0.1014\n",
      "Epoch 45/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0318 - mae: 0.1321 - val_loss: 0.0194 - val_mae: 0.0982\n",
      "Epoch 46/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0328 - mae: 0.1334 - val_loss: 0.0209 - val_mae: 0.1004\n",
      "Epoch 47/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0338 - mae: 0.1322 - val_loss: 0.0201 - val_mae: 0.0990\n",
      "Epoch 48/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0315 - mae: 0.1289 - val_loss: 0.0202 - val_mae: 0.1000\n",
      "Epoch 49/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0323 - mae: 0.1316 - val_loss: 0.0217 - val_mae: 0.0991\n",
      "Epoch 50/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0328 - mae: 0.1318 - val_loss: 0.0224 - val_mae: 0.1021\n",
      "Epoch 51/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0334 - mae: 0.1332 - val_loss: 0.0213 - val_mae: 0.1011\n",
      "Epoch 52/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0314 - mae: 0.1290 - val_loss: 0.0208 - val_mae: 0.0983\n",
      "Epoch 53/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0321 - mae: 0.1297 - val_loss: 0.0216 - val_mae: 0.0998\n",
      "Epoch 54/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0319 - mae: 0.1283 - val_loss: 0.0206 - val_mae: 0.0980\n",
      "Epoch 55/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0331 - mae: 0.1336 - val_loss: 0.0221 - val_mae: 0.1009\n",
      "Epoch 56/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0315 - mae: 0.1274 - val_loss: 0.0223 - val_mae: 0.0994\n",
      "Epoch 57/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0329 - mae: 0.1332 - val_loss: 0.0204 - val_mae: 0.0967\n",
      "Epoch 58/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0307 - mae: 0.1265 - val_loss: 0.0202 - val_mae: 0.0958\n",
      "Epoch 59/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0323 - mae: 0.1312 - val_loss: 0.0199 - val_mae: 0.0945\n",
      "Epoch 60/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0316 - mae: 0.1290 - val_loss: 0.0202 - val_mae: 0.0970\n",
      "Epoch 61/100\n",
      "1163/1163 [==============================] - 0s 72us/step - loss: 0.0319 - mae: 0.1296 - val_loss: 0.0195 - val_mae: 0.0963\n",
      "Epoch 62/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0327 - mae: 0.1306 - val_loss: 0.0191 - val_mae: 0.0974\n",
      "Epoch 63/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0337 - mae: 0.1341 - val_loss: 0.0201 - val_mae: 0.0977\n",
      "Epoch 64/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0307 - mae: 0.1272 - val_loss: 0.0209 - val_mae: 0.0960\n",
      "Epoch 65/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0317 - mae: 0.1280 - val_loss: 0.0207 - val_mae: 0.0971\n",
      "Epoch 66/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0306 - mae: 0.1262 - val_loss: 0.0208 - val_mae: 0.0963\n",
      "Epoch 67/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0311 - mae: 0.1277 - val_loss: 0.0203 - val_mae: 0.0973\n",
      "Epoch 68/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0304 - mae: 0.1273 - val_loss: 0.0213 - val_mae: 0.0980\n",
      "Epoch 69/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0311 - mae: 0.1270 - val_loss: 0.0210 - val_mae: 0.0964\n",
      "Epoch 70/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0317 - mae: 0.1295 - val_loss: 0.0212 - val_mae: 0.0972\n",
      "Epoch 71/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0303 - mae: 0.1267 - val_loss: 0.0209 - val_mae: 0.0984\n",
      "Epoch 72/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0313 - mae: 0.1246 - val_loss: 0.0224 - val_mae: 0.1013\n",
      "Epoch 73/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0302 - mae: 0.1271 - val_loss: 0.0208 - val_mae: 0.0979\n",
      "Epoch 74/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0316 - mae: 0.1313 - val_loss: 0.0214 - val_mae: 0.0979\n",
      "Epoch 75/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0305 - mae: 0.1254 - val_loss: 0.0227 - val_mae: 0.0986\n",
      "Epoch 76/100\n",
      "1163/1163 [==============================] - 0s 71us/step - loss: 0.0319 - mae: 0.1299 - val_loss: 0.0214 - val_mae: 0.0994\n",
      "Epoch 77/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0317 - mae: 0.1301 - val_loss: 0.0196 - val_mae: 0.0965\n",
      "Epoch 78/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0316 - mae: 0.1297 - val_loss: 0.0207 - val_mae: 0.0978\n",
      "Epoch 79/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0296 - mae: 0.1245 - val_loss: 0.0212 - val_mae: 0.0975\n",
      "Epoch 80/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0327 - mae: 0.1298 - val_loss: 0.0203 - val_mae: 0.0980\n",
      "Epoch 81/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0310 - mae: 0.1263 - val_loss: 0.0218 - val_mae: 0.0992\n",
      "Epoch 82/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0298 - mae: 0.1256 - val_loss: 0.0221 - val_mae: 0.0998\n",
      "Epoch 83/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0308 - mae: 0.1280 - val_loss: 0.0227 - val_mae: 0.1001\n",
      "Epoch 84/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0313 - mae: 0.1298 - val_loss: 0.0225 - val_mae: 0.1010\n",
      "Epoch 85/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0308 - mae: 0.1256 - val_loss: 0.0220 - val_mae: 0.0994\n",
      "Epoch 86/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0312 - mae: 0.1293 - val_loss: 0.0224 - val_mae: 0.1001\n",
      "Epoch 87/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0298 - mae: 0.1266 - val_loss: 0.0232 - val_mae: 0.1014\n",
      "Epoch 88/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0304 - mae: 0.1262 - val_loss: 0.0231 - val_mae: 0.1006\n",
      "Epoch 89/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0297 - mae: 0.1249 - val_loss: 0.0243 - val_mae: 0.1006\n",
      "Epoch 90/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0286 - mae: 0.1224 - val_loss: 0.0232 - val_mae: 0.0994\n",
      "Epoch 91/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0303 - mae: 0.1263 - val_loss: 0.0251 - val_mae: 0.1021\n",
      "Epoch 92/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0313 - mae: 0.1271 - val_loss: 0.0238 - val_mae: 0.1011\n",
      "Epoch 93/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0294 - mae: 0.1240 - val_loss: 0.0237 - val_mae: 0.1009\n",
      "Epoch 94/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0289 - mae: 0.1238 - val_loss: 0.0239 - val_mae: 0.1004\n",
      "Epoch 95/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0301 - mae: 0.1230 - val_loss: 0.0239 - val_mae: 0.1002\n",
      "Epoch 96/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0316 - mae: 0.1280 - val_loss: 0.0223 - val_mae: 0.1003\n",
      "Epoch 97/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0311 - mae: 0.1268 - val_loss: 0.0219 - val_mae: 0.1004\n",
      "Epoch 98/100\n",
      "1163/1163 [==============================] - 0s 72us/step - loss: 0.0299 - mae: 0.1263 - val_loss: 0.0217 - val_mae: 0.0994\n",
      "Epoch 99/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0299 - mae: 0.1247 - val_loss: 0.0228 - val_mae: 0.0998\n",
      "Epoch 100/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0314 - mae: 0.1291 - val_loss: 0.0217 - val_mae: 0.0998\n",
      "processing fold # 1\n",
      "Train on 1163 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "1163/1163 [==============================] - 0s 284us/step - loss: 1.2429 - mae: 0.8736 - val_loss: 0.1007 - val_mae: 0.2584\n",
      "Epoch 2/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.7553 - mae: 0.6798 - val_loss: 0.0661 - val_mae: 0.2117\n",
      "Epoch 3/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.5258 - mae: 0.5619 - val_loss: 0.0512 - val_mae: 0.1892\n",
      "Epoch 4/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.3336 - mae: 0.4551 - val_loss: 0.0420 - val_mae: 0.1684\n",
      "Epoch 5/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.1802 - mae: 0.3358 - val_loss: 0.0370 - val_mae: 0.1488\n",
      "Epoch 6/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0966 - mae: 0.2413 - val_loss: 0.0351 - val_mae: 0.1394\n",
      "Epoch 7/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0659 - mae: 0.1960 - val_loss: 0.0351 - val_mae: 0.1359\n",
      "Epoch 8/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0512 - mae: 0.1702 - val_loss: 0.0330 - val_mae: 0.1391\n",
      "Epoch 9/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0460 - mae: 0.1621 - val_loss: 0.0319 - val_mae: 0.1256\n",
      "Epoch 10/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0456 - mae: 0.1591 - val_loss: 0.0303 - val_mae: 0.1257\n",
      "Epoch 11/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0448 - mae: 0.1560 - val_loss: 0.0284 - val_mae: 0.1257\n",
      "Epoch 12/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0427 - mae: 0.1536 - val_loss: 0.0298 - val_mae: 0.1213\n",
      "Epoch 13/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0422 - mae: 0.1516 - val_loss: 0.0278 - val_mae: 0.1246\n",
      "Epoch 14/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0421 - mae: 0.1533 - val_loss: 0.0286 - val_mae: 0.1216\n",
      "Epoch 15/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0387 - mae: 0.1457 - val_loss: 0.0268 - val_mae: 0.1220\n",
      "Epoch 16/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0404 - mae: 0.1472 - val_loss: 0.0267 - val_mae: 0.1169\n",
      "Epoch 17/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0393 - mae: 0.1474 - val_loss: 0.0259 - val_mae: 0.1178\n",
      "Epoch 18/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0369 - mae: 0.1449 - val_loss: 0.0248 - val_mae: 0.1130\n",
      "Epoch 19/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0361 - mae: 0.1404 - val_loss: 0.0246 - val_mae: 0.1137\n",
      "Epoch 20/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0370 - mae: 0.1424 - val_loss: 0.0243 - val_mae: 0.1114\n",
      "Epoch 21/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0357 - mae: 0.1389 - val_loss: 0.0239 - val_mae: 0.1093\n",
      "Epoch 22/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0356 - mae: 0.1398 - val_loss: 0.0251 - val_mae: 0.1085\n",
      "Epoch 23/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0346 - mae: 0.1352 - val_loss: 0.0264 - val_mae: 0.1088\n",
      "Epoch 24/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0342 - mae: 0.1369 - val_loss: 0.0238 - val_mae: 0.1084\n",
      "Epoch 25/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0335 - mae: 0.1352 - val_loss: 0.0237 - val_mae: 0.1108\n",
      "Epoch 26/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0343 - mae: 0.1370 - val_loss: 0.0232 - val_mae: 0.1082\n",
      "Epoch 27/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0335 - mae: 0.1335 - val_loss: 0.0238 - val_mae: 0.1057\n",
      "Epoch 28/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0333 - mae: 0.1324 - val_loss: 0.0236 - val_mae: 0.1066\n",
      "Epoch 29/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0321 - mae: 0.1317 - val_loss: 0.0240 - val_mae: 0.1059\n",
      "Epoch 30/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0333 - mae: 0.1314 - val_loss: 0.0230 - val_mae: 0.1061\n",
      "Epoch 31/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0315 - mae: 0.1295 - val_loss: 0.0229 - val_mae: 0.1078\n",
      "Epoch 32/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0335 - mae: 0.1351 - val_loss: 0.0227 - val_mae: 0.1077\n",
      "Epoch 33/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0333 - mae: 0.1366 - val_loss: 0.0243 - val_mae: 0.1049\n",
      "Epoch 34/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0334 - mae: 0.1349 - val_loss: 0.0243 - val_mae: 0.1045\n",
      "Epoch 35/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0331 - mae: 0.1344 - val_loss: 0.0231 - val_mae: 0.1040\n",
      "Epoch 36/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0327 - mae: 0.1316 - val_loss: 0.0230 - val_mae: 0.1036\n",
      "Epoch 37/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0311 - mae: 0.1287 - val_loss: 0.0230 - val_mae: 0.1036\n",
      "Epoch 38/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0321 - mae: 0.1313 - val_loss: 0.0226 - val_mae: 0.1032\n",
      "Epoch 39/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0329 - mae: 0.1319 - val_loss: 0.0235 - val_mae: 0.1034\n",
      "Epoch 40/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0328 - mae: 0.1328 - val_loss: 0.0226 - val_mae: 0.1075\n",
      "Epoch 41/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0318 - mae: 0.1320 - val_loss: 0.0222 - val_mae: 0.1039\n",
      "Epoch 42/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0311 - mae: 0.1294 - val_loss: 0.0233 - val_mae: 0.1031\n",
      "Epoch 43/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0312 - mae: 0.1294 - val_loss: 0.0248 - val_mae: 0.1052\n",
      "Epoch 44/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0303 - mae: 0.1255 - val_loss: 0.0233 - val_mae: 0.1030\n",
      "Epoch 45/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0332 - mae: 0.1312 - val_loss: 0.0230 - val_mae: 0.1073\n",
      "Epoch 46/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0316 - mae: 0.1302 - val_loss: 0.0232 - val_mae: 0.1028\n",
      "Epoch 47/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0311 - mae: 0.1273 - val_loss: 0.0226 - val_mae: 0.1034\n",
      "Epoch 48/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0296 - mae: 0.1258 - val_loss: 0.0235 - val_mae: 0.1025\n",
      "Epoch 49/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0312 - mae: 0.1267 - val_loss: 0.0227 - val_mae: 0.1036\n",
      "Epoch 50/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0317 - mae: 0.1316 - val_loss: 0.0239 - val_mae: 0.1040\n",
      "Epoch 51/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0313 - mae: 0.1281 - val_loss: 0.0231 - val_mae: 0.1028\n",
      "Epoch 52/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0315 - mae: 0.1291 - val_loss: 0.0227 - val_mae: 0.1035\n",
      "Epoch 53/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0316 - mae: 0.1296 - val_loss: 0.0230 - val_mae: 0.1027\n",
      "Epoch 54/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0301 - mae: 0.1251 - val_loss: 0.0236 - val_mae: 0.1034\n",
      "Epoch 55/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0314 - mae: 0.1286 - val_loss: 0.0237 - val_mae: 0.1030\n",
      "Epoch 56/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0297 - mae: 0.1259 - val_loss: 0.0237 - val_mae: 0.1033\n",
      "Epoch 57/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0295 - mae: 0.1234 - val_loss: 0.0235 - val_mae: 0.1059\n",
      "Epoch 58/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0301 - mae: 0.1265 - val_loss: 0.0234 - val_mae: 0.1025\n",
      "Epoch 59/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0310 - mae: 0.1264 - val_loss: 0.0225 - val_mae: 0.1026\n",
      "Epoch 60/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0296 - mae: 0.1252 - val_loss: 0.0234 - val_mae: 0.1057\n",
      "Epoch 61/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0328 - mae: 0.1330 - val_loss: 0.0242 - val_mae: 0.1038\n",
      "Epoch 62/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0295 - mae: 0.1238 - val_loss: 0.0242 - val_mae: 0.1044\n",
      "Epoch 63/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0296 - mae: 0.1239 - val_loss: 0.0236 - val_mae: 0.1034\n",
      "Epoch 64/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0303 - mae: 0.1265 - val_loss: 0.0262 - val_mae: 0.1079\n",
      "Epoch 65/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0317 - mae: 0.1289 - val_loss: 0.0241 - val_mae: 0.1038\n",
      "Epoch 66/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0305 - mae: 0.1259 - val_loss: 0.0231 - val_mae: 0.1034\n",
      "Epoch 67/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0292 - mae: 0.1239 - val_loss: 0.0239 - val_mae: 0.1028\n",
      "Epoch 68/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0295 - mae: 0.1227 - val_loss: 0.0236 - val_mae: 0.1057\n",
      "Epoch 69/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0294 - mae: 0.1237 - val_loss: 0.0229 - val_mae: 0.1030\n",
      "Epoch 70/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0301 - mae: 0.1247 - val_loss: 0.0228 - val_mae: 0.1046\n",
      "Epoch 71/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0305 - mae: 0.1269 - val_loss: 0.0237 - val_mae: 0.1024\n",
      "Epoch 72/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0289 - mae: 0.1233 - val_loss: 0.0231 - val_mae: 0.1047\n",
      "Epoch 73/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0294 - mae: 0.1265 - val_loss: 0.0238 - val_mae: 0.1029\n",
      "Epoch 74/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0304 - mae: 0.1259 - val_loss: 0.0233 - val_mae: 0.1035\n",
      "Epoch 75/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0286 - mae: 0.1224 - val_loss: 0.0239 - val_mae: 0.1031\n",
      "Epoch 76/100\n",
      "1163/1163 [==============================] - 0s 56us/step - loss: 0.0298 - mae: 0.1242 - val_loss: 0.0238 - val_mae: 0.1038\n",
      "Epoch 77/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0297 - mae: 0.1226 - val_loss: 0.0240 - val_mae: 0.1034\n",
      "Epoch 78/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0283 - mae: 0.1220 - val_loss: 0.0242 - val_mae: 0.1042\n",
      "Epoch 79/100\n",
      "1163/1163 [==============================] - 0s 56us/step - loss: 0.0276 - mae: 0.1198 - val_loss: 0.0239 - val_mae: 0.1028\n",
      "Epoch 80/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0277 - mae: 0.1201 - val_loss: 0.0254 - val_mae: 0.1047\n",
      "Epoch 81/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0298 - mae: 0.1235 - val_loss: 0.0242 - val_mae: 0.1046\n",
      "Epoch 82/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0312 - mae: 0.1285 - val_loss: 0.0239 - val_mae: 0.1037\n",
      "Epoch 83/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0291 - mae: 0.1239 - val_loss: 0.0240 - val_mae: 0.1053\n",
      "Epoch 84/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0284 - mae: 0.1219 - val_loss: 0.0247 - val_mae: 0.1035\n",
      "Epoch 85/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0295 - mae: 0.1227 - val_loss: 0.0232 - val_mae: 0.1025\n",
      "Epoch 86/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0297 - mae: 0.1237 - val_loss: 0.0232 - val_mae: 0.1027\n",
      "Epoch 87/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0288 - mae: 0.1240 - val_loss: 0.0247 - val_mae: 0.1042\n",
      "Epoch 88/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0289 - mae: 0.1238 - val_loss: 0.0240 - val_mae: 0.1056\n",
      "Epoch 89/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0285 - mae: 0.1226 - val_loss: 0.0245 - val_mae: 0.1062\n",
      "Epoch 90/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0296 - mae: 0.1233 - val_loss: 0.0267 - val_mae: 0.1104\n",
      "Epoch 91/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0293 - mae: 0.1259 - val_loss: 0.0250 - val_mae: 0.1045\n",
      "Epoch 92/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0288 - mae: 0.1239 - val_loss: 0.0244 - val_mae: 0.1042\n",
      "Epoch 93/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0291 - mae: 0.1223 - val_loss: 0.0250 - val_mae: 0.1045\n",
      "Epoch 94/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0290 - mae: 0.1234 - val_loss: 0.0262 - val_mae: 0.1056\n",
      "Epoch 95/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0288 - mae: 0.1226 - val_loss: 0.0260 - val_mae: 0.1053\n",
      "Epoch 96/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0291 - mae: 0.1236 - val_loss: 0.0243 - val_mae: 0.1047\n",
      "Epoch 97/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0291 - mae: 0.1219 - val_loss: 0.0235 - val_mae: 0.1042\n",
      "Epoch 98/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0286 - mae: 0.1248 - val_loss: 0.0251 - val_mae: 0.1047\n",
      "Epoch 99/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0290 - mae: 0.1213 - val_loss: 0.0257 - val_mae: 0.1067\n",
      "Epoch 100/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0299 - mae: 0.1257 - val_loss: 0.0236 - val_mae: 0.1032\n",
      "processing fold # 2\n",
      "Train on 1163 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "1163/1163 [==============================] - 0s 279us/step - loss: 1.7448 - mae: 1.0401 - val_loss: 0.0966 - val_mae: 0.2259\n",
      "Epoch 2/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 1.1192 - mae: 0.8380 - val_loss: 0.0554 - val_mae: 0.1900\n",
      "Epoch 3/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.7675 - mae: 0.6891 - val_loss: 0.0343 - val_mae: 0.1448\n",
      "Epoch 4/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.5062 - mae: 0.5663 - val_loss: 0.0282 - val_mae: 0.1246\n",
      "Epoch 5/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.2927 - mae: 0.4240 - val_loss: 0.0272 - val_mae: 0.1266\n",
      "Epoch 6/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.1587 - mae: 0.3116 - val_loss: 0.0264 - val_mae: 0.1274\n",
      "Epoch 7/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0939 - mae: 0.2366 - val_loss: 0.0264 - val_mae: 0.1237\n",
      "Epoch 8/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0663 - mae: 0.1938 - val_loss: 0.0274 - val_mae: 0.1277\n",
      "Epoch 9/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0511 - mae: 0.1670 - val_loss: 0.0271 - val_mae: 0.1295\n",
      "Epoch 10/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0527 - mae: 0.1685 - val_loss: 0.0266 - val_mae: 0.1286\n",
      "Epoch 11/100\n",
      "1163/1163 [==============================] - ETA: 0s - loss: 0.0482 - mae: 0.162 - 0s 69us/step - loss: 0.0462 - mae: 0.1580 - val_loss: 0.0258 - val_mae: 0.1324\n",
      "Epoch 12/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0455 - mae: 0.1586 - val_loss: 0.0235 - val_mae: 0.1202\n",
      "Epoch 13/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0423 - mae: 0.1522 - val_loss: 0.0227 - val_mae: 0.1162\n",
      "Epoch 14/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0439 - mae: 0.1548 - val_loss: 0.0216 - val_mae: 0.1144\n",
      "Epoch 15/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0421 - mae: 0.1506 - val_loss: 0.0215 - val_mae: 0.1122\n",
      "Epoch 16/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0417 - mae: 0.1501 - val_loss: 0.0207 - val_mae: 0.1107\n",
      "Epoch 17/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0385 - mae: 0.1446 - val_loss: 0.0201 - val_mae: 0.1097\n",
      "Epoch 18/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0382 - mae: 0.1451 - val_loss: 0.0199 - val_mae: 0.1076\n",
      "Epoch 19/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0377 - mae: 0.1451 - val_loss: 0.0200 - val_mae: 0.1048\n",
      "Epoch 20/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0391 - mae: 0.1427 - val_loss: 0.0207 - val_mae: 0.1115\n",
      "Epoch 21/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0375 - mae: 0.1438 - val_loss: 0.0195 - val_mae: 0.1038\n",
      "Epoch 22/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0371 - mae: 0.1400 - val_loss: 0.0195 - val_mae: 0.1075\n",
      "Epoch 23/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0367 - mae: 0.1396 - val_loss: 0.0203 - val_mae: 0.1098\n",
      "Epoch 24/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0380 - mae: 0.1441 - val_loss: 0.0196 - val_mae: 0.1071\n",
      "Epoch 25/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0372 - mae: 0.1420 - val_loss: 0.0196 - val_mae: 0.1072\n",
      "Epoch 26/100\n",
      "1163/1163 [==============================] - 0s 73us/step - loss: 0.0369 - mae: 0.1404 - val_loss: 0.0192 - val_mae: 0.1035\n",
      "Epoch 27/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0357 - mae: 0.1373 - val_loss: 0.0197 - val_mae: 0.1055\n",
      "Epoch 28/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0339 - mae: 0.1333 - val_loss: 0.0191 - val_mae: 0.1016\n",
      "Epoch 29/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0351 - mae: 0.1354 - val_loss: 0.0195 - val_mae: 0.1066\n",
      "Epoch 30/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0343 - mae: 0.1395 - val_loss: 0.0184 - val_mae: 0.0987\n",
      "Epoch 31/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0351 - mae: 0.1357 - val_loss: 0.0204 - val_mae: 0.1098\n",
      "Epoch 32/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0328 - mae: 0.1333 - val_loss: 0.0193 - val_mae: 0.1018\n",
      "Epoch 33/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0338 - mae: 0.1349 - val_loss: 0.0194 - val_mae: 0.0996\n",
      "Epoch 34/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0345 - mae: 0.1338 - val_loss: 0.0198 - val_mae: 0.1018\n",
      "Epoch 35/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0343 - mae: 0.1345 - val_loss: 0.0239 - val_mae: 0.1108\n",
      "Epoch 36/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0362 - mae: 0.1404 - val_loss: 0.0193 - val_mae: 0.1018\n",
      "Epoch 37/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0334 - mae: 0.1353 - val_loss: 0.0206 - val_mae: 0.1032\n",
      "Epoch 38/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0318 - mae: 0.1285 - val_loss: 0.0224 - val_mae: 0.1016\n",
      "Epoch 39/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0346 - mae: 0.1330 - val_loss: 0.0196 - val_mae: 0.1001\n",
      "Epoch 40/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0331 - mae: 0.1325 - val_loss: 0.0216 - val_mae: 0.1048\n",
      "Epoch 41/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0324 - mae: 0.1318 - val_loss: 0.0213 - val_mae: 0.1028\n",
      "Epoch 42/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0332 - mae: 0.1341 - val_loss: 0.0221 - val_mae: 0.1022\n",
      "Epoch 43/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0324 - mae: 0.1316 - val_loss: 0.0211 - val_mae: 0.1011\n",
      "Epoch 44/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0345 - mae: 0.1335 - val_loss: 0.0191 - val_mae: 0.0995\n",
      "Epoch 45/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0324 - mae: 0.1300 - val_loss: 0.0217 - val_mae: 0.1035\n",
      "Epoch 46/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0303 - mae: 0.1280 - val_loss: 0.0251 - val_mae: 0.1042\n",
      "Epoch 47/100\n",
      "1163/1163 [==============================] - 0s 73us/step - loss: 0.0307 - mae: 0.1284 - val_loss: 0.0243 - val_mae: 0.1027\n",
      "Epoch 48/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0312 - mae: 0.1282 - val_loss: 0.0220 - val_mae: 0.0992\n",
      "Epoch 49/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0310 - mae: 0.1264 - val_loss: 0.0219 - val_mae: 0.1018\n",
      "Epoch 50/100\n",
      "1163/1163 [==============================] - 0s 71us/step - loss: 0.0323 - mae: 0.1298 - val_loss: 0.0216 - val_mae: 0.1030\n",
      "Epoch 51/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0317 - mae: 0.1303 - val_loss: 0.0247 - val_mae: 0.1017\n",
      "Epoch 52/100\n",
      "1163/1163 [==============================] - 0s 75us/step - loss: 0.0325 - mae: 0.1308 - val_loss: 0.0240 - val_mae: 0.0994\n",
      "Epoch 53/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0333 - mae: 0.1321 - val_loss: 0.0213 - val_mae: 0.1001\n",
      "Epoch 54/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0320 - mae: 0.1315 - val_loss: 0.0215 - val_mae: 0.0994\n",
      "Epoch 55/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0319 - mae: 0.1305 - val_loss: 0.0234 - val_mae: 0.1000\n",
      "Epoch 56/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0300 - mae: 0.1259 - val_loss: 0.0241 - val_mae: 0.1002\n",
      "Epoch 57/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0314 - mae: 0.1289 - val_loss: 0.0241 - val_mae: 0.1014\n",
      "Epoch 58/100\n",
      "1163/1163 [==============================] - 0s 75us/step - loss: 0.0318 - mae: 0.1284 - val_loss: 0.0242 - val_mae: 0.1033\n",
      "Epoch 59/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0309 - mae: 0.1271 - val_loss: 0.0250 - val_mae: 0.1034\n",
      "Epoch 60/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0299 - mae: 0.1255 - val_loss: 0.0241 - val_mae: 0.1019\n",
      "Epoch 61/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0320 - mae: 0.1311 - val_loss: 0.0205 - val_mae: 0.1011\n",
      "Epoch 62/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0309 - mae: 0.1291 - val_loss: 0.0251 - val_mae: 0.1029\n",
      "Epoch 63/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0327 - mae: 0.1307 - val_loss: 0.0225 - val_mae: 0.1015\n",
      "Epoch 64/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0316 - mae: 0.1285 - val_loss: 0.0198 - val_mae: 0.0983\n",
      "Epoch 65/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0295 - mae: 0.1250 - val_loss: 0.0212 - val_mae: 0.0991\n",
      "Epoch 66/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0309 - mae: 0.1261 - val_loss: 0.0304 - val_mae: 0.1115\n",
      "Epoch 67/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0335 - mae: 0.1321 - val_loss: 0.0208 - val_mae: 0.1064\n",
      "Epoch 68/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0320 - mae: 0.1284 - val_loss: 0.0211 - val_mae: 0.1006\n",
      "Epoch 69/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0315 - mae: 0.1280 - val_loss: 0.0296 - val_mae: 0.1084\n",
      "Epoch 70/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0307 - mae: 0.1285 - val_loss: 0.0226 - val_mae: 0.0997\n",
      "Epoch 71/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0302 - mae: 0.1257 - val_loss: 0.0246 - val_mae: 0.1061\n",
      "Epoch 72/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0309 - mae: 0.1287 - val_loss: 0.0216 - val_mae: 0.0986\n",
      "Epoch 73/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0300 - mae: 0.1247 - val_loss: 0.0266 - val_mae: 0.1050\n",
      "Epoch 74/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0307 - mae: 0.1271 - val_loss: 0.0217 - val_mae: 0.1026\n",
      "Epoch 75/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0306 - mae: 0.1276 - val_loss: 0.0259 - val_mae: 0.1066\n",
      "Epoch 76/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0307 - mae: 0.1265 - val_loss: 0.0305 - val_mae: 0.1082\n",
      "Epoch 77/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0310 - mae: 0.1266 - val_loss: 0.0244 - val_mae: 0.1016\n",
      "Epoch 78/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0316 - mae: 0.1288 - val_loss: 0.0221 - val_mae: 0.0996\n",
      "Epoch 79/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0288 - mae: 0.1208 - val_loss: 0.0261 - val_mae: 0.1030\n",
      "Epoch 80/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0292 - mae: 0.1250 - val_loss: 0.0238 - val_mae: 0.1018\n",
      "Epoch 81/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0298 - mae: 0.1243 - val_loss: 0.0237 - val_mae: 0.1014\n",
      "Epoch 82/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0311 - mae: 0.1272 - val_loss: 0.0221 - val_mae: 0.1030\n",
      "Epoch 83/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0310 - mae: 0.1281 - val_loss: 0.0253 - val_mae: 0.1089\n",
      "Epoch 84/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0304 - mae: 0.1259 - val_loss: 0.0256 - val_mae: 0.1022\n",
      "Epoch 85/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0303 - mae: 0.1242 - val_loss: 0.0298 - val_mae: 0.1072\n",
      "Epoch 86/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0298 - mae: 0.1255 - val_loss: 0.0312 - val_mae: 0.1068\n",
      "Epoch 87/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0287 - mae: 0.1214 - val_loss: 0.0339 - val_mae: 0.1096\n",
      "Epoch 88/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0310 - mae: 0.1255 - val_loss: 0.0310 - val_mae: 0.1113\n",
      "Epoch 89/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0304 - mae: 0.1278 - val_loss: 0.0281 - val_mae: 0.1055\n",
      "Epoch 90/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0305 - mae: 0.1280 - val_loss: 0.0278 - val_mae: 0.1038\n",
      "Epoch 91/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0312 - mae: 0.1261 - val_loss: 0.0293 - val_mae: 0.1080\n",
      "Epoch 92/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0304 - mae: 0.1266 - val_loss: 0.0311 - val_mae: 0.1090\n",
      "Epoch 93/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0307 - mae: 0.1255 - val_loss: 0.0283 - val_mae: 0.1066\n",
      "Epoch 94/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0311 - mae: 0.1275 - val_loss: 0.0296 - val_mae: 0.1082\n",
      "Epoch 95/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0286 - mae: 0.1242 - val_loss: 0.0283 - val_mae: 0.1069\n",
      "Epoch 96/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0304 - mae: 0.1251 - val_loss: 0.0285 - val_mae: 0.1065\n",
      "Epoch 97/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0290 - mae: 0.1243 - val_loss: 0.0375 - val_mae: 0.1136\n",
      "Epoch 98/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0306 - mae: 0.1272 - val_loss: 0.0360 - val_mae: 0.1144\n",
      "Epoch 99/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0293 - mae: 0.1242 - val_loss: 0.0366 - val_mae: 0.1168\n",
      "Epoch 100/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0287 - mae: 0.1231 - val_loss: 0.0261 - val_mae: 0.1052\n",
      "processing fold # 3\n",
      "Train on 1163 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "1163/1163 [==============================] - 0s 298us/step - loss: 1.6858 - mae: 1.0289 - val_loss: 0.1174 - val_mae: 0.2699\n",
      "Epoch 2/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 1.1490 - mae: 0.8273 - val_loss: 0.0766 - val_mae: 0.2124\n",
      "Epoch 3/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.7666 - mae: 0.6855 - val_loss: 0.0541 - val_mae: 0.1740\n",
      "Epoch 4/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.5420 - mae: 0.5772 - val_loss: 0.0481 - val_mae: 0.1562\n",
      "Epoch 5/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.2951 - mae: 0.4289 - val_loss: 0.0440 - val_mae: 0.1411\n",
      "Epoch 6/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.1767 - mae: 0.3265 - val_loss: 0.0394 - val_mae: 0.1339\n",
      "Epoch 7/100\n",
      "1163/1163 [==============================] - 0s 78us/step - loss: 0.0954 - mae: 0.2396 - val_loss: 0.0381 - val_mae: 0.1329\n",
      "Epoch 8/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0658 - mae: 0.1921 - val_loss: 0.0364 - val_mae: 0.1358\n",
      "Epoch 9/100\n",
      "1163/1163 [==============================] - 0s 74us/step - loss: 0.0544 - mae: 0.1715 - val_loss: 0.0325 - val_mae: 0.1359\n",
      "Epoch 10/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0488 - mae: 0.1630 - val_loss: 0.0320 - val_mae: 0.1324\n",
      "Epoch 11/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0462 - mae: 0.1598 - val_loss: 0.0286 - val_mae: 0.1295\n",
      "Epoch 12/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0455 - mae: 0.1597 - val_loss: 0.0286 - val_mae: 0.1316\n",
      "Epoch 13/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0452 - mae: 0.1567 - val_loss: 0.0259 - val_mae: 0.1241\n",
      "Epoch 14/100\n",
      "1163/1163 [==============================] - 0s 75us/step - loss: 0.0448 - mae: 0.1565 - val_loss: 0.0254 - val_mae: 0.1212\n",
      "Epoch 15/100\n",
      "1163/1163 [==============================] - 0s 73us/step - loss: 0.0415 - mae: 0.1488 - val_loss: 0.0228 - val_mae: 0.1136\n",
      "Epoch 16/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0411 - mae: 0.1508 - val_loss: 0.0247 - val_mae: 0.1108\n",
      "Epoch 17/100\n",
      "1163/1163 [==============================] - 0s 72us/step - loss: 0.0391 - mae: 0.1443 - val_loss: 0.0217 - val_mae: 0.1098\n",
      "Epoch 18/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0410 - mae: 0.1494 - val_loss: 0.0207 - val_mae: 0.1102\n",
      "Epoch 19/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0370 - mae: 0.1406 - val_loss: 0.0193 - val_mae: 0.1055\n",
      "Epoch 20/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0392 - mae: 0.1446 - val_loss: 0.0203 - val_mae: 0.1055\n",
      "Epoch 21/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0358 - mae: 0.1403 - val_loss: 0.0186 - val_mae: 0.1028\n",
      "Epoch 22/100\n",
      "1163/1163 [==============================] - 0s 71us/step - loss: 0.0391 - mae: 0.1438 - val_loss: 0.0191 - val_mae: 0.1011\n",
      "Epoch 23/100\n",
      "1163/1163 [==============================] - 0s 73us/step - loss: 0.0340 - mae: 0.1336 - val_loss: 0.0186 - val_mae: 0.0974\n",
      "Epoch 24/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0368 - mae: 0.1386 - val_loss: 0.0186 - val_mae: 0.1069\n",
      "Epoch 25/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0350 - mae: 0.1366 - val_loss: 0.0187 - val_mae: 0.0944\n",
      "Epoch 26/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0349 - mae: 0.1366 - val_loss: 0.0175 - val_mae: 0.1009\n",
      "Epoch 27/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0353 - mae: 0.1367 - val_loss: 0.0173 - val_mae: 0.0937\n",
      "Epoch 28/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0325 - mae: 0.1327 - val_loss: 0.0173 - val_mae: 0.0978\n",
      "Epoch 29/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0371 - mae: 0.1407 - val_loss: 0.0175 - val_mae: 0.0927\n",
      "Epoch 30/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0345 - mae: 0.1359 - val_loss: 0.0176 - val_mae: 0.0928\n",
      "Epoch 31/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0329 - mae: 0.1339 - val_loss: 0.0172 - val_mae: 0.0908\n",
      "Epoch 32/100\n",
      "1163/1163 [==============================] - 0s 70us/step - loss: 0.0331 - mae: 0.1315 - val_loss: 0.0177 - val_mae: 0.0955\n",
      "Epoch 33/100\n",
      "1163/1163 [==============================] - 0s 69us/step - loss: 0.0336 - mae: 0.1327 - val_loss: 0.0178 - val_mae: 0.0929\n",
      "Epoch 34/100\n",
      "1163/1163 [==============================] - 0s 71us/step - loss: 0.0324 - mae: 0.1298 - val_loss: 0.0174 - val_mae: 0.0949\n",
      "Epoch 35/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0333 - mae: 0.1335 - val_loss: 0.0179 - val_mae: 0.0933\n",
      "Epoch 36/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0345 - mae: 0.1323 - val_loss: 0.0174 - val_mae: 0.0936\n",
      "Epoch 37/100\n",
      "1163/1163 [==============================] - 0s 71us/step - loss: 0.0358 - mae: 0.1396 - val_loss: 0.0179 - val_mae: 0.1006\n",
      "Epoch 38/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0326 - mae: 0.1308 - val_loss: 0.0176 - val_mae: 0.0962\n",
      "Epoch 39/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0317 - mae: 0.1299 - val_loss: 0.0178 - val_mae: 0.0928\n",
      "Epoch 40/100\n",
      "1163/1163 [==============================] - 0s 67us/step - loss: 0.0320 - mae: 0.1326 - val_loss: 0.0176 - val_mae: 0.0944\n",
      "Epoch 41/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0319 - mae: 0.1305 - val_loss: 0.0175 - val_mae: 0.0923\n",
      "Epoch 42/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0326 - mae: 0.1313 - val_loss: 0.0189 - val_mae: 0.0970\n",
      "Epoch 43/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0309 - mae: 0.1273 - val_loss: 0.0189 - val_mae: 0.0936\n",
      "Epoch 44/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0303 - mae: 0.1255 - val_loss: 0.0187 - val_mae: 0.0966\n",
      "Epoch 45/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0327 - mae: 0.1309 - val_loss: 0.0189 - val_mae: 0.0951\n",
      "Epoch 46/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0329 - mae: 0.1303 - val_loss: 0.0188 - val_mae: 0.0934\n",
      "Epoch 47/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0320 - mae: 0.1279 - val_loss: 0.0180 - val_mae: 0.0934\n",
      "Epoch 48/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0297 - mae: 0.1235 - val_loss: 0.0185 - val_mae: 0.0942\n",
      "Epoch 49/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0326 - mae: 0.1314 - val_loss: 0.0186 - val_mae: 0.0930\n",
      "Epoch 50/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0308 - mae: 0.1294 - val_loss: 0.0191 - val_mae: 0.0917\n",
      "Epoch 51/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0323 - mae: 0.1284 - val_loss: 0.0186 - val_mae: 0.0911\n",
      "Epoch 52/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0317 - mae: 0.1270 - val_loss: 0.0178 - val_mae: 0.0937\n",
      "Epoch 53/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0319 - mae: 0.1316 - val_loss: 0.0186 - val_mae: 0.0985\n",
      "Epoch 54/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0317 - mae: 0.1296 - val_loss: 0.0185 - val_mae: 0.0927\n",
      "Epoch 55/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0317 - mae: 0.1293 - val_loss: 0.0181 - val_mae: 0.0966\n",
      "Epoch 56/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0326 - mae: 0.1330 - val_loss: 0.0183 - val_mae: 0.0946\n",
      "Epoch 57/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0319 - mae: 0.1293 - val_loss: 0.0186 - val_mae: 0.0957\n",
      "Epoch 58/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0320 - mae: 0.1311 - val_loss: 0.0182 - val_mae: 0.0906\n",
      "Epoch 59/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0318 - mae: 0.1292 - val_loss: 0.0177 - val_mae: 0.0937\n",
      "Epoch 60/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0303 - mae: 0.1282 - val_loss: 0.0185 - val_mae: 0.0897\n",
      "Epoch 61/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0322 - mae: 0.1278 - val_loss: 0.0182 - val_mae: 0.0917\n",
      "Epoch 62/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0314 - mae: 0.1281 - val_loss: 0.0175 - val_mae: 0.0889\n",
      "Epoch 63/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0310 - mae: 0.1273 - val_loss: 0.0194 - val_mae: 0.0931\n",
      "Epoch 64/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0309 - mae: 0.1264 - val_loss: 0.0178 - val_mae: 0.0898\n",
      "Epoch 65/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0314 - mae: 0.1267 - val_loss: 0.0184 - val_mae: 0.0907\n",
      "Epoch 66/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0326 - mae: 0.1294 - val_loss: 0.0185 - val_mae: 0.0919\n",
      "Epoch 67/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0323 - mae: 0.1307 - val_loss: 0.0189 - val_mae: 0.0919\n",
      "Epoch 68/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0296 - mae: 0.1242 - val_loss: 0.0223 - val_mae: 0.0995\n",
      "Epoch 69/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0304 - mae: 0.1278 - val_loss: 0.0198 - val_mae: 0.0902\n",
      "Epoch 70/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0321 - mae: 0.1284 - val_loss: 0.0178 - val_mae: 0.0926\n",
      "Epoch 71/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0298 - mae: 0.1245 - val_loss: 0.0181 - val_mae: 0.0941\n",
      "Epoch 72/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0297 - mae: 0.1236 - val_loss: 0.0189 - val_mae: 0.0983\n",
      "Epoch 73/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0302 - mae: 0.1290 - val_loss: 0.0185 - val_mae: 0.0919\n",
      "Epoch 74/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0302 - mae: 0.1243 - val_loss: 0.0177 - val_mae: 0.0934\n",
      "Epoch 75/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0289 - mae: 0.1247 - val_loss: 0.0183 - val_mae: 0.0902\n",
      "Epoch 76/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0306 - mae: 0.1270 - val_loss: 0.0177 - val_mae: 0.0905\n",
      "Epoch 77/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0292 - mae: 0.1252 - val_loss: 0.0210 - val_mae: 0.0943\n",
      "Epoch 78/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0298 - mae: 0.1230 - val_loss: 0.0218 - val_mae: 0.0990\n",
      "Epoch 79/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0304 - mae: 0.1264 - val_loss: 0.0197 - val_mae: 0.0962\n",
      "Epoch 80/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0310 - mae: 0.1308 - val_loss: 0.0190 - val_mae: 0.0941\n",
      "Epoch 81/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0313 - mae: 0.1266 - val_loss: 0.0184 - val_mae: 0.0908\n",
      "Epoch 82/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0292 - mae: 0.1238 - val_loss: 0.0200 - val_mae: 0.0958\n",
      "Epoch 83/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0316 - mae: 0.1291 - val_loss: 0.0190 - val_mae: 0.0912\n",
      "Epoch 84/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0306 - mae: 0.1265 - val_loss: 0.0204 - val_mae: 0.0941\n",
      "Epoch 85/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0302 - mae: 0.1250 - val_loss: 0.0217 - val_mae: 0.0967\n",
      "Epoch 86/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0288 - mae: 0.1247 - val_loss: 0.0207 - val_mae: 0.0942\n",
      "Epoch 87/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0312 - mae: 0.1280 - val_loss: 0.0186 - val_mae: 0.0906\n",
      "Epoch 88/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0297 - mae: 0.1240 - val_loss: 0.0213 - val_mae: 0.0957\n",
      "Epoch 89/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0318 - mae: 0.1295 - val_loss: 0.0184 - val_mae: 0.0903\n",
      "Epoch 90/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0301 - mae: 0.1254 - val_loss: 0.0203 - val_mae: 0.0966\n",
      "Epoch 91/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0264 - mae: 0.1169 - val_loss: 0.0267 - val_mae: 0.1036\n",
      "Epoch 92/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0281 - mae: 0.1193 - val_loss: 0.0217 - val_mae: 0.0941\n",
      "Epoch 93/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0311 - mae: 0.1258 - val_loss: 0.0211 - val_mae: 0.0965\n",
      "Epoch 94/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0291 - mae: 0.1203 - val_loss: 0.0199 - val_mae: 0.0932\n",
      "Epoch 95/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0299 - mae: 0.1243 - val_loss: 0.0205 - val_mae: 0.0964\n",
      "Epoch 96/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0293 - mae: 0.1233 - val_loss: 0.0191 - val_mae: 0.0974\n",
      "Epoch 97/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0277 - mae: 0.1211 - val_loss: 0.0255 - val_mae: 0.1059\n",
      "Epoch 98/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0279 - mae: 0.1204 - val_loss: 0.0210 - val_mae: 0.0984\n",
      "Epoch 99/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0302 - mae: 0.1248 - val_loss: 0.0192 - val_mae: 0.0924\n",
      "Epoch 100/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0294 - mae: 0.1235 - val_loss: 0.0185 - val_mae: 0.0983\n",
      "processing fold # 4\n",
      "Train on 1163 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "1163/1163 [==============================] - 0s 312us/step - loss: 1.1030 - mae: 0.8263 - val_loss: 0.0715 - val_mae: 0.2020\n",
      "Epoch 2/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.7751 - mae: 0.6837 - val_loss: 0.0394 - val_mae: 0.1471\n",
      "Epoch 3/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.4451 - mae: 0.5211 - val_loss: 0.0319 - val_mae: 0.1270\n",
      "Epoch 4/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.2708 - mae: 0.4077 - val_loss: 0.0296 - val_mae: 0.1180\n",
      "Epoch 5/100\n",
      "1163/1163 [==============================] - 0s 73us/step - loss: 0.1637 - mae: 0.3148 - val_loss: 0.0312 - val_mae: 0.1233\n",
      "Epoch 6/100\n",
      "1163/1163 [==============================] - 0s 75us/step - loss: 0.0959 - mae: 0.2437 - val_loss: 0.0327 - val_mae: 0.1277\n",
      "Epoch 7/100\n",
      "1163/1163 [==============================] - 0s 74us/step - loss: 0.0619 - mae: 0.1889 - val_loss: 0.0360 - val_mae: 0.1262\n",
      "Epoch 8/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0528 - mae: 0.1672 - val_loss: 0.0339 - val_mae: 0.1324\n",
      "Epoch 9/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0482 - mae: 0.1657 - val_loss: 0.0296 - val_mae: 0.1268\n",
      "Epoch 10/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0453 - mae: 0.1587 - val_loss: 0.0270 - val_mae: 0.1249\n",
      "Epoch 11/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0462 - mae: 0.1624 - val_loss: 0.0270 - val_mae: 0.1147\n",
      "Epoch 12/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0426 - mae: 0.1530 - val_loss: 0.0258 - val_mae: 0.1083\n",
      "Epoch 13/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0420 - mae: 0.1527 - val_loss: 0.0251 - val_mae: 0.1063\n",
      "Epoch 14/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0410 - mae: 0.1498 - val_loss: 0.0221 - val_mae: 0.1092\n",
      "Epoch 15/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0400 - mae: 0.1474 - val_loss: 0.0214 - val_mae: 0.1051\n",
      "Epoch 16/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0382 - mae: 0.1459 - val_loss: 0.0210 - val_mae: 0.0969\n",
      "Epoch 17/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0390 - mae: 0.1466 - val_loss: 0.0194 - val_mae: 0.0968\n",
      "Epoch 18/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0392 - mae: 0.1473 - val_loss: 0.0203 - val_mae: 0.0976\n",
      "Epoch 19/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0388 - mae: 0.1468 - val_loss: 0.0203 - val_mae: 0.0952\n",
      "Epoch 20/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0357 - mae: 0.1402 - val_loss: 0.0188 - val_mae: 0.0927\n",
      "Epoch 21/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0355 - mae: 0.1400 - val_loss: 0.0186 - val_mae: 0.0913\n",
      "Epoch 22/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0352 - mae: 0.1394 - val_loss: 0.0186 - val_mae: 0.0921\n",
      "Epoch 23/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0332 - mae: 0.1323 - val_loss: 0.0193 - val_mae: 0.0951\n",
      "Epoch 24/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0335 - mae: 0.1360 - val_loss: 0.0181 - val_mae: 0.0897\n",
      "Epoch 25/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0343 - mae: 0.1361 - val_loss: 0.0182 - val_mae: 0.0928\n",
      "Epoch 26/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0354 - mae: 0.1373 - val_loss: 0.0222 - val_mae: 0.1024\n",
      "Epoch 27/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0341 - mae: 0.1371 - val_loss: 0.0204 - val_mae: 0.0962\n",
      "Epoch 28/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0332 - mae: 0.1336 - val_loss: 0.0220 - val_mae: 0.0994\n",
      "Epoch 29/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0340 - mae: 0.1363 - val_loss: 0.0193 - val_mae: 0.0948\n",
      "Epoch 30/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0336 - mae: 0.1362 - val_loss: 0.0199 - val_mae: 0.0961\n",
      "Epoch 31/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0326 - mae: 0.1315 - val_loss: 0.0233 - val_mae: 0.0997\n",
      "Epoch 32/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0332 - mae: 0.1355 - val_loss: 0.0191 - val_mae: 0.0932\n",
      "Epoch 33/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0330 - mae: 0.1320 - val_loss: 0.0261 - val_mae: 0.1063\n",
      "Epoch 34/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0337 - mae: 0.1345 - val_loss: 0.0215 - val_mae: 0.0979\n",
      "Epoch 35/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0308 - mae: 0.1299 - val_loss: 0.0238 - val_mae: 0.1013\n",
      "Epoch 36/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0327 - mae: 0.1333 - val_loss: 0.0218 - val_mae: 0.0977\n",
      "Epoch 37/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0306 - mae: 0.1270 - val_loss: 0.0227 - val_mae: 0.0997\n",
      "Epoch 38/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0331 - mae: 0.1314 - val_loss: 0.0228 - val_mae: 0.0995\n",
      "Epoch 39/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0318 - mae: 0.1307 - val_loss: 0.0252 - val_mae: 0.0999\n",
      "Epoch 40/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0314 - mae: 0.1302 - val_loss: 0.0227 - val_mae: 0.0986\n",
      "Epoch 41/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0325 - mae: 0.1310 - val_loss: 0.0247 - val_mae: 0.1007\n",
      "Epoch 42/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0318 - mae: 0.1298 - val_loss: 0.0218 - val_mae: 0.0967\n",
      "Epoch 43/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0310 - mae: 0.1287 - val_loss: 0.0246 - val_mae: 0.0989\n",
      "Epoch 44/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0287 - mae: 0.1227 - val_loss: 0.0257 - val_mae: 0.1001\n",
      "Epoch 45/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0316 - mae: 0.1300 - val_loss: 0.0308 - val_mae: 0.1065\n",
      "Epoch 46/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0317 - mae: 0.1294 - val_loss: 0.0273 - val_mae: 0.1032\n",
      "Epoch 47/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0310 - mae: 0.1275 - val_loss: 0.0276 - val_mae: 0.1034\n",
      "Epoch 48/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0318 - mae: 0.1303 - val_loss: 0.0249 - val_mae: 0.1006\n",
      "Epoch 49/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0323 - mae: 0.1321 - val_loss: 0.0236 - val_mae: 0.1006\n",
      "Epoch 50/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0313 - mae: 0.1285 - val_loss: 0.0275 - val_mae: 0.1031\n",
      "Epoch 51/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0303 - mae: 0.1269 - val_loss: 0.0300 - val_mae: 0.1028\n",
      "Epoch 52/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0295 - mae: 0.1250 - val_loss: 0.0246 - val_mae: 0.0993\n",
      "Epoch 53/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0309 - mae: 0.1284 - val_loss: 0.0258 - val_mae: 0.0999\n",
      "Epoch 54/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0311 - mae: 0.1278 - val_loss: 0.0216 - val_mae: 0.0985\n",
      "Epoch 55/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0303 - mae: 0.1242 - val_loss: 0.0228 - val_mae: 0.0984\n",
      "Epoch 56/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0304 - mae: 0.1284 - val_loss: 0.0264 - val_mae: 0.1022\n",
      "Epoch 57/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0283 - mae: 0.1225 - val_loss: 0.0297 - val_mae: 0.1030\n",
      "Epoch 58/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0290 - mae: 0.1238 - val_loss: 0.0309 - val_mae: 0.1049\n",
      "Epoch 59/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0286 - mae: 0.1210 - val_loss: 0.0286 - val_mae: 0.1022\n",
      "Epoch 60/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0304 - mae: 0.1265 - val_loss: 0.0335 - val_mae: 0.1084\n",
      "Epoch 61/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0297 - mae: 0.1231 - val_loss: 0.0345 - val_mae: 0.1070\n",
      "Epoch 62/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0302 - mae: 0.1256 - val_loss: 0.0288 - val_mae: 0.1040\n",
      "Epoch 63/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0300 - mae: 0.1252 - val_loss: 0.0356 - val_mae: 0.1091\n",
      "Epoch 64/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0311 - mae: 0.1283 - val_loss: 0.0304 - val_mae: 0.1033\n",
      "Epoch 65/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0301 - mae: 0.1266 - val_loss: 0.0332 - val_mae: 0.1082\n",
      "Epoch 66/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0279 - mae: 0.1213 - val_loss: 0.0285 - val_mae: 0.1039\n",
      "Epoch 67/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0316 - mae: 0.1291 - val_loss: 0.0263 - val_mae: 0.1008\n",
      "Epoch 68/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0302 - mae: 0.1266 - val_loss: 0.0247 - val_mae: 0.1001\n",
      "Epoch 69/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0292 - mae: 0.1238 - val_loss: 0.0281 - val_mae: 0.1018\n",
      "Epoch 70/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0292 - mae: 0.1253 - val_loss: 0.0320 - val_mae: 0.1051\n",
      "Epoch 71/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0306 - mae: 0.1252 - val_loss: 0.0288 - val_mae: 0.1048\n",
      "Epoch 72/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0300 - mae: 0.1263 - val_loss: 0.0236 - val_mae: 0.1000\n",
      "Epoch 73/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0283 - mae: 0.1231 - val_loss: 0.0281 - val_mae: 0.1043\n",
      "Epoch 74/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0320 - mae: 0.1268 - val_loss: 0.0227 - val_mae: 0.0979\n",
      "Epoch 75/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0303 - mae: 0.1267 - val_loss: 0.0229 - val_mae: 0.0964\n",
      "Epoch 76/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0296 - mae: 0.1250 - val_loss: 0.0255 - val_mae: 0.1013\n",
      "Epoch 77/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0307 - mae: 0.1261 - val_loss: 0.0276 - val_mae: 0.1077\n",
      "Epoch 78/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0311 - mae: 0.1269 - val_loss: 0.0279 - val_mae: 0.1011\n",
      "Epoch 79/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0285 - mae: 0.1234 - val_loss: 0.0287 - val_mae: 0.1018\n",
      "Epoch 80/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0293 - mae: 0.1230 - val_loss: 0.0340 - val_mae: 0.1068\n",
      "Epoch 81/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0312 - mae: 0.1289 - val_loss: 0.0281 - val_mae: 0.1021\n",
      "Epoch 82/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0297 - mae: 0.1251 - val_loss: 0.0242 - val_mae: 0.0992\n",
      "Epoch 83/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0291 - mae: 0.1244 - val_loss: 0.0336 - val_mae: 0.1067\n",
      "Epoch 84/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0302 - mae: 0.1268 - val_loss: 0.0222 - val_mae: 0.0983\n",
      "Epoch 85/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0300 - mae: 0.1263 - val_loss: 0.0248 - val_mae: 0.1008\n",
      "Epoch 86/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0298 - mae: 0.1265 - val_loss: 0.0286 - val_mae: 0.1029\n",
      "Epoch 87/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0295 - mae: 0.1249 - val_loss: 0.0287 - val_mae: 0.1026\n",
      "Epoch 88/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0280 - mae: 0.1214 - val_loss: 0.0335 - val_mae: 0.1067\n",
      "Epoch 89/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0289 - mae: 0.1234 - val_loss: 0.0442 - val_mae: 0.1130\n",
      "Epoch 90/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0275 - mae: 0.1193 - val_loss: 0.0395 - val_mae: 0.1131\n",
      "Epoch 91/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0307 - mae: 0.1255 - val_loss: 0.0296 - val_mae: 0.1040\n",
      "Epoch 92/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0285 - mae: 0.1220 - val_loss: 0.0487 - val_mae: 0.1161\n",
      "Epoch 93/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0281 - mae: 0.1226 - val_loss: 0.0468 - val_mae: 0.1165\n",
      "Epoch 94/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0276 - mae: 0.1202 - val_loss: 0.0491 - val_mae: 0.1156\n",
      "Epoch 95/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0288 - mae: 0.1208 - val_loss: 0.0380 - val_mae: 0.1104\n",
      "Epoch 96/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0285 - mae: 0.1219 - val_loss: 0.0515 - val_mae: 0.1173\n",
      "Epoch 97/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0292 - mae: 0.1271 - val_loss: 0.0334 - val_mae: 0.1094\n",
      "Epoch 98/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0294 - mae: 0.1219 - val_loss: 0.0347 - val_mae: 0.1071\n",
      "Epoch 99/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0294 - mae: 0.1249 - val_loss: 0.0310 - val_mae: 0.1087\n",
      "Epoch 100/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0301 - mae: 0.1248 - val_loss: 0.0499 - val_mae: 0.1167\n",
      "processing fold # 5\n",
      "Train on 1163 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "1163/1163 [==============================] - 0s 285us/step - loss: 1.5597 - mae: 0.9877 - val_loss: 0.0904 - val_mae: 0.2215\n",
      "Epoch 2/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 1.0062 - mae: 0.7985 - val_loss: 0.0892 - val_mae: 0.2159\n",
      "Epoch 3/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.6955 - mae: 0.6660 - val_loss: 0.0915 - val_mae: 0.2260\n",
      "Epoch 4/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.4503 - mae: 0.5304 - val_loss: 0.0725 - val_mae: 0.1849\n",
      "Epoch 5/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.2683 - mae: 0.4070 - val_loss: 0.0526 - val_mae: 0.1443\n",
      "Epoch 6/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.1680 - mae: 0.3202 - val_loss: 0.0526 - val_mae: 0.1407\n",
      "Epoch 7/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0880 - mae: 0.2294 - val_loss: 0.0479 - val_mae: 0.1329\n",
      "Epoch 8/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0614 - mae: 0.1831 - val_loss: 0.0417 - val_mae: 0.1328\n",
      "Epoch 9/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0503 - mae: 0.1682 - val_loss: 0.0417 - val_mae: 0.1319\n",
      "Epoch 10/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0447 - mae: 0.1582 - val_loss: 0.0380 - val_mae: 0.1264\n",
      "Epoch 11/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0444 - mae: 0.1560 - val_loss: 0.0381 - val_mae: 0.1314\n",
      "Epoch 12/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0417 - mae: 0.1531 - val_loss: 0.0343 - val_mae: 0.1356\n",
      "Epoch 13/100\n",
      "1163/1163 [==============================] - 0s 66us/step - loss: 0.0417 - mae: 0.1546 - val_loss: 0.0345 - val_mae: 0.1276\n",
      "Epoch 14/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0404 - mae: 0.1509 - val_loss: 0.0342 - val_mae: 0.1293\n",
      "Epoch 15/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0387 - mae: 0.1448 - val_loss: 0.0326 - val_mae: 0.1243\n",
      "Epoch 16/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0383 - mae: 0.1457 - val_loss: 0.0318 - val_mae: 0.1216\n",
      "Epoch 17/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0377 - mae: 0.1452 - val_loss: 0.0326 - val_mae: 0.1188\n",
      "Epoch 18/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0377 - mae: 0.1424 - val_loss: 0.0327 - val_mae: 0.1163\n",
      "Epoch 19/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0363 - mae: 0.1391 - val_loss: 0.0312 - val_mae: 0.1158\n",
      "Epoch 20/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0378 - mae: 0.1417 - val_loss: 0.0303 - val_mae: 0.1172\n",
      "Epoch 21/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0358 - mae: 0.1390 - val_loss: 0.0315 - val_mae: 0.1153\n",
      "Epoch 22/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0335 - mae: 0.1340 - val_loss: 0.0301 - val_mae: 0.1167\n",
      "Epoch 23/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0345 - mae: 0.1368 - val_loss: 0.0301 - val_mae: 0.1130\n",
      "Epoch 24/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0365 - mae: 0.1400 - val_loss: 0.0321 - val_mae: 0.1168\n",
      "Epoch 25/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0350 - mae: 0.1342 - val_loss: 0.0293 - val_mae: 0.1131\n",
      "Epoch 26/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0321 - mae: 0.1323 - val_loss: 0.0296 - val_mae: 0.1112\n",
      "Epoch 27/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0349 - mae: 0.1372 - val_loss: 0.0308 - val_mae: 0.1098\n",
      "Epoch 28/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0325 - mae: 0.1324 - val_loss: 0.0296 - val_mae: 0.1133\n",
      "Epoch 29/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0334 - mae: 0.1347 - val_loss: 0.0300 - val_mae: 0.1119\n",
      "Epoch 30/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0333 - mae: 0.1365 - val_loss: 0.0309 - val_mae: 0.1104\n",
      "Epoch 31/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0331 - mae: 0.1335 - val_loss: 0.0306 - val_mae: 0.1104\n",
      "Epoch 32/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0326 - mae: 0.1332 - val_loss: 0.0308 - val_mae: 0.1105\n",
      "Epoch 33/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0323 - mae: 0.1325 - val_loss: 0.0304 - val_mae: 0.1079\n",
      "Epoch 34/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0327 - mae: 0.1313 - val_loss: 0.0296 - val_mae: 0.1093\n",
      "Epoch 35/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0313 - mae: 0.1296 - val_loss: 0.0307 - val_mae: 0.1099\n",
      "Epoch 36/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0314 - mae: 0.1305 - val_loss: 0.0303 - val_mae: 0.1104\n",
      "Epoch 37/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0327 - mae: 0.1320 - val_loss: 0.0300 - val_mae: 0.1104\n",
      "Epoch 38/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0299 - mae: 0.1296 - val_loss: 0.0314 - val_mae: 0.1116\n",
      "Epoch 39/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0296 - mae: 0.1260 - val_loss: 0.0313 - val_mae: 0.1093\n",
      "Epoch 40/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0305 - mae: 0.1269 - val_loss: 0.0316 - val_mae: 0.1114\n",
      "Epoch 41/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0306 - mae: 0.1275 - val_loss: 0.0312 - val_mae: 0.1082\n",
      "Epoch 42/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0300 - mae: 0.1303 - val_loss: 0.0321 - val_mae: 0.1115\n",
      "Epoch 43/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0292 - mae: 0.1258 - val_loss: 0.0322 - val_mae: 0.1098\n",
      "Epoch 44/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0293 - mae: 0.1252 - val_loss: 0.0318 - val_mae: 0.1095\n",
      "Epoch 45/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0297 - mae: 0.1271 - val_loss: 0.0325 - val_mae: 0.1090\n",
      "Epoch 46/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0290 - mae: 0.1242 - val_loss: 0.0341 - val_mae: 0.1115\n",
      "Epoch 47/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0315 - mae: 0.1279 - val_loss: 0.0309 - val_mae: 0.1085\n",
      "Epoch 48/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0311 - mae: 0.1288 - val_loss: 0.0307 - val_mae: 0.1087\n",
      "Epoch 49/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0297 - mae: 0.1255 - val_loss: 0.0322 - val_mae: 0.1111\n",
      "Epoch 50/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0293 - mae: 0.1246 - val_loss: 0.0330 - val_mae: 0.1129\n",
      "Epoch 51/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0302 - mae: 0.1274 - val_loss: 0.0325 - val_mae: 0.1104\n",
      "Epoch 52/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0292 - mae: 0.1267 - val_loss: 0.0320 - val_mae: 0.1102\n",
      "Epoch 53/100\n",
      "1163/1163 [==============================] - 0s 65us/step - loss: 0.0297 - mae: 0.1254 - val_loss: 0.0319 - val_mae: 0.1105\n",
      "Epoch 54/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0303 - mae: 0.1259 - val_loss: 0.0313 - val_mae: 0.1108\n",
      "Epoch 55/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0271 - mae: 0.1223 - val_loss: 0.0348 - val_mae: 0.1147\n",
      "Epoch 56/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0295 - mae: 0.1249 - val_loss: 0.0337 - val_mae: 0.1118\n",
      "Epoch 57/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0290 - mae: 0.1247 - val_loss: 0.0346 - val_mae: 0.1115\n",
      "Epoch 58/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0313 - mae: 0.1282 - val_loss: 0.0330 - val_mae: 0.1119\n",
      "Epoch 59/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0305 - mae: 0.1266 - val_loss: 0.0331 - val_mae: 0.1111\n",
      "Epoch 60/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0289 - mae: 0.1240 - val_loss: 0.0356 - val_mae: 0.1154\n",
      "Epoch 61/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0328 - mae: 0.1305 - val_loss: 0.0326 - val_mae: 0.1130\n",
      "Epoch 62/100\n",
      "1163/1163 [==============================] - 0s 68us/step - loss: 0.0292 - mae: 0.1244 - val_loss: 0.0332 - val_mae: 0.1126\n",
      "Epoch 63/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0293 - mae: 0.1262 - val_loss: 0.0324 - val_mae: 0.1097\n",
      "Epoch 64/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0284 - mae: 0.1239 - val_loss: 0.0320 - val_mae: 0.1095\n",
      "Epoch 65/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0293 - mae: 0.1240 - val_loss: 0.0325 - val_mae: 0.1119\n",
      "Epoch 66/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0282 - mae: 0.1249 - val_loss: 0.0327 - val_mae: 0.1106\n",
      "Epoch 67/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0303 - mae: 0.1252 - val_loss: 0.0321 - val_mae: 0.1097\n",
      "Epoch 68/100\n",
      "1163/1163 [==============================] - 0s 63us/step - loss: 0.0279 - mae: 0.1225 - val_loss: 0.0324 - val_mae: 0.1085\n",
      "Epoch 69/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0297 - mae: 0.1251 - val_loss: 0.0334 - val_mae: 0.1128\n",
      "Epoch 70/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0266 - mae: 0.1207 - val_loss: 0.0356 - val_mae: 0.1167\n",
      "Epoch 71/100\n",
      "1163/1163 [==============================] - 0s 64us/step - loss: 0.0285 - mae: 0.1232 - val_loss: 0.0337 - val_mae: 0.1105\n",
      "Epoch 72/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0284 - mae: 0.1210 - val_loss: 0.0351 - val_mae: 0.1125\n",
      "Epoch 73/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0276 - mae: 0.1213 - val_loss: 0.0348 - val_mae: 0.1118\n",
      "Epoch 74/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0309 - mae: 0.1262 - val_loss: 0.0318 - val_mae: 0.1119\n",
      "Epoch 75/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0280 - mae: 0.1251 - val_loss: 0.0332 - val_mae: 0.1127\n",
      "Epoch 76/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0293 - mae: 0.1261 - val_loss: 0.0339 - val_mae: 0.1129\n",
      "Epoch 77/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0283 - mae: 0.1247 - val_loss: 0.0323 - val_mae: 0.1097\n",
      "Epoch 78/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0290 - mae: 0.1240 - val_loss: 0.0332 - val_mae: 0.1103\n",
      "Epoch 79/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0276 - mae: 0.1240 - val_loss: 0.0342 - val_mae: 0.1116\n",
      "Epoch 80/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0290 - mae: 0.1234 - val_loss: 0.0393 - val_mae: 0.1197\n",
      "Epoch 81/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0281 - mae: 0.1244 - val_loss: 0.0337 - val_mae: 0.1101\n",
      "Epoch 82/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0270 - mae: 0.1212 - val_loss: 0.0379 - val_mae: 0.1172\n",
      "Epoch 83/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0279 - mae: 0.1216 - val_loss: 0.0384 - val_mae: 0.1174\n",
      "Epoch 84/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0261 - mae: 0.1188 - val_loss: 0.0391 - val_mae: 0.1161\n",
      "Epoch 85/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0291 - mae: 0.1236 - val_loss: 0.0368 - val_mae: 0.1157\n",
      "Epoch 86/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0268 - mae: 0.1198 - val_loss: 0.0378 - val_mae: 0.1152\n",
      "Epoch 87/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0280 - mae: 0.1222 - val_loss: 0.0380 - val_mae: 0.1172\n",
      "Epoch 88/100\n",
      "1163/1163 [==============================] - 0s 57us/step - loss: 0.0284 - mae: 0.1250 - val_loss: 0.0360 - val_mae: 0.1153\n",
      "Epoch 89/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0284 - mae: 0.1251 - val_loss: 0.0381 - val_mae: 0.1177\n",
      "Epoch 90/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0272 - mae: 0.1193 - val_loss: 0.0378 - val_mae: 0.1181\n",
      "Epoch 91/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0268 - mae: 0.1199 - val_loss: 0.0395 - val_mae: 0.1181\n",
      "Epoch 92/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0269 - mae: 0.1199 - val_loss: 0.0411 - val_mae: 0.1206\n",
      "Epoch 93/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0271 - mae: 0.1214 - val_loss: 0.0389 - val_mae: 0.1176\n",
      "Epoch 94/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0283 - mae: 0.1238 - val_loss: 0.0354 - val_mae: 0.1154\n",
      "Epoch 95/100\n",
      "1163/1163 [==============================] - 0s 59us/step - loss: 0.0283 - mae: 0.1212 - val_loss: 0.0368 - val_mae: 0.1165\n",
      "Epoch 96/100\n",
      "1163/1163 [==============================] - 0s 61us/step - loss: 0.0268 - mae: 0.1201 - val_loss: 0.0386 - val_mae: 0.1185\n",
      "Epoch 97/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0270 - mae: 0.1198 - val_loss: 0.0366 - val_mae: 0.1169\n",
      "Epoch 98/100\n",
      "1163/1163 [==============================] - 0s 58us/step - loss: 0.0282 - mae: 0.1215 - val_loss: 0.0349 - val_mae: 0.1126\n",
      "Epoch 99/100\n",
      "1163/1163 [==============================] - 0s 62us/step - loss: 0.0261 - mae: 0.1184 - val_loss: 0.0401 - val_mae: 0.1193\n",
      "Epoch 100/100\n",
      "1163/1163 [==============================] - 0s 60us/step - loss: 0.0290 - mae: 0.1251 - val_loss: 0.0355 - val_mae: 0.1134\n"
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "num_val = len(Xtrain)//k\n",
    "all_hist = []\n",
    "for i in range(k):\n",
    "    print('processing fold #',i)\n",
    "    val_data = Xtrain[i*num_val:(i+1)*num_val]\n",
    "    val_target = Ytrain[i*num_val:(i+1)*num_val]\n",
    "    \n",
    "    partial_data = np.concatenate([Xtrain[:i*num_val],Xtrain[(i+1)*num_val:]],axis = 0)\n",
    "    partial_target = np.concatenate([Ytrain[:i*num_val],Ytrain[(i+1)*num_val:]],axis = 0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_data,partial_target,epochs=100,batch_size=20,verbose = 1,validation_data=(val_data,val_target))\n",
    "    model.save(\"file2.h5\")\n",
    "    mae_hist = history.history['val_mae']\n",
    "    all_hist.append(mae_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_ame_hist = [np.mean([x[i] for x in all_hist]) for i in range(70)]\n",
    "def smoothcurve(pnt,factor=0.9):\n",
    "    smoothed_pnt = []\n",
    "    for j in pnt:\n",
    "        if smoothed_pnt:\n",
    "            previous = smoothed_pnt[-1]\n",
    "            smoothed_pnt.append(previous * factor + j * (1-factor))\n",
    "        else:\n",
    "            smoothed_pnt.append(j)\n",
    "    return smoothed_pnt\n",
    "\n",
    "smoothmae_hist = smoothcurve(avg_ame_hist[10:])\n",
    "\n",
    "plt.plot(range(1,len(smoothmae_hist) + 1),smoothmae_hist)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/599 [==============================] - 0s 18us/step\n",
      "599/599 [==============================] - 0s 77us/step\n",
      "0.10245712846517563\n"
     ]
    }
   ],
   "source": [
    "test_mse,test_mae = model.evaluate(Xtest,Ytest)\n",
    "var = load_model(\"file2.h5\")\n",
    "result =var.evaluate(Xtest,Ytest)\n",
    "print(test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
