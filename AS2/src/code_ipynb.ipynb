{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "(trainX,trainY),(testX,testY) = cifar10.load_data()\n",
    "trainY3 = ((trainY == 0)+(trainY ==1) +(trainY==2))\n",
    "trainY3_class = trainY[trainY3]\n",
    "trainX_class = trainX[trainY3[:,0],:,:,:]\n",
    "testY3 = ((testY == 0)+(testY ==1) +(testY==2))\n",
    "testY3_class = testY[testY3]\n",
    "testX_class = testX[testY3[:,0],:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "10000\n",
      "10000\n",
      "15000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(trainY))\n",
    "print(len(testX))\n",
    "print(len(testY))\n",
    "print(len(trainY3_class))\n",
    "print(len(trainX_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.001\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.001\n",
    "        \n",
    "    # y = Wx + b\n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "\n",
    "trainX = trainX_class.reshape(15000,32*3*32)\n",
    "a1 = nn_Sigmoid().forward(nn_Linear(3072,3).forward(trainX))\n",
    "\n",
    "# # Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "# x1 = np.array([[1, 2, 2, 3]])\n",
    "# a1 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x1))\n",
    "# print('x[1] = '+ str(x1))\n",
    "# print('a[1] = ' + str(a1))\n",
    "\n",
    "# # Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "# x2 = np.array([[4, 5, 2, 1]])\n",
    "# a2 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x2))\n",
    "# print('x[2] = '+ str(x2))\n",
    "# print('a[2] = ' + str(a2))\n",
    "\n",
    "# # We can also compute both at once, which could be more efficient since it requires a single matrix multiplication.\n",
    "# x = np.concatenate((x1, x2), axis = 0)\n",
    "# a = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x))\n",
    "# print('x = ' + str(x))\n",
    "# print('a = ' + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Category:  # MSE = mean squared error.\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W = [[-0.01314412 -0.00094724 -0.00427989]\n",
      " [ 0.00305823  0.01117694  0.00056257]\n",
      " [ 0.00102789  0.00480435  0.00208372]\n",
      " ...\n",
      " [ 0.00763975 -0.00621717 -0.00254085]\n",
      " [-0.00964656 -0.01512588  0.00163176]\n",
      " [-0.01553434  0.00151929 -0.00118366]]\n",
      "B = [[ 0.00319039  0.00387922 -0.00148443]]\n",
      "\n",
      "x1 = [[170 180 198 ...  73  77  80]\n",
      " [159 102 101 ... 182  57  19]\n",
      " [164 206  84 ... 122 170  44]\n",
      " ...\n",
      " [145 161 194 ...  37  39  54]\n",
      " [189 211 240 ... 195 190 171]\n",
      " [229 229 239 ... 163 163 161]]\n",
      "a0 = [[ -16.16137337   10.66272905  -28.33179919]\n",
      " [ -57.08310076   22.15582951 -102.94878474]\n",
      " [ -74.28491613  -67.49568553  -43.58044166]\n",
      " ...\n",
      " [ -59.28341437  -17.07522754 -149.44208947]\n",
      " [-124.4865879   -44.92349987  -95.56421585]\n",
      " [ -39.87055799   11.18301258  -51.50317583]]\n",
      "a1 = [[9.57645309e-08 9.99976599e-01 4.96199071e-13]\n",
      " [1.61854333e-25 1.00000000e+00 1.94944447e-45]\n",
      " [5.47609327e-33 4.86402974e-30 1.18373549e-19]\n",
      " ...\n",
      " [1.79283478e-26 3.83992625e-08 1.25350243e-65]\n",
      " [8.63300130e-55 3.09009549e-20 3.14042474e-42]\n",
      " [4.83544752e-18 9.99986092e-01 4.28997674e-23]]\n",
      "\n",
      "loss = 15411.554419467193\n",
      "\n",
      "da1 = [[ 2.87293593e-03 -7.02015505e-01  1.48859721e-08]\n",
      " [ 4.85562998e-21 -7.16088522e-06  5.84833342e-41]\n",
      " [ 1.64282798e-28  1.45920892e-25 -3.00000000e+04]\n",
      " ...\n",
      " [ 5.37850435e-22  1.15197788e-03 -3.00000000e+04]\n",
      " [ 2.58990039e-50 -3.00000000e+04  9.42127423e-38]\n",
      " [ 1.45063426e-13 -4.17248286e-01  1.28699302e-18]]\n",
      "da0 = [[ 2.75125335e-010 -1.64271412e-005  7.38640556e-021]\n",
      " [ 7.85904750e-046 -1.70927590e-015  1.14010013e-085]\n",
      " [ 8.99627926e-061  7.09763560e-055 -3.55120646e-015]\n",
      " ...\n",
      " [ 9.64276968e-048  4.42350992e-011 -3.76050729e-061]\n",
      " [ 2.23586134e-104 -9.27028648e-016  2.95868027e-079]\n",
      " [ 7.01446582e-031 -5.80312368e-006  5.52117012e-041]]\n",
      "dx1 = [[ 1.55567898e-08 -1.83604263e-07 -7.89214555e-08 ...  1.02132392e-07\n",
      "   2.48472361e-07 -2.49618012e-08]\n",
      " [ 1.61909043e-18 -1.91044672e-17 -8.21195992e-18 ...  1.06268547e-17\n",
      "   2.58543071e-17 -2.59687911e-18]\n",
      " [ 1.51987789e-17 -1.99779952e-18 -7.39971394e-18 ...  9.02308149e-18\n",
      "  -5.79470841e-18  4.20343155e-18]\n",
      " ...\n",
      " [-4.19011500e-14  4.94412868e-13  2.12520905e-13 ... -2.75017025e-13\n",
      "  -6.69094931e-13  6.72057712e-14]\n",
      " [ 8.78116407e-19 -1.03613398e-17 -4.45377021e-18 ...  5.76349245e-18\n",
      "   1.40221268e-17 -1.40842174e-18]\n",
      " [ 5.49693704e-09 -6.48611413e-08 -2.78802380e-08 ...  3.60789924e-08\n",
      "   8.77773692e-08 -8.81660512e-09]]\n",
      "\n",
      "dW = [[1.24627119e+07 8.38445975e+07 6.83158060e+06]\n",
      " [1.07849444e+07 9.83281317e+07 7.98103903e+06]\n",
      " [7.39721086e+06 1.08302439e+08 8.99178028e+06]\n",
      " ...\n",
      " [2.54745240e+07 8.14794463e+07 9.18056290e+06]\n",
      " [2.38413398e+07 8.25715914e+07 9.65992575e+06]\n",
      " [2.07932340e+07 7.62229035e+07 1.01660618e+07]]\n",
      "dB = [[ 2.75125335e-010 -1.64271412e-005  7.38640556e-021]\n",
      " [ 7.85904750e-046 -1.70927590e-015  1.14010013e-085]\n",
      " [ 8.99627926e-061  7.09763560e-055 -3.55120646e-015]\n",
      " ...\n",
      " [ 9.64276968e-048  4.42350992e-011 -3.76050729e-061]\n",
      " [ 2.23586134e-104 -9.27028648e-016  2.95868027e-079]\n",
      " [ 7.01446582e-031 -5.80312368e-006  5.52117012e-041]]\n"
     ]
    }
   ],
   "source": [
    "# This is referred above as f(u).\n",
    "class Cross_Category:\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "        \n",
    "    def backward(self, predictions, labels):\n",
    "        num_samples = labels.shape[0]\n",
    "        return num_samples * 2 * (predictions - labels)\n",
    "\n",
    "# This is referred above as g(v).\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # It is usually a good idea to use gv from the forward pass and not recompute it again here.\n",
    "        gv = 1 / (1 + np.exp(-x))  \n",
    "        return np.multiply(np.multiply(gv, (1 - gv)), gradOutput)\n",
    "\n",
    "# This is referred above as h(W, b)\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.01\n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        self.gradBias = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # dL/dw = dh/dw * dL/dv\n",
    "        self.gradWeight = np.dot(x.T, gradOutput)\n",
    "        # dL/db = dh/db * dL/dv\n",
    "        self.gradBias = np.copy(gradOutput)\n",
    "        # return dL/dx = dh/dx * dL/dv\n",
    "        return np.dot(gradOutput, self.weight.T)\n",
    "    \n",
    "    def getParameters(self):\n",
    "        params = [self.weight, self.bias]\n",
    "        gradParams = [self.gradWeight, self.gradBias]\n",
    "        return params, gradParams\n",
    "    \n",
    "trainX = trainX_class.reshape(15000,32*3*32)\n",
    "a1 = nn_Sigmoid().forward(nn_Linear(3072,3).forward(trainX))\n",
    "trainY = to_categorical(trainY3_class)\n",
    "\n",
    "# # Let's test some dummy inputs for a full pass of forward and backward propagation.\n",
    "# x1 = np.array([[1, 2, 2, 3]])\n",
    "# y1 = np.array([[0.25, 0.25, 0.25]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(3072, 3)  # h(W, b)\n",
    "sigmoid = nn_Sigmoid()  # g(v)\n",
    "loss = nn_MSECriterion()  # f(u)\n",
    "\n",
    "# Forward-propagation.\n",
    "a0 = linear.forward(trainX)\n",
    "a1 = sigmoid.forward(a0)\n",
    "loss_val = loss.forward(a1, trainY) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "da1 = loss.backward(a1, trainY)\n",
    "da0 = sigmoid.backward(a0, da1)\n",
    "dx1 = linear.backward(trainX, da0)\n",
    "\n",
    "# Show parameters of the linear layer.\n",
    "print('\\nW = ' + str(linear.weight))\n",
    "print('B = ' + str(linear.bias))\n",
    "\n",
    "# Show the intermediate outputs in the forward pass.\n",
    "print('\\nx1 = '+ str(trainX))\n",
    "print('a0 = ' + str(a0))\n",
    "print('a1 = ' + str(a1))\n",
    "\n",
    "print('\\nloss = ' + str(loss_val))\n",
    "\n",
    "# Show the intermediate gradients with respect to inputs in the backward pass.\n",
    "print('\\nda1 = ' + str(da1))\n",
    "print('da0 = ' + str(da0))\n",
    "print('dx1 = ' + str(dx1))\n",
    "\n",
    "# Show the gradients with respect to parameters.\n",
    "print('\\ndW = ' + str(linear.gradWeight))\n",
    "print('dB = ' + str(linear.gradBias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We will compute derivatives with respect to a single data pair (x,y)\n",
    "# x = np.array([[2.34, 3.8, 34.44, 5.33]])\n",
    "# y = np.array([[3.2, 4.2, 5.3]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(3072, 3)\n",
    "sigmoid = nn_Sigmoid()\n",
    "criterion = Cross_Category()\n",
    "\n",
    "# Forward-propagation.\n",
    "a0 = linear.forward(trainX)\n",
    "a1 = sigmoid.forward(a0)\n",
    "loss = criterion.forward(a1, trainY) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "da1 = criterion.backward(a1, trainY)\n",
    "da0 = sigmoid.backward(a0, da1)\n",
    "dx = linear.backward(trainX, da0)\n",
    "\n",
    "gradWeight = linear.gradWeight\n",
    "gradBias = linear.gradBias\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear.weight)\n",
    "approxGradBias = np.zeros_like(linear.bias)\n",
    "\n",
    "# We will verify here that gradWeights are correct and leave it as an excercise\n",
    "# to verify the gradBias.\n",
    "epsilon = 0.0001\n",
    "for i in range(0, linear.weight.shape[0]):\n",
    "    for j in range(0, linear.weight.shape[1]):\n",
    "        # Compute f(w)\n",
    "        fw = criterion.forward(sigmoid.forward(linear.forward(trainX)), trainY) # Loss function.\n",
    "        # Compute f(w + eps)\n",
    "        shifted_weight = np.copy(linear.weight)\n",
    "        shifted_weight[i, j] = shifted_weight[i, j] + epsilon\n",
    "        shifted_linear = nn_Linear(3072, 3)\n",
    "        shifted_linear.bias = linear.bias\n",
    "        shifted_linear.weight = shifted_weight\n",
    "        fw_epsilon = criterion.forward(sigmoid.forward(shifted_linear.forward(trainX)), trainY) # Loss function\n",
    "        # Compute (f(w + eps) - f(w)) / eps\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / epsilon\n",
    "\n",
    "# These two outputs should be similar up to some precision.\n",
    "print('gradWeight: ' + str(gradWeight))\n",
    "print('\\napproxGradWeight: ' + str(approxGradWeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "\n",
    "# Generate random inputs within some range.\n",
    "x = np.random.uniform(0, 6, (dataset_size, 4))\n",
    "# Generate outputs based on the inputs using some function.\n",
    "y1 = np.sin(x.sum(axis = 1))\n",
    "y2 = np.sin(x[:, 1] * 6)\n",
    "y3 = np.sin(x[:, 1] + x[:, 3])\n",
    "y = np.array([y1, y2, y3]).T\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.1\n",
    "\n",
    "model = {}\n",
    "model['linear'] = nn_Linear(3072, 3)\n",
    "model['linear2'] = nn_Linear(3, 3)\n",
    "model['sigmoid'] = nn_Sigmoid()\n",
    "model['loss'] = Cross_Category()\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    loss = 0\n",
    "    for i in range(0, dataset_size):\n",
    "        xi = trainX[i:i+1, :]\n",
    "        yi = trainY[i:i+1, :]\n",
    "\n",
    "        # Forward.\n",
    "        a0 = model['linear'].forward(xi)\n",
    "        a1 = model['sigmoid'].forward(a0)\n",
    "        a2 = model['linear2'].forward(a1)\n",
    "        a3 = model['sigmoid'].forward(a2)\n",
    "        a4 = model['linear2'].forward(a3)\n",
    "        a5 = model['sigmoid'].forward(a4)\n",
    "        loss += model['loss'].forward(a5, yi)\n",
    "\n",
    "        # Backward.\n",
    "        da1 = model['loss'].backward(a1, yi)\n",
    "        da0 = model['sigmoid'].backward(a0, da1)\n",
    "        model['linear'].backward(xi, da0)\n",
    "        \n",
    "        model['linear'].weight = model['linear'].weight - learningRate * model['linear'].gradWeight\n",
    "        model['linear'].bias = model['linear'].bias - learningRate * model['linear'].gradBias\n",
    "    \n",
    "    if epoch % 10 == 0: print('epoch[%d] = %.8f' % (epoch, loss / dataset_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
